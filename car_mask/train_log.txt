INFO: 2017-08-15 11:32:28: main.py:15 **  start
INFO: 2017-08-15 11:35:33: main.py:15 **  start
INFO: 2017-08-22 23:55:56: main.py:22 **  start
INFO: 2017-08-22 23:55:56: main.py:24 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-22 23:55:56: main.py:25 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-22 23:55:56: main.py:26 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-22 23:56:42: main.py:22 **  start
INFO: 2017-08-22 23:56:42: main.py:24 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-22 23:56:42: main.py:25 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-22 23:56:42: main.py:26 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-22 23:56:42: main.py:27 **  读取数据集...
INFO: 2017-08-22 23:57:54: main.py:22 **  start
INFO: 2017-08-22 23:57:54: main.py:24 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-22 23:57:54: main.py:25 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-22 23:57:54: main.py:26 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-22 23:57:54: main.py:27 **  读取数据集...
INFO: 2017-08-23 00:08:55: main.py:22 **  start
INFO: 2017-08-23 00:08:55: main.py:24 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-23 00:08:55: main.py:25 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-23 00:08:55: main.py:26 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-23 00:08:55: main.py:27 **  读取数据集...
INFO: 2017-08-23 00:11:25: main.py:22 **  start
INFO: 2017-08-23 00:11:25: main.py:24 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-23 00:11:25: main.py:25 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-23 00:11:25: main.py:26 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-23 00:11:25: main.py:27 **  读取数据集...
INFO: 2017-08-23 00:11:59: main.py:22 **  start
INFO: 2017-08-23 00:11:59: main.py:24 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-23 00:11:59: main.py:25 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-23 00:11:59: main.py:26 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-23 00:11:59: main.py:27 **  读取数据集...
INFO: 2017-08-23 00:12:20: main.py:22 **  start
INFO: 2017-08-23 00:12:20: main.py:24 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-23 00:12:20: main.py:25 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-23 00:12:20: main.py:26 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-23 00:12:20: main.py:27 **  读取数据集...
INFO: 2017-08-23 00:15:10: main.py:22 **  start
INFO: 2017-08-23 00:15:10: main.py:24 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-23 00:15:10: main.py:25 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-23 00:15:10: main.py:26 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-23 00:15:10: main.py:27 **  读取数据集...
INFO: 2017-08-23 00:15:59: main.py:22 **  start
INFO: 2017-08-23 00:15:59: main.py:24 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-23 00:15:59: main.py:25 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-23 00:15:59: main.py:26 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-23 00:15:59: main.py:27 **  读取数据集...
INFO: 2017-08-23 00:17:57: main.py:22 **  start
INFO: 2017-08-23 00:17:57: main.py:24 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-23 00:17:57: main.py:25 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-23 00:17:57: main.py:26 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-23 00:17:57: main.py:27 **  读取数据集...
INFO: 2017-08-23 00:18:42: main.py:22 **  start
INFO: 2017-08-23 00:18:42: main.py:24 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-23 00:18:42: main.py:25 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-23 00:18:42: main.py:26 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-23 00:18:42: main.py:27 **  读取数据集...
INFO: 2017-08-23 00:21:07: main.py:22 **  start
INFO: 2017-08-23 00:21:07: main.py:24 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-23 00:21:07: main.py:25 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-23 00:21:07: main.py:26 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-23 00:21:07: main.py:27 **  读取数据集...
INFO: 2017-08-23 00:22:18: main.py:22 **  start
INFO: 2017-08-23 00:22:18: main.py:24 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-23 00:22:18: main.py:25 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-23 00:22:18: main.py:26 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-23 00:22:18: main.py:27 **  读取数据集...
INFO: 2017-08-23 00:22:37: main.py:22 **  start
INFO: 2017-08-23 00:22:37: main.py:24 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-23 00:22:37: main.py:25 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-23 00:22:37: main.py:26 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-23 00:22:37: main.py:27 **  读取数据集...
INFO: 2017-08-23 20:07:09: main.py:22 **  start
INFO: 2017-08-23 20:07:09: main.py:24 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-23 20:07:09: main.py:25 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-23 20:07:09: main.py:26 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-23 20:07:09: main.py:27 **  读取数据集...
INFO: 2017-08-23 20:07:12: main.py:37 **  训练数据集长度318
INFO: 2017-08-23 20:12:01: main.py:22 **  start
INFO: 2017-08-23 20:12:01: main.py:24 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-23 20:12:01: main.py:25 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-23 20:12:01: main.py:26 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-23 20:12:01: main.py:27 **  读取数据集...
INFO: 2017-08-23 20:12:04: main.py:38 **  训练数据集长度318
INFO: 2017-08-23 20:12:19: main.py:22 **  start
INFO: 2017-08-23 20:12:19: main.py:24 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-23 20:12:19: main.py:25 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-23 20:12:19: main.py:26 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-23 20:12:19: main.py:27 **  读取数据集...
INFO: 2017-08-23 20:12:22: main.py:38 **  训练数据集长度318
INFO: 2017-08-23 20:14:34: main.py:22 **  start
INFO: 2017-08-23 20:14:34: main.py:24 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-23 20:14:34: main.py:25 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-23 20:14:34: main.py:26 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-23 20:14:34: main.py:27 **  读取数据集...
INFO: 2017-08-23 20:16:35: main.py:22 **  start
INFO: 2017-08-23 20:16:35: main.py:24 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-23 20:16:35: main.py:25 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-23 20:16:35: main.py:26 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-23 20:16:35: main.py:27 **  读取数据集...
INFO: 2017-08-23 20:16:38: main.py:38 **  训练数据集长度318
INFO: 2017-08-23 20:17:58: main.py:22 **  start
INFO: 2017-08-23 20:17:58: main.py:24 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-23 20:17:58: main.py:25 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-23 20:17:58: main.py:26 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-23 20:17:58: main.py:27 **  读取数据集...
INFO: 2017-08-23 20:22:02: main.py:22 **  start
INFO: 2017-08-23 20:22:02: main.py:24 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-23 20:22:02: main.py:25 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-23 20:22:02: main.py:26 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-23 20:22:02: main.py:27 **  读取数据集...
INFO: 2017-08-23 20:22:05: main.py:38 **  train data sample counts
INFO: 2017-08-23 20:22:05: main.py:39 **  train data set batch size
INFO: 2017-08-23 20:22:05: main.py:40 **  train data batch counts
INFO: 2017-08-23 20:22:30: main.py:22 **  start
INFO: 2017-08-23 20:22:30: main.py:24 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-23 20:22:30: main.py:25 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-23 20:22:30: main.py:26 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-23 20:22:30: main.py:27 **  读取数据集...
INFO: 2017-08-23 20:22:33: main.py:37 **  train data sample counts 5088
INFO: 2017-08-23 20:22:33: main.py:38 **  train data set batch size 16
INFO: 2017-08-23 20:22:33: main.py:39 **  train data batch counts 318
INFO: 2017-08-23 21:33:10: main.py:22 **  start
INFO: 2017-08-23 21:33:10: main.py:24 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-23 21:33:10: main.py:25 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-23 21:33:10: main.py:26 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-23 21:33:10: main.py:27 **  读取数据集...
INFO: 2017-08-23 21:33:13: main.py:37 **  train data sample counts 5088
INFO: 2017-08-23 21:33:13: main.py:38 **  train data set batch size 16
INFO: 2017-08-23 21:33:13: main.py:39 **  train data batch counts 318
INFO: 2017-08-23 21:33:55: main.py:22 **  start
INFO: 2017-08-23 21:33:55: main.py:24 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-23 21:33:55: main.py:25 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-23 21:33:55: main.py:26 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-23 21:33:55: main.py:27 **  读取数据集...
INFO: 2017-08-23 21:33:58: main.py:37 **  train data sample counts 5088
INFO: 2017-08-23 21:33:58: main.py:38 **  train data set batch size 16
INFO: 2017-08-23 21:33:58: main.py:39 **  train data batch counts 318
INFO: 2017-08-26 00:24:59: main.py:26 **  start
INFO: 2017-08-26 00:24:59: main.py:28 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-26 00:24:59: main.py:29 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-26 00:24:59: main.py:30 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-26 00:24:59: main.py:31 **  读取数据集...
INFO: 2017-08-26 00:25:02: main.py:41 **  train data sample counts 5088
INFO: 2017-08-26 00:25:02: main.py:42 **  train data set batch size 16
INFO: 2017-08-26 00:25:02: main.py:43 **  train data batch counts 318
INFO: 2017-08-26 00:25:04: main.py:50 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 16:19:27: main.py:26 **  start
INFO: 2017-08-27 16:19:27: main.py:28 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 16:19:27: main.py:29 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 16:19:27: main.py:30 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 16:19:27: main.py:31 **  读取数据集...
INFO: 2017-08-27 16:19:31: main.py:41 **  train data sample counts 5088
INFO: 2017-08-27 16:19:31: main.py:42 **  train data set batch size 16
INFO: 2017-08-27 16:19:31: main.py:43 **  train data batch counts 318
INFO: 2017-08-27 16:19:33: main.py:50 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 16:21:58: main.py:26 **  start
INFO: 2017-08-27 16:21:58: main.py:28 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 16:21:58: main.py:29 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 16:21:58: main.py:30 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 16:21:58: main.py:31 **  读取数据集...
INFO: 2017-08-27 16:22:02: main.py:41 **  train data sample counts 5088
INFO: 2017-08-27 16:22:02: main.py:42 **  train data set batch size 4
INFO: 2017-08-27 16:22:02: main.py:43 **  train data batch counts 1272
INFO: 2017-08-27 16:22:04: main.py:50 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 16:25:08: main.py:27 **  start
INFO: 2017-08-27 16:25:08: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 16:25:08: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 16:25:08: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 16:25:08: main.py:32 **  读取数据集...
INFO: 2017-08-27 16:25:11: main.py:42 **  train data sample counts 5088
INFO: 2017-08-27 16:25:11: main.py:43 **  train data set batch size 1
INFO: 2017-08-27 16:25:11: main.py:44 **  train data batch counts 5088
INFO: 2017-08-27 16:25:13: main.py:51 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 16:32:17: main.py:27 **  start
INFO: 2017-08-27 16:32:17: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 16:32:17: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 16:32:17: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 16:32:17: main.py:32 **  读取数据集...
INFO: 2017-08-27 16:32:20: main.py:42 **  train data sample counts 5088
INFO: 2017-08-27 16:32:20: main.py:43 **  train data set batch size 1
INFO: 2017-08-27 16:32:20: main.py:44 **  train data batch counts 5088
INFO: 2017-08-27 16:32:20: main.py:51 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 17:04:43: main.py:27 **  start
INFO: 2017-08-27 17:04:43: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 17:04:43: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 17:04:43: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 17:04:43: main.py:32 **  读取数据集...
INFO: 2017-08-27 17:04:46: main.py:42 **  train data sample counts 5088
INFO: 2017-08-27 17:04:46: main.py:43 **  train data set batch size 1
INFO: 2017-08-27 17:04:46: main.py:44 **  train data batch counts 5088
INFO: 2017-08-27 17:04:47: main.py:51 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 17:05:19: main.py:27 **  start
INFO: 2017-08-27 17:05:19: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 17:05:19: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 17:05:19: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 17:05:19: main.py:32 **  读取数据集...
INFO: 2017-08-27 17:05:22: main.py:42 **  train data sample counts 5088
INFO: 2017-08-27 17:05:22: main.py:43 **  train data set batch size 1
INFO: 2017-08-27 17:05:22: main.py:44 **  train data batch counts 5088
INFO: 2017-08-27 17:05:22: main.py:51 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 17:05:46: main.py:27 **  start
INFO: 2017-08-27 17:05:46: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 17:05:46: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 17:05:46: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 17:05:46: main.py:32 **  读取数据集...
INFO: 2017-08-27 17:05:49: main.py:42 **  train data sample counts 5088
INFO: 2017-08-27 17:05:49: main.py:43 **  train data set batch size 1
INFO: 2017-08-27 17:05:49: main.py:44 **  train data batch counts 5088
INFO: 2017-08-27 17:05:49: main.py:51 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 17:06:51: main.py:27 **  start
INFO: 2017-08-27 17:06:51: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 17:06:51: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 17:06:51: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 17:06:51: main.py:32 **  读取数据集...
INFO: 2017-08-27 17:06:54: main.py:42 **  train data sample counts 5088
INFO: 2017-08-27 17:06:54: main.py:43 **  train data set batch size 1
INFO: 2017-08-27 17:06:54: main.py:44 **  train data batch counts 5088
INFO: 2017-08-27 17:06:54: main.py:51 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 17:10:16: main.py:27 **  start
INFO: 2017-08-27 17:10:16: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 17:10:16: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 17:10:16: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 17:10:16: main.py:32 **  读取数据集...
INFO: 2017-08-27 17:10:19: main.py:42 **  train data sample counts 5088
INFO: 2017-08-27 17:10:19: main.py:43 **  train data set batch size 1
INFO: 2017-08-27 17:10:19: main.py:44 **  train data batch counts 5088
INFO: 2017-08-27 17:10:20: main.py:51 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 17:12:58: main.py:27 **  start
INFO: 2017-08-27 17:12:58: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 17:12:58: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 17:12:58: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 17:12:58: main.py:32 **  读取数据集...
INFO: 2017-08-27 17:13:01: main.py:42 **  train data sample counts 5088
INFO: 2017-08-27 17:13:01: main.py:43 **  train data set batch size 1
INFO: 2017-08-27 17:13:01: main.py:44 **  train data batch counts 5088
INFO: 2017-08-27 17:13:01: main.py:51 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 17:13:18: main.py:27 **  start
INFO: 2017-08-27 17:13:18: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 17:13:18: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 17:13:18: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 17:13:18: main.py:32 **  读取数据集...
INFO: 2017-08-27 17:13:21: main.py:42 **  train data sample counts 5088
INFO: 2017-08-27 17:13:21: main.py:43 **  train data set batch size 1
INFO: 2017-08-27 17:13:21: main.py:44 **  train data batch counts 5088
INFO: 2017-08-27 17:13:21: main.py:51 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 17:16:23: main.py:27 **  start
INFO: 2017-08-27 17:16:23: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 17:16:23: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 17:16:23: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 17:16:23: main.py:32 **  读取数据集...
INFO: 2017-08-27 17:16:26: main.py:42 **  train data sample counts 5088
INFO: 2017-08-27 17:16:26: main.py:43 **  train data set batch size 1
INFO: 2017-08-27 17:16:26: main.py:44 **  train data batch counts 5088
INFO: 2017-08-27 17:16:26: main.py:51 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 17:17:05: main.py:27 **  start
INFO: 2017-08-27 17:17:05: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 17:17:05: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 17:17:05: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 17:17:05: main.py:32 **  读取数据集...
INFO: 2017-08-27 17:17:08: main.py:42 **  train data sample counts 5088
INFO: 2017-08-27 17:17:08: main.py:43 **  train data set batch size 1
INFO: 2017-08-27 17:17:08: main.py:44 **  train data batch counts 5088
INFO: 2017-08-27 17:17:09: main.py:51 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 17:18:02: main.py:27 **  start
INFO: 2017-08-27 17:18:02: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 17:18:02: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 17:18:02: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 17:18:02: main.py:32 **  读取数据集...
INFO: 2017-08-27 17:18:05: main.py:42 **  train data sample counts 5088
INFO: 2017-08-27 17:18:05: main.py:43 **  train data set batch size 1
INFO: 2017-08-27 17:18:05: main.py:44 **  train data batch counts 5088
INFO: 2017-08-27 17:18:05: main.py:51 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 17:20:04: main.py:27 **  start
INFO: 2017-08-27 17:20:04: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 17:20:04: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 17:20:04: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 17:20:04: main.py:32 **  读取数据集...
INFO: 2017-08-27 17:20:07: main.py:42 **  train data sample counts 5088
INFO: 2017-08-27 17:20:07: main.py:43 **  train data set batch size 1
INFO: 2017-08-27 17:20:07: main.py:44 **  train data batch counts 5088
INFO: 2017-08-27 17:20:07: main.py:51 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 17:20:52: main.py:27 **  start
INFO: 2017-08-27 17:20:52: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 17:20:52: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 17:20:52: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 17:20:52: main.py:32 **  读取数据集...
INFO: 2017-08-27 17:20:55: main.py:42 **  train data sample counts 5088
INFO: 2017-08-27 17:20:55: main.py:43 **  train data set batch size 1
INFO: 2017-08-27 17:20:55: main.py:44 **  train data batch counts 5088
INFO: 2017-08-27 17:20:55: main.py:51 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 17:29:01: main.py:27 **  start
INFO: 2017-08-27 17:29:01: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 17:29:01: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 17:29:01: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 17:29:01: main.py:32 **  读取数据集...
INFO: 2017-08-27 17:29:04: main.py:42 **  train data sample counts 5088
INFO: 2017-08-27 17:29:04: main.py:43 **  train data set batch size 1
INFO: 2017-08-27 17:29:04: main.py:44 **  train data batch counts 5088
INFO: 2017-08-27 17:29:05: main.py:51 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 17:36:18: main.py:27 **  start
INFO: 2017-08-27 17:36:18: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 17:36:18: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 17:36:18: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 17:36:18: main.py:32 **  读取数据集...
INFO: 2017-08-27 17:36:22: main.py:42 **  train data sample counts 5088
INFO: 2017-08-27 17:36:22: main.py:43 **  train data set batch size 1
INFO: 2017-08-27 17:36:22: main.py:44 **  train data batch counts 5088
INFO: 2017-08-27 17:36:22: main.py:51 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 18:07:45: main.py:27 **  start
INFO: 2017-08-27 18:07:45: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 18:07:45: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 18:07:45: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 18:07:45: main.py:32 **  读取数据集...
INFO: 2017-08-27 18:07:48: main.py:42 **  train data sample counts 5088
INFO: 2017-08-27 18:07:48: main.py:43 **  train data set batch size 1
INFO: 2017-08-27 18:07:48: main.py:44 **  train data batch counts 5088
INFO: 2017-08-27 18:07:48: main.py:51 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 18:08:26: main.py:27 **  start
INFO: 2017-08-27 18:08:26: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 18:08:26: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 18:08:26: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 18:08:26: main.py:32 **  读取数据集...
INFO: 2017-08-27 18:08:28: main.py:42 **  train data sample counts 5088
INFO: 2017-08-27 18:08:28: main.py:43 **  train data set batch size 1
INFO: 2017-08-27 18:08:28: main.py:44 **  train data batch counts 5088
INFO: 2017-08-27 18:08:29: main.py:51 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 18:08:47: main.py:27 **  start
INFO: 2017-08-27 18:08:47: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 18:08:47: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 18:08:47: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 18:08:47: main.py:32 **  读取数据集...
INFO: 2017-08-27 18:08:50: main.py:42 **  train data sample counts 5088
INFO: 2017-08-27 18:08:50: main.py:43 **  train data set batch size 1
INFO: 2017-08-27 18:08:50: main.py:44 **  train data batch counts 5088
INFO: 2017-08-27 18:08:50: main.py:51 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 18:09:13: main.py:27 **  start
INFO: 2017-08-27 18:09:13: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 18:09:13: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 18:09:13: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 18:09:13: main.py:32 **  读取数据集...
INFO: 2017-08-27 18:09:16: main.py:42 **  train data sample counts 5088
INFO: 2017-08-27 18:09:16: main.py:43 **  train data set batch size 1
INFO: 2017-08-27 18:09:16: main.py:44 **  train data batch counts 5088
INFO: 2017-08-27 18:09:16: main.py:51 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 18:11:04: main.py:27 **  start
INFO: 2017-08-27 18:11:04: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 18:11:04: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 18:11:04: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 18:11:04: main.py:32 **  读取数据集...
INFO: 2017-08-27 18:11:07: main.py:42 **  train data sample counts 5088
INFO: 2017-08-27 18:11:07: main.py:43 **  train data set batch size 1
INFO: 2017-08-27 18:11:07: main.py:44 **  train data batch counts 5088
INFO: 2017-08-27 18:11:07: main.py:51 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 18:11:58: main.py:27 **  start
INFO: 2017-08-27 18:11:58: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 18:11:58: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 18:11:58: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 18:11:58: main.py:32 **  读取数据集...
INFO: 2017-08-27 18:12:01: main.py:42 **  train data sample counts 5088
INFO: 2017-08-27 18:12:01: main.py:43 **  train data set batch size 1
INFO: 2017-08-27 18:12:01: main.py:44 **  train data batch counts 5088
INFO: 2017-08-27 18:12:01: main.py:51 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 18:14:11: main.py:27 **  start
INFO: 2017-08-27 18:14:11: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 18:14:11: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 18:14:11: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 18:14:11: main.py:32 **  读取数据集...
INFO: 2017-08-27 18:14:14: main.py:42 **  train data sample counts 5088
INFO: 2017-08-27 18:14:14: main.py:43 **  train data set batch size 1
INFO: 2017-08-27 18:14:14: main.py:44 **  train data batch counts 5088
INFO: 2017-08-27 18:14:15: main.py:51 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 18:15:27: main.py:27 **  start
INFO: 2017-08-27 18:15:27: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 18:15:27: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 18:15:27: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 18:15:27: main.py:32 **  读取数据集...
INFO: 2017-08-27 18:15:30: main.py:42 **  train data sample counts 5088
INFO: 2017-08-27 18:15:30: main.py:43 **  train data set batch size 1
INFO: 2017-08-27 18:15:30: main.py:44 **  train data batch counts 5088
INFO: 2017-08-27 18:15:30: main.py:51 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 18:16:32: main.py:27 **  start
INFO: 2017-08-27 18:16:32: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 18:16:32: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 18:16:32: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 18:16:32: main.py:32 **  读取数据集...
INFO: 2017-08-27 18:16:35: main.py:42 **  train data sample counts 5088
INFO: 2017-08-27 18:16:35: main.py:43 **  train data set batch size 1
INFO: 2017-08-27 18:16:35: main.py:44 **  train data batch counts 5088
INFO: 2017-08-27 18:16:36: main.py:51 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 18:17:50: main.py:27 **  start
INFO: 2017-08-27 18:17:50: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 18:17:50: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 18:17:50: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 18:17:50: main.py:32 **  读取数据集...
INFO: 2017-08-27 18:17:53: main.py:42 **  train data sample counts 5088
INFO: 2017-08-27 18:17:53: main.py:43 **  train data set batch size 1
INFO: 2017-08-27 18:17:53: main.py:44 **  train data batch counts 5088
INFO: 2017-08-27 18:17:53: main.py:51 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 18:19:41: main.py:27 **  start
INFO: 2017-08-27 18:19:41: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 18:19:41: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 18:19:41: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 18:19:41: main.py:32 **  读取数据集...
INFO: 2017-08-27 18:19:44: main.py:42 **  train data sample counts 5088
INFO: 2017-08-27 18:19:44: main.py:43 **  train data set batch size 1
INFO: 2017-08-27 18:19:44: main.py:44 **  train data batch counts 5088
INFO: 2017-08-27 18:19:44: main.py:51 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 18:20:04: main.py:27 **  start
INFO: 2017-08-27 18:20:04: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 18:20:04: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 18:20:04: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 18:20:04: main.py:32 **  读取数据集...
INFO: 2017-08-27 18:20:07: main.py:42 **  train data sample counts 5088
INFO: 2017-08-27 18:20:07: main.py:43 **  train data set batch size 1
INFO: 2017-08-27 18:20:07: main.py:44 **  train data batch counts 5088
INFO: 2017-08-27 18:20:07: main.py:51 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 18:20:49: main.py:27 **  start
INFO: 2017-08-27 18:20:49: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 18:20:49: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 18:20:49: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 18:20:49: main.py:32 **  读取数据集...
INFO: 2017-08-27 18:20:52: main.py:42 **  train data sample counts 5088
INFO: 2017-08-27 18:20:52: main.py:43 **  train data set batch size 1
INFO: 2017-08-27 18:20:52: main.py:44 **  train data batch counts 5088
INFO: 2017-08-27 18:20:53: main.py:51 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 19:03:08: main.py:27 **  start
INFO: 2017-08-27 19:03:08: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 19:03:08: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 19:03:08: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 19:03:08: main.py:32 **  读取数据集...
INFO: 2017-08-27 19:03:11: main.py:43 **  train data sample counts 5088
INFO: 2017-08-27 19:03:11: main.py:44 **  train data set batch size 1
INFO: 2017-08-27 19:03:11: main.py:45 **  train data batch counts 5088
INFO: 2017-08-27 19:03:11: main.py:52 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 19:05:28: main.py:27 **  start
INFO: 2017-08-27 19:05:28: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 19:05:28: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 19:05:28: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 19:05:28: main.py:32 **  读取数据集...
INFO: 2017-08-27 19:05:31: main.py:43 **  train data sample counts 5088
INFO: 2017-08-27 19:05:31: main.py:44 **  train data set batch size 1
INFO: 2017-08-27 19:05:31: main.py:45 **  train data batch counts 5088
INFO: 2017-08-27 19:05:31: main.py:52 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 19:07:14: main.py:27 **  start
INFO: 2017-08-27 19:07:14: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 19:07:14: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 19:07:14: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 19:07:14: main.py:32 **  读取数据集...
INFO: 2017-08-27 19:07:17: main.py:43 **  train data sample counts 5088
INFO: 2017-08-27 19:07:17: main.py:44 **  train data set batch size 1
INFO: 2017-08-27 19:07:17: main.py:45 **  train data batch counts 5088
INFO: 2017-08-27 19:07:17: main.py:52 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 19:11:20: main.py:27 **  start
INFO: 2017-08-27 19:11:20: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 19:11:20: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 19:11:20: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 19:11:20: main.py:32 **  读取数据集...
INFO: 2017-08-27 19:11:23: main.py:43 **  train data sample counts 5088
INFO: 2017-08-27 19:11:23: main.py:44 **  train data set batch size 1
INFO: 2017-08-27 19:11:23: main.py:45 **  train data batch counts 5088
INFO: 2017-08-27 19:11:23: main.py:52 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 19:11:55: main.py:27 **  start
INFO: 2017-08-27 19:11:55: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 19:11:55: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 19:11:55: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 19:11:55: main.py:32 **  读取数据集...
INFO: 2017-08-27 19:11:58: main.py:43 **  train data sample counts 5088
INFO: 2017-08-27 19:11:58: main.py:44 **  train data set batch size 1
INFO: 2017-08-27 19:11:58: main.py:45 **  train data batch counts 5088
INFO: 2017-08-27 19:11:58: main.py:52 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 19:12:51: main.py:27 **  start
INFO: 2017-08-27 19:12:51: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 19:12:51: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 19:12:51: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 19:12:51: main.py:32 **  读取数据集...
INFO: 2017-08-27 19:12:54: main.py:43 **  train data sample counts 5088
INFO: 2017-08-27 19:12:54: main.py:44 **  train data set batch size 1
INFO: 2017-08-27 19:12:54: main.py:45 **  train data batch counts 5088
INFO: 2017-08-27 19:12:54: main.py:52 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 19:13:18: main.py:27 **  start
INFO: 2017-08-27 19:13:18: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 19:13:18: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 19:13:18: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 19:13:18: main.py:32 **  读取数据集...
INFO: 2017-08-27 19:13:21: main.py:43 **  train data sample counts 5088
INFO: 2017-08-27 19:13:21: main.py:44 **  train data set batch size 1
INFO: 2017-08-27 19:13:21: main.py:45 **  train data batch counts 5088
INFO: 2017-08-27 19:13:21: main.py:52 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 19:13:31: main.py:27 **  start
INFO: 2017-08-27 19:13:31: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 19:13:31: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 19:13:31: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 19:13:31: main.py:32 **  读取数据集...
INFO: 2017-08-27 19:13:34: main.py:43 **  train data sample counts 5088
INFO: 2017-08-27 19:13:34: main.py:44 **  train data set batch size 1
INFO: 2017-08-27 19:13:34: main.py:45 **  train data batch counts 5088
INFO: 2017-08-27 19:13:34: main.py:52 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 19:14:22: main.py:27 **  start
INFO: 2017-08-27 19:14:22: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 19:14:22: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 19:14:22: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 19:14:22: main.py:32 **  读取数据集...
INFO: 2017-08-27 19:14:25: main.py:43 **  train data sample counts 5088
INFO: 2017-08-27 19:14:25: main.py:44 **  train data set batch size 1
INFO: 2017-08-27 19:14:25: main.py:45 **  train data batch counts 5088
INFO: 2017-08-27 19:14:25: main.py:52 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 19:15:07: main.py:27 **  start
INFO: 2017-08-27 19:15:07: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 19:15:07: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 19:15:07: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 19:15:07: main.py:32 **  读取数据集...
INFO: 2017-08-27 19:15:10: main.py:43 **  train data sample counts 5088
INFO: 2017-08-27 19:15:10: main.py:44 **  train data set batch size 1
INFO: 2017-08-27 19:15:10: main.py:45 **  train data batch counts 5088
INFO: 2017-08-27 19:15:10: main.py:52 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 19:16:21: main.py:27 **  start
INFO: 2017-08-27 19:16:21: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 19:16:21: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 19:16:21: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 19:16:21: main.py:32 **  读取数据集...
INFO: 2017-08-27 19:16:24: main.py:43 **  train data sample counts 5088
INFO: 2017-08-27 19:16:24: main.py:44 **  train data set batch size 1
INFO: 2017-08-27 19:16:24: main.py:45 **  train data batch counts 5088
INFO: 2017-08-27 19:16:24: main.py:52 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 19:21:35: main.py:27 **  start
INFO: 2017-08-27 19:21:35: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 19:21:35: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 19:21:35: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 19:21:35: main.py:32 **  读取数据集...
INFO: 2017-08-27 19:21:38: main.py:43 **  train data sample counts 5088
INFO: 2017-08-27 19:21:38: main.py:44 **  train data set batch size 1
INFO: 2017-08-27 19:21:38: main.py:45 **  train data batch counts 5088
INFO: 2017-08-27 19:21:38: main.py:52 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 19:22:52: main.py:27 **  start
INFO: 2017-08-27 19:22:52: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 19:22:52: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 19:22:52: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 19:22:52: main.py:32 **  读取数据集...
INFO: 2017-08-27 19:22:55: main.py:43 **  train data sample counts 5088
INFO: 2017-08-27 19:22:55: main.py:44 **  train data set batch size 1
INFO: 2017-08-27 19:22:55: main.py:45 **  train data batch counts 5088
INFO: 2017-08-27 19:22:55: main.py:52 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 19:24:50: main.py:27 **  start
INFO: 2017-08-27 19:24:50: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 19:24:50: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 19:24:50: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 19:24:50: main.py:32 **  读取数据集...
INFO: 2017-08-27 19:24:53: main.py:43 **  train data sample counts 5088
INFO: 2017-08-27 19:24:53: main.py:44 **  train data set batch size 1
INFO: 2017-08-27 19:24:53: main.py:45 **  train data batch counts 5088
INFO: 2017-08-27 19:24:53: main.py:52 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 19:25:02: main.py:27 **  start
INFO: 2017-08-27 19:25:02: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 19:25:02: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 19:25:02: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 19:25:02: main.py:32 **  读取数据集...
INFO: 2017-08-27 19:25:05: main.py:43 **  train data sample counts 5088
INFO: 2017-08-27 19:25:05: main.py:44 **  train data set batch size 1
INFO: 2017-08-27 19:25:05: main.py:45 **  train data batch counts 5088
INFO: 2017-08-27 19:25:05: main.py:52 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 19:25:55: main.py:27 **  start
INFO: 2017-08-27 19:25:55: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 19:25:55: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 19:25:55: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 19:25:55: main.py:32 **  读取数据集...
INFO: 2017-08-27 19:25:58: main.py:43 **  train data sample counts 5088
INFO: 2017-08-27 19:25:58: main.py:44 **  train data set batch size 1
INFO: 2017-08-27 19:25:58: main.py:45 **  train data batch counts 5088
INFO: 2017-08-27 19:25:58: main.py:52 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 19:27:01: main.py:27 **  start
INFO: 2017-08-27 19:27:01: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 19:27:01: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 19:27:01: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 19:27:01: main.py:32 **  读取数据集...
INFO: 2017-08-27 19:27:04: main.py:43 **  train data sample counts 5088
INFO: 2017-08-27 19:27:04: main.py:44 **  train data set batch size 1
INFO: 2017-08-27 19:27:04: main.py:45 **  train data batch counts 5088
INFO: 2017-08-27 19:27:04: main.py:52 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 19:27:45: main.py:27 **  start
INFO: 2017-08-27 19:27:45: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 19:27:45: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 19:27:45: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 19:27:45: main.py:32 **  读取数据集...
INFO: 2017-08-27 19:27:48: main.py:43 **  train data sample counts 5088
INFO: 2017-08-27 19:27:48: main.py:44 **  train data set batch size 1
INFO: 2017-08-27 19:27:48: main.py:45 **  train data batch counts 5088
INFO: 2017-08-27 19:27:48: main.py:52 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 19:30:21: main.py:27 **  start
INFO: 2017-08-27 19:30:21: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 19:30:21: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 19:30:21: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 19:30:21: main.py:32 **  读取数据集...
INFO: 2017-08-27 19:30:24: main.py:43 **  train data sample counts 5088
INFO: 2017-08-27 19:30:24: main.py:44 **  train data set batch size 1
INFO: 2017-08-27 19:30:24: main.py:45 **  train data batch counts 5088
INFO: 2017-08-27 19:30:24: main.py:52 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 19:31:29: main.py:27 **  start
INFO: 2017-08-27 19:31:29: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 19:31:29: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 19:31:29: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 19:31:29: main.py:32 **  读取数据集...
INFO: 2017-08-27 19:31:32: main.py:43 **  train data sample counts 5088
INFO: 2017-08-27 19:31:32: main.py:44 **  train data set batch size 1
INFO: 2017-08-27 19:31:32: main.py:45 **  train data batch counts 5088
INFO: 2017-08-27 19:31:33: main.py:52 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 19:32:18: main.py:27 **  start
INFO: 2017-08-27 19:32:18: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 19:32:18: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 19:32:18: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 19:32:18: main.py:32 **  读取数据集...
INFO: 2017-08-27 19:32:21: main.py:43 **  train data sample counts 5088
INFO: 2017-08-27 19:32:21: main.py:44 **  train data set batch size 1
INFO: 2017-08-27 19:32:21: main.py:45 **  train data batch counts 5088
INFO: 2017-08-27 19:32:21: main.py:52 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 19:35:58: main.py:27 **  start
INFO: 2017-08-27 19:35:58: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 19:35:58: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 19:35:58: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 19:35:58: main.py:32 **  读取数据集...
INFO: 2017-08-27 19:36:01: main.py:43 **  train data sample counts 5088
INFO: 2017-08-27 19:36:01: main.py:44 **  train data set batch size 1
INFO: 2017-08-27 19:36:01: main.py:45 **  train data batch counts 5088
INFO: 2017-08-27 19:36:01: main.py:52 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 19:38:02: main.py:27 **  start
INFO: 2017-08-27 19:38:02: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 19:38:02: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 19:38:02: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 19:38:02: main.py:32 **  读取数据集...
INFO: 2017-08-27 19:38:05: main.py:43 **  train data sample counts 5088
INFO: 2017-08-27 19:38:05: main.py:44 **  train data set batch size 1
INFO: 2017-08-27 19:38:05: main.py:45 **  train data batch counts 5088
INFO: 2017-08-27 19:38:06: main.py:52 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 19:42:59: main.py:27 **  start
INFO: 2017-08-27 19:42:59: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 19:42:59: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 19:42:59: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 19:42:59: main.py:32 **  读取数据集...
INFO: 2017-08-27 19:43:02: main.py:43 **  train data sample counts 5088
INFO: 2017-08-27 19:43:02: main.py:44 **  train data set batch size 1
INFO: 2017-08-27 19:43:02: main.py:45 **  train data batch counts 5088
INFO: 2017-08-27 19:43:02: main.py:52 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 19:44:19: main.py:27 **  start
INFO: 2017-08-27 19:44:19: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 19:44:19: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 19:44:19: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 19:44:19: main.py:32 **  读取数据集...
INFO: 2017-08-27 19:44:22: main.py:43 **  train data sample counts 5088
INFO: 2017-08-27 19:44:22: main.py:44 **  train data set batch size 1
INFO: 2017-08-27 19:44:22: main.py:45 **  train data batch counts 5088
INFO: 2017-08-27 19:44:22: main.py:52 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 19:44:27: main.py:82 **  0 0 1.22753500938 0.00594238284975
INFO: 2017-08-27 19:44:48: main.py:82 **  0 0 2.5110886097 0.019678067416
INFO: 2017-08-27 19:45:32: main.py:27 **  start
INFO: 2017-08-27 19:45:32: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 19:45:32: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 19:45:32: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 19:45:32: main.py:32 **  读取数据集...
INFO: 2017-08-27 19:45:35: main.py:43 **  train data sample counts 5088
INFO: 2017-08-27 19:45:35: main.py:44 **  train data set batch size 1
INFO: 2017-08-27 19:45:35: main.py:45 **  train data batch counts 5088
INFO: 2017-08-27 19:45:35: main.py:52 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 19:46:04: main.py:27 **  start
INFO: 2017-08-27 19:46:04: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 19:46:04: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 19:46:04: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 19:46:04: main.py:32 **  读取数据集...
INFO: 2017-08-27 19:46:07: main.py:43 **  train data sample counts 5088
INFO: 2017-08-27 19:46:07: main.py:44 **  train data set batch size 1
INFO: 2017-08-27 19:46:07: main.py:45 **  train data batch counts 5088
INFO: 2017-08-27 19:46:07: main.py:52 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 19:47:21: main.py:27 **  start
INFO: 2017-08-27 19:47:21: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 19:47:21: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 19:47:21: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 19:47:21: main.py:32 **  读取数据集...
INFO: 2017-08-27 19:47:24: main.py:43 **  train data sample counts 5088
INFO: 2017-08-27 19:47:24: main.py:44 **  train data set batch size 1
INFO: 2017-08-27 19:47:24: main.py:45 **  train data batch counts 5088
INFO: 2017-08-27 19:47:24: main.py:52 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 19:47:58: main.py:27 **  start
INFO: 2017-08-27 19:47:58: main.py:29 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-27 19:47:58: main.py:30 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-27 19:47:58: main.py:31 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-27 19:47:58: main.py:32 **  读取数据集...
INFO: 2017-08-27 19:48:01: main.py:43 **  train data sample counts 5088
INFO: 2017-08-27 19:48:01: main.py:44 **  train data set batch size 1
INFO: 2017-08-27 19:48:01: main.py:45 **  train data batch counts 5088
INFO: 2017-08-27 19:48:03: main.py:52 **  <class 'net.net.CarUNet'>


INFO: 2017-08-27 19:48:03: main.py:83 **  0 0 1.76501321793 0.0980609878898
INFO: 2017-08-27 19:48:03: main.py:83 **  0 0 2.66560745239 1.02500178665
INFO: 2017-08-27 19:48:04: main.py:83 **  0 0 4.17450857162 1.35956468433
INFO: 2017-08-27 19:48:04: main.py:83 **  0 0 5.12652426958 2.23565717787
INFO: 2017-08-27 19:48:04: main.py:83 **  0 0 6.30864244699 2.88638510555
INFO: 2017-08-27 19:48:04: main.py:83 **  0 0 7.32735484838 3.69174893945
INFO: 2017-08-27 19:48:04: main.py:83 **  0 0 8.88593930006 3.9571614787
INFO: 2017-08-27 19:48:04: main.py:83 **  0 0 10.1958019137 4.4570613429
INFO: 2017-08-27 19:48:05: main.py:83 **  0 0 11.3549708724 5.10491495579
INFO: 2017-08-27 19:48:05: main.py:83 **  0 0 12.3481149077 5.94963752478
INFO: 2017-08-27 19:48:05: main.py:83 **  0 0 13.4620928168 6.66804807633
INFO: 2017-08-27 19:48:05: main.py:83 **  0 0 14.5870227218 7.37826132029
INFO: 2017-08-27 19:48:05: main.py:83 **  0 0 16.2341784835 7.5392415002
INFO: 2017-08-27 19:48:05: main.py:83 **  0 0 18.0113481879 7.59086010978
INFO: 2017-08-27 19:48:05: main.py:83 **  0 0 19.2368437648 8.11985991523
INFO: 2017-08-27 19:48:06: main.py:83 **  0 0 20.3565974832 8.81844673678
INFO: 2017-08-27 19:48:06: main.py:83 **  0 0 21.4068650603 9.54220752046
INFO: 2017-08-27 19:48:06: main.py:83 **  0 0 22.5170368552 10.1786815934
INFO: 2017-08-27 19:48:06: main.py:83 **  0 0 23.5686610341 10.9181557111
INFO: 2017-08-27 19:48:06: main.py:83 **  0 0 24.774178803 11.4933206849
INFO: 2017-08-27 19:48:06: main.py:83 **  0 0 25.6720414758 12.429133635
INFO: 2017-08-27 19:48:06: main.py:83 **  0 0 26.8659012914 12.9226134829
INFO: 2017-08-27 19:48:07: main.py:83 **  0 0 28.4253411889 13.129372742
INFO: 2017-08-27 19:48:07: main.py:83 **  0 0 29.4142834544 13.912401285
INFO: 2017-08-27 19:48:07: main.py:83 **  0 0 30.3505343795 14.787590947
INFO: 2017-08-27 19:48:07: main.py:83 **  0 0 31.2933796048 15.6849150322
INFO: 2017-08-27 19:48:07: main.py:83 **  0 0 32.3883581758 16.3103294633
INFO: 2017-08-27 19:48:07: main.py:83 **  0 0 33.4068191648 17.0452246331
INFO: 2017-08-27 19:48:07: main.py:83 **  0 0 34.9628939033 17.2263154052
INFO: 2017-08-27 19:48:08: main.py:83 **  0 0 35.8321319222 18.1746066473
INFO: 2017-08-27 19:48:08: main.py:83 **  0 0 37.1810930371 18.48119184
INFO: 2017-08-27 19:48:08: main.py:83 **  0 0 38.3574470878 18.9628760777
INFO: 2017-08-27 19:48:08: main.py:83 **  0 0 39.2832380533 19.809070047
INFO: 2017-08-27 19:48:08: main.py:83 **  0 0 40.3551920652 20.4861307703
INFO: 2017-08-27 19:48:08: main.py:83 **  0 0 41.3620399237 21.1499933563
INFO: 2017-08-27 19:48:08: main.py:83 **  0 0 42.6620186567 21.4694807492
INFO: 2017-08-27 19:48:09: main.py:83 **  0 0 43.8789145947 22.1026264392
INFO: 2017-08-27 19:48:09: main.py:83 **  0 0 45.7653803825 22.103480573
INFO: 2017-08-27 19:48:09: main.py:83 **  0 0 46.9925096035 22.7101904109
INFO: 2017-08-27 19:48:09: main.py:83 **  0 0 47.8852418661 23.4771664694
INFO: 2017-08-27 19:48:09: main.py:83 **  0 0 48.9939879179 24.115322943
INFO: 2017-08-27 19:48:09: main.py:83 **  0 0 50.4117230177 24.495211567
INFO: 2017-08-27 19:48:09: main.py:83 **  0 0 51.3440854549 25.4531052962
INFO: 2017-08-27 19:48:10: main.py:83 **  0 0 52.2958418727 26.2359222427
INFO: 2017-08-27 19:48:10: main.py:83 **  0 0 53.2659662366 27.1311057463
INFO: 2017-08-27 19:48:10: main.py:83 **  0 0 54.2043486238 28.0608501688
INFO: 2017-08-27 19:48:10: main.py:83 **  0 0 55.7695310712 28.3765197053
INFO: 2017-08-27 19:48:10: main.py:83 **  0 0 57.3507118821 28.6094614819
INFO: 2017-08-27 19:48:10: main.py:83 **  0 0 58.2296192646 29.3764578059
INFO: 2017-08-27 19:48:11: main.py:83 **  0 0 59.2905619144 30.0320771292
INFO: 2017-08-27 19:48:11: main.py:83 **  0 0 60.2247442603 30.8251808837
INFO: 2017-08-27 19:48:11: main.py:83 **  0 0 62.0134561658 30.8252031339
INFO: 2017-08-27 19:48:11: main.py:83 **  0 0 62.9514421821 31.6846191418
INFO: 2017-08-27 19:48:11: main.py:83 **  0 0 63.9721965194 32.4548323882
INFO: 2017-08-27 19:48:11: main.py:83 **  0 0 64.8318282962 33.3586439026
INFO: 2017-08-27 19:48:11: main.py:83 **  0 0 65.7837551236 34.0994011891
INFO: 2017-08-27 19:48:12: main.py:83 **  0 0 66.6321522593 34.9762489927
INFO: 2017-08-27 19:48:12: main.py:83 **  0 0 67.609646678 35.6691850555
INFO: 2017-08-27 19:48:12: main.py:83 **  0 0 68.6399232149 36.3565171492
INFO: 2017-08-27 19:48:12: main.py:83 **  0 0 69.7850040197 37.0035344494
INFO: 2017-08-27 19:48:12: main.py:83 **  0 0 71.4964139462 37.1050809306
INFO: 2017-08-27 19:48:12: main.py:83 **  0 0 72.5386614799 37.7625521702
INFO: 2017-08-27 19:48:12: main.py:83 **  0 0 73.6787405014 38.2828547639
INFO: 2017-08-27 19:48:13: main.py:83 **  0 0 74.6384772062 38.9930759353
INFO: 2017-08-27 19:48:13: main.py:83 **  0 0 75.622563839 39.6694588346
INFO: 2017-08-27 19:48:13: main.py:83 **  0 0 76.7700200081 40.3033662123
INFO: 2017-08-27 19:48:13: main.py:83 **  0 0 78.1196095943 40.6084741038
INFO: 2017-08-27 19:48:13: main.py:83 **  0 0 79.2029340267 41.1908761543
INFO: 2017-08-27 19:48:13: main.py:83 **  0 0 80.4300453663 41.5477143807
INFO: 2017-08-27 19:48:13: main.py:83 **  0 0 81.4415464401 42.2671293897
INFO: 2017-08-27 19:48:14: main.py:83 **  0 0 82.4189118743 43.0829959792
INFO: 2017-08-27 19:48:14: main.py:83 **  0 0 83.2454262376 43.9645127935
INFO: 2017-08-27 19:48:14: main.py:83 **  0 0 84.2149386406 44.7543431801
INFO: 2017-08-27 19:48:14: main.py:83 **  0 0 85.998095274 44.7543603884
INFO: 2017-08-27 19:48:14: main.py:83 **  0 0 86.7542327642 45.6555356489
INFO: 2017-08-27 19:48:14: main.py:83 **  0 0 88.2135082483 46.0671757268
INFO: 2017-08-27 19:48:14: main.py:83 **  0 0 89.0817260146 46.9199051665
INFO: 2017-08-27 19:48:15: main.py:83 **  0 0 90.3416662812 47.5412425803
INFO: 2017-08-27 19:48:15: main.py:83 **  0 0 91.1275610328 48.3872526096
INFO: 2017-08-27 19:48:15: main.py:83 **  0 0 92.1148610711 49.1270251678
INFO: 2017-08-27 19:48:15: main.py:83 **  0 0 92.9891538024 49.7873139189
INFO: 2017-08-27 19:48:15: main.py:83 **  0 0 93.8129800558 50.53108577
INFO: 2017-08-27 19:48:15: main.py:83 **  0 0 94.7010918856 51.2418254422
INFO: 2017-08-27 19:48:15: main.py:83 **  0 0 95.7694853544 51.9805644559
INFO: 2017-08-27 19:48:16: main.py:83 **  0 0 96.4677290916 52.9001009868
INFO: 2017-08-27 19:48:16: main.py:83 **  0 0 98.3403863907 52.9001143746
INFO: 2017-08-27 19:48:16: main.py:83 **  0 0 99.4970180988 53.5981784634
INFO: 2017-08-27 19:48:16: main.py:83 **  0 0 100.273283601 54.4783653192
INFO: 2017-08-27 19:48:16: main.py:83 **  0 0 101.216529012 55.3015758567
INFO: 2017-08-27 19:48:16: main.py:83 **  0 0 102.119017661 56.0894417338
INFO: 2017-08-27 19:48:16: main.py:83 **  0 0 102.876995444 56.8476889901
INFO: 2017-08-27 19:48:17: main.py:83 **  0 0 104.220059276 57.261339473
INFO: 2017-08-27 19:48:17: main.py:83 **  0 0 105.281593442 57.8178646438
INFO: 2017-08-27 19:48:17: main.py:83 **  0 0 106.957246661 58.0005934112
INFO: 2017-08-27 19:48:17: main.py:83 **  0 0 107.970118642 58.6348044746
INFO: 2017-08-27 19:48:17: main.py:83 **  0 0 108.759764194 59.4655564301
INFO: 2017-08-27 19:48:17: main.py:83 **  0 0 109.528581262 60.3252767198
INFO: 2017-08-27 19:48:18: main.py:83 **  0 0 110.212380171 61.1897442214
INFO: 2017-08-27 19:48:18: main.py:83 **  0 0 112.030937433 61.277699808
INFO: 2017-08-27 19:48:18: main.py:83 **  0 0 113.07302928 61.9655402698
INFO: 2017-08-27 19:48:18: main.py:83 **  0 0 114.095581293 62.6498274602
INFO: 2017-08-27 19:48:18: main.py:83 **  0 0 115.17635107 63.4059340753
INFO: 2017-08-27 19:48:18: main.py:83 **  0 0 115.953304052 64.2262833513
INFO: 2017-08-27 19:48:18: main.py:83 **  0 0 117.384321213 64.8064657845
INFO: 2017-08-27 19:48:19: main.py:83 **  0 0 118.0645051 65.7180445589
INFO: 2017-08-27 19:48:19: main.py:83 **  0 0 119.076185584 66.5057847299
INFO: 2017-08-27 19:48:19: main.py:83 **  0 0 120.005445957 67.2684328355
INFO: 2017-08-27 19:48:19: main.py:83 **  0 0 120.984352767 67.9549121656
INFO: 2017-08-27 19:48:19: main.py:83 **  0 0 121.895070672 68.7299790777
INFO: 2017-08-27 19:48:19: main.py:83 **  0 0 122.980865717 69.297043363
INFO: 2017-08-27 19:48:19: main.py:83 **  0 0 124.214713454 69.8848163284
INFO: 2017-08-27 19:48:20: main.py:83 **  0 0 125.32669127 70.5697506465
INFO: 2017-08-27 19:48:20: main.py:83 **  0 0 126.551303864 71.2697673835
INFO: 2017-08-27 19:48:20: main.py:83 **  0 0 127.691073179 71.8054131664
INFO: 2017-08-27 19:48:20: main.py:83 **  0 0 128.230755687 72.755342523
INFO: 2017-08-27 19:48:20: main.py:83 **  0 0 129.694198728 73.0548045434
INFO: 2017-08-27 19:48:20: main.py:83 **  0 0 130.486017346 73.8928079404
INFO: 2017-08-27 19:48:20: main.py:83 **  0 0 131.649000525 74.6323162712
INFO: 2017-08-27 19:48:21: main.py:83 **  0 0 132.689797044 75.2700975336
INFO: 2017-08-27 19:48:21: main.py:83 **  0 0 133.34840703 76.1870074071
INFO: 2017-08-27 19:48:21: main.py:83 **  0 0 135.038976192 76.3648246475
INFO: 2017-08-27 19:48:21: main.py:83 **  0 0 135.91408205 77.1651827641
INFO: 2017-08-27 19:48:21: main.py:83 **  0 0 136.806451321 77.8905475683
INFO: 2017-08-27 19:48:21: main.py:83 **  0 0 137.686613858 78.719902987
INFO: 2017-08-28 23:51:17: main.py:28 **  start
INFO: 2017-08-28 23:51:17: main.py:30 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-28 23:51:17: main.py:31 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-28 23:51:17: main.py:32 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-28 23:51:17: main.py:33 **  读取数据集...
INFO: 2017-08-28 23:51:20: main.py:44 **  train data sample counts 5088
INFO: 2017-08-28 23:51:20: main.py:45 **  train data set batch size 1
INFO: 2017-08-28 23:51:20: main.py:46 **  train data batch counts 5088
INFO: 2017-08-28 23:51:22: main.py:53 **  <class 'net.net.CarUNet'>


INFO: 2017-08-28 23:51:22: main.py:58 **  epoch    iter      rate   | smooth_loss/acc | train_loss/acc | valid_loss/acc ... 

INFO: 2017-08-28 23:51:22: main.py:59 **  --------------------------------------------------------------------------------------------------

INFO: 2017-08-28 23:51:23: main.py:87 **  0 0 0 1.81643462181 7.75151693233e-06
INFO: 2017-08-28 23:51:23: main.py:87 **  0 1 0 1.60399669409 0.209246860411
INFO: 2017-08-28 23:51:24: main.py:87 **  0 2 0 1.51529244582 0.298118850566
INFO: 2017-08-28 23:51:24: main.py:87 **  0 3 0 1.39114806056 0.430925578607
INFO: 2017-08-28 23:51:24: main.py:87 **  0 4 0 1.36836202145 0.451919162856
INFO: 2017-08-28 23:51:24: main.py:87 **  0 5 0 1.34676396847 0.472534209975
INFO: 2017-08-28 23:51:24: main.py:87 **  0 6 0 1.33776909964 0.480497411735
INFO: 2017-08-28 23:51:24: main.py:87 **  0 7 0 1.29021725804 0.531504102415
INFO: 2017-08-28 23:51:24: main.py:87 **  0 8 0 1.2633724014 0.559028625812
INFO: 2017-08-28 23:51:25: main.py:87 **  0 9 0 1.22256546021 0.601667571359
INFO: 2017-08-28 23:51:25: main.py:87 **  0 10 0 1.20932249589 0.616461613226
INFO: 2017-08-28 23:51:25: main.py:87 **  0 11 0 1.2459085385 0.580733577653
INFO: 2017-08-28 23:51:25: main.py:87 **  0 12 0 1.27718513746 0.549363795605
INFO: 2017-08-28 23:51:25: main.py:87 **  0 13 0 1.29459546294 0.531134027664
INFO: 2017-08-28 23:51:25: main.py:87 **  0 14 0 1.30807723999 0.51595388571
INFO: 2017-08-28 23:51:25: main.py:87 **  0 15 0 1.31626701355 0.50526771992
INFO: 2017-08-28 23:51:26: main.py:87 **  0 16 0 1.34356319203 0.478479023975
INFO: 2017-08-28 23:51:26: main.py:87 **  0 17 0 1.32611634996 0.495768076592
INFO: 2017-08-28 23:51:26: main.py:87 **  0 18 0 1.32876851684 0.489790709156
INFO: 2017-08-28 23:51:26: main.py:87 **  0 19 0 1.31984241009 0.497897016775
INFO: 2017-08-28 23:51:26: main.py:87 **  0 20 0 1.32547496614 0.489986324378
INFO: 2017-08-28 23:51:26: main.py:87 **  0 21 0 1.34367110512 0.469592877711
INFO: 2017-08-28 23:51:26: main.py:87 **  0 22 0 1.34486746788 0.467110428114
INFO: 2017-08-28 23:51:27: main.py:87 **  0 23 0 1.36138647298 0.447647983077
INFO: 2017-08-28 23:51:27: main.py:87 **  0 24 0 1.35579810619 0.459408802264
INFO: 2017-08-28 23:51:27: main.py:87 **  0 25 0 1.35915172559 0.459041057177
INFO: 2017-08-28 23:51:27: main.py:87 **  0 26 0 1.3469911593 0.469132664462
INFO: 2017-08-28 23:51:27: main.py:87 **  0 27 0 1.3361871498 0.481006688725
INFO: 2017-08-28 23:51:27: main.py:87 **  0 28 0 1.33257944008 0.490780510156
INFO: 2017-08-28 23:51:27: main.py:87 **  0 29 0 1.34199174245 0.477891969079
INFO: 2017-08-28 23:51:28: main.py:87 **  0 30 0 1.33365240405 0.48405131351
INFO: 2017-08-28 23:51:28: main.py:87 **  0 31 0 1.33603223786 0.482495746011
INFO: 2017-08-28 23:51:28: main.py:87 **  0 32 0 1.33354919246 0.489876872115
INFO: 2017-08-28 23:51:28: main.py:87 **  0 33 0 1.32860013667 0.490779421254
INFO: 2017-08-28 23:51:28: main.py:87 **  0 34 0 1.32213193689 0.494254826132
INFO: 2017-08-28 23:51:28: main.py:87 **  0 35 0 1.31885338161 0.504279790979
INFO: 2017-08-28 23:51:28: main.py:87 **  0 36 0 1.32647610999 0.490651703804
INFO: 2017-08-28 23:51:29: main.py:87 **  0 37 0 1.31964461113 0.495057144876
INFO: 2017-08-28 23:51:29: main.py:87 **  0 38 0 1.32131764522 0.492753402198
INFO: 2017-08-28 23:51:29: main.py:87 **  0 39 0 1.31787222922 0.500991020013
INFO: 2017-08-28 23:51:29: main.py:87 **  0 40 0 1.31780349336 0.495366025659
INFO: 2017-08-28 23:51:29: main.py:87 **  0 41 0 1.31272418158 0.503528731597
INFO: 2017-08-28 23:51:29: main.py:87 **  0 42 0 1.30623925564 0.503841823302
INFO: 2017-08-28 23:51:30: main.py:87 **  0 43 0 1.30223805796 0.502456789359
INFO: 2017-08-28 23:51:30: main.py:87 **  0 44 0 1.29811171691 0.506938441294
INFO: 2017-08-28 23:51:30: main.py:87 **  0 45 0 1.30054178186 0.501882255431
INFO: 2017-08-28 23:51:30: main.py:87 **  0 46 0 1.30228063908 0.497396364203
INFO: 2017-08-28 23:51:30: main.py:87 **  0 47 0 1.30062737564 0.501070548188
INFO: 2017-08-28 23:51:30: main.py:87 **  0 48 0 1.29661918173 0.501582116829
INFO: 2017-08-28 23:51:30: main.py:87 **  0 49 0 1.2923755908 0.508805402843
INFO: 2017-08-28 23:51:31: main.py:87 **  0 50 0 1.28662985914 0.511600232911
INFO: 2017-08-28 23:51:31: main.py:87 **  0 51 0 1.28321072688 0.512628783713
INFO: 2017-08-28 23:51:31: main.py:87 **  0 52 0 1.2838442483 0.51167555156
INFO: 2017-08-28 23:51:31: main.py:87 **  0 53 0 1.28130706593 0.512513323264
INFO: 2017-08-28 23:51:31: main.py:87 **  0 54 0 1.2788835027 0.516561861767
INFO: 2017-08-28 23:51:31: main.py:87 **  0 55 0 1.27497983617 0.519480969515
INFO: 2017-08-28 23:51:31: main.py:87 **  0 56 0 1.2676190598 0.524637114869
INFO: 2017-08-28 23:51:32: main.py:87 **  0 57 0 1.26085141815 0.530825504222
INFO: 2017-08-28 23:51:32: main.py:87 **  0 58 0 1.25396739224 0.535938637007
INFO: 2017-08-28 23:51:32: main.py:87 **  0 59 0 1.24726715883 0.542177942666
INFO: 2017-08-28 23:51:32: main.py:87 **  0 60 0 1.24631863735 0.543007638877
INFO: 2017-08-28 23:51:32: main.py:87 **  0 61 0 1.23867819194 0.54962768504
INFO: 2017-08-28 23:51:32: main.py:87 **  0 62 0 1.23774939681 0.550146386049
INFO: 2017-08-28 23:51:32: main.py:87 **  0 63 0 1.23717408068 0.551296541722
INFO: 2017-08-28 23:51:33: main.py:87 **  0 64 0 1.22902797552 0.557324880153
INFO: 2017-08-28 23:51:33: main.py:87 **  0 65 0 1.2257239638 0.558315437195
INFO: 2017-08-28 23:51:33: main.py:87 **  0 66 0 1.22161091442 0.560071860919
INFO: 2017-08-28 23:51:33: main.py:87 **  0 67 0 1.21489048004 0.564194819746
INFO: 2017-08-28 23:51:33: main.py:87 **  0 68 0 1.21263825375 0.565572528971
INFO: 2017-08-28 23:51:33: main.py:87 **  0 69 0 1.20599640267 0.56968429793
INFO: 2017-08-28 23:51:33: main.py:87 **  0 70 0 1.20557920698 0.570144082688
INFO: 2017-08-28 23:51:34: main.py:87 **  0 71 0 1.20306276613 0.571872440703
INFO: 2017-08-28 23:51:34: main.py:87 **  0 72 0 1.1955384408 0.575605590272
INFO: 2017-08-28 23:51:34: main.py:87 **  0 73 0 1.18876854793 0.579841801663
INFO: 2017-08-28 23:51:34: main.py:87 **  0 74 0 1.18595609268 0.580588360844
INFO: 2017-08-28 23:51:34: main.py:87 **  0 75 0 1.1854276712 0.580426061054
INFO: 2017-08-28 23:51:34: main.py:87 **  0 76 0 1.18291817934 0.581084333668
INFO: 2017-08-28 23:51:34: main.py:87 **  0 77 0 1.18320830434 0.580572256884
INFO: 2017-08-28 23:51:35: main.py:87 **  0 78 0 1.18664043538 0.578813154692
INFO: 2017-08-28 23:51:35: main.py:87 **  0 79 0 1.18572105095 0.579398800636
INFO: 2017-08-28 23:51:35: main.py:87 **  0 80 0 1.18378387042 0.580733317117
INFO: 2017-08-28 23:51:35: main.py:87 **  0 81 0 1.1795213695 0.583592294391
INFO: 2017-08-28 23:51:35: main.py:87 **  0 82 0 1.18933117031 0.576561322396
INFO: 2017-08-28 23:51:35: main.py:87 **  0 83 0 1.18481835084 0.578061101675
INFO: 2017-08-28 23:51:35: main.py:87 **  0 84 0 1.18634620484 0.57789306078
INFO: 2017-08-28 23:51:36: main.py:87 **  0 85 0 1.18195087619 0.578794617276
INFO: 2017-08-28 23:51:36: main.py:87 **  0 86 0 1.17926121032 0.578040754642
INFO: 2017-08-28 23:51:36: main.py:87 **  0 87 0 1.17934396863 0.577265811222
INFO: 2017-08-28 23:51:36: main.py:87 **  0 88 0 1.18794481674 0.571368880052
INFO: 2017-08-28 23:51:36: main.py:87 **  0 89 0 1.18764741156 0.5706414735
INFO: 2017-08-28 23:51:36: main.py:87 **  0 90 0 1.18422820411 0.570963284047
INFO: 2017-08-28 23:51:37: main.py:87 **  0 91 0 1.18216195897 0.569800935589
INFO: 2017-08-28 23:51:37: main.py:87 **  0 92 0 1.18315271985 0.568672064396
INFO: 2017-08-28 23:51:37: main.py:87 **  0 93 0 1.19144343125 0.56280869137
INFO: 2017-08-28 23:51:37: main.py:87 **  0 94 0 1.19917740885 0.556884704605
INFO: 2017-08-28 23:51:37: main.py:87 **  0 95 0 1.19450315585 0.558379042114
INFO: 2017-08-28 23:51:37: main.py:87 **  0 96 0 1.19465472772 0.558132439387
INFO: 2017-08-28 23:51:37: main.py:87 **  0 97 0 1.19055288179 0.561368970083
INFO: 2017-08-28 23:51:38: main.py:87 **  0 98 0 1.18610905337 0.563192302376
INFO: 2017-08-28 23:51:38: main.py:87 **  0 99 0 1.1903579253 0.561166169193
INFO: 2017-08-28 23:51:38: main.py:87 **  0 100 0 1.19332818407 0.556957085383
INFO: 2017-08-28 23:51:38: main.py:87 **  0 101 0 1.1910668594 0.558168659458
INFO: 2017-08-28 23:51:38: main.py:87 **  0 102 0 1.19670766592 0.554012955389
INFO: 2017-08-28 23:51:38: main.py:87 **  0 103 0 1.19437659876 0.554622517752
INFO: 2017-08-28 23:51:38: main.py:87 **  0 104 0 1.20078461454 0.549340628932
INFO: 2017-08-28 23:51:39: main.py:87 **  0 105 0 1.19699718761 0.551850886763
INFO: 2017-08-28 23:51:39: main.py:87 **  0 106 0 1.19394359466 0.554450597085
INFO: 2017-08-28 23:51:39: main.py:87 **  0 107 0 1.18995527813 0.55737934392
INFO: 2017-08-28 23:51:39: main.py:87 **  0 108 0 1.18501460935 0.560587413739
INFO: 2017-08-28 23:51:39: main.py:87 **  0 109 0 1.18662557439 0.559520957612
INFO: 2017-08-28 23:51:39: main.py:87 **  0 110 0 1.18787476447 0.559555606189
INFO: 2017-08-28 23:51:39: main.py:87 **  0 111 0 1.18780179588 0.559125447839
INFO: 2017-08-28 23:51:40: main.py:87 **  0 112 0 1.18525618631 0.55967411207
INFO: 2017-08-28 23:51:40: main.py:87 **  0 113 0 1.19030480688 0.554764936808
INFO: 2017-08-28 23:51:40: main.py:87 **  0 114 0 1.18690737486 0.556319091031
INFO: 2017-08-28 23:51:40: main.py:87 **  0 115 0 1.19145856387 0.551523574944
INFO: 2017-08-28 23:51:40: main.py:87 **  0 116 0 1.19568108341 0.546929761277
INFO: 2017-08-28 23:51:40: main.py:87 **  0 117 0 1.19893485156 0.545364898032
INFO: 2017-08-28 23:51:40: main.py:87 **  0 118 0 1.19933427432 0.543954522563
INFO: 2017-08-28 23:51:41: main.py:87 **  0 119 0 1.20085980942 0.543896579637
INFO: 2017-08-28 23:51:41: main.py:87 **  0 120 0 1.20120746685 0.543427987427
INFO: 2017-08-28 23:51:41: main.py:87 **  0 121 0 1.19826083447 0.544006332895
INFO: 2017-08-28 23:51:41: main.py:87 **  0 122 0 1.19789271868 0.544136029815
INFO: 2017-08-28 23:51:41: main.py:87 **  0 123 0 1.19676258727 0.545550908671
INFO: 2017-08-28 23:51:41: main.py:87 **  0 124 0 1.19423804569 0.54686103
INFO: 2017-08-28 23:51:41: main.py:87 **  0 125 0 1.19401289262 0.546915231615
INFO: 2017-08-28 23:51:42: main.py:87 **  0 126 0 1.19054990819 0.548600517289
INFO: 2017-08-28 23:51:42: main.py:87 **  0 127 0 1.19467715872 0.544314761041
INFO: 2017-08-28 23:51:42: main.py:87 **  0 128 0 1.19594845245 0.54347572895
INFO: 2017-08-28 23:51:42: main.py:87 **  0 129 0 1.19358026248 0.544907246049
INFO: 2017-08-28 23:51:42: main.py:87 **  0 130 0 1.19423527208 0.543686369539
INFO: 2017-08-28 23:51:42: main.py:87 **  0 131 0 1.19113861611 0.546244503513
INFO: 2017-08-28 23:51:42: main.py:87 **  0 132 0 1.19093302856 0.546642274874
INFO: 2017-08-28 23:51:43: main.py:87 **  0 133 0 1.19186514705 0.545684727137
INFO: 2017-08-28 23:51:43: main.py:87 **  0 134 0 1.19175678271 0.545929600883
INFO: 2017-08-28 23:51:43: main.py:87 **  0 135 0 1.18943668464 0.54698544114
INFO: 2017-08-28 23:51:43: main.py:87 **  0 136 0 1.18693006604 0.547904811941
INFO: 2017-08-28 23:51:43: main.py:87 **  0 137 0 1.18455829266 0.549043587455
INFO: 2017-08-28 23:51:43: main.py:87 **  0 138 0 1.18489821566 0.549940511777
INFO: 2017-08-28 23:51:44: main.py:87 **  0 139 0 1.185491696 0.550816594149
INFO: 2017-08-28 23:51:44: main.py:87 **  0 140 0 1.18703028708 0.54946186682
INFO: 2017-08-28 23:51:44: main.py:87 **  0 141 0 1.18656259649 0.550519772692
INFO: 2017-08-28 23:51:44: main.py:87 **  0 142 0 1.1846547206 0.551908348699
INFO: 2017-08-28 23:51:44: main.py:87 **  0 143 0 1.18251501603 0.55314569494
INFO: 2017-08-28 23:51:44: main.py:87 **  0 144 0 1.18155369676 0.554633107555
INFO: 2017-08-28 23:51:44: main.py:87 **  0 145 0 1.18040317215 0.555563010051
INFO: 2017-08-28 23:51:45: main.py:87 **  0 146 0 1.17700759043 0.557950419917
INFO: 2017-08-28 23:51:45: main.py:87 **  0 147 0 1.17473470802 0.559648275657
INFO: 2017-08-28 23:51:45: main.py:87 **  0 148 0 1.17492441643 0.559955535199
INFO: 2017-08-28 23:51:45: main.py:87 **  0 149 0 1.1785793674 0.557452207685
INFO: 2017-08-28 23:51:45: main.py:87 **  0 150 0 1.17568278194 0.559877859717
INFO: 2017-08-28 23:51:45: main.py:87 **  0 151 0 1.17295553222 0.56090211386
INFO: 2017-08-28 23:51:45: main.py:87 **  0 152 0 1.1702783883 0.563376385675
INFO: 2017-08-28 23:51:46: main.py:87 **  0 153 0 1.16842484087 0.564645614988
INFO: 2017-08-28 23:51:46: main.py:87 **  0 154 0 1.17235967959 0.561410488457
INFO: 2017-08-28 23:51:46: main.py:87 **  0 155 0 1.16931424882 0.563477824733
INFO: 2017-08-28 23:51:46: main.py:87 **  0 156 0 1.1674305136 0.563930512073
INFO: 2017-08-28 23:51:46: main.py:87 **  0 157 0 1.16742939776 0.562945542862
INFO: 2017-08-28 23:51:46: main.py:87 **  0 158 0 1.16374709134 0.565361281639
INFO: 2017-08-28 23:51:46: main.py:87 **  0 159 0 1.16119173169 0.56776194389
INFO: 2017-08-28 23:51:47: main.py:87 **  0 160 0 1.15910435315 0.569197985354
INFO: 2017-08-28 23:51:47: main.py:87 **  0 161 0 1.15840976179 0.569785267878
INFO: 2017-08-28 23:51:47: main.py:87 **  0 162 0 1.15535176349 0.57134122098
INFO: 2017-08-28 23:51:47: main.py:87 **  0 163 0 1.1541358556 0.572873041061
INFO: 2017-08-28 23:51:47: main.py:87 **  0 164 0 1.15126824776 0.573924552699
INFO: 2017-08-28 23:51:47: main.py:87 **  0 165 0 1.14879325427 0.575073933772
INFO: 2017-08-28 23:51:47: main.py:87 **  0 166 0 1.14845932744 0.574558131736
INFO: 2017-08-28 23:51:48: main.py:87 **  0 167 0 1.14531327145 0.576418544592
INFO: 2017-08-28 23:51:48: main.py:87 **  0 168 0 1.14278044362 0.578198363822
INFO: 2017-08-28 23:51:48: main.py:87 **  0 169 0 1.14365515499 0.577761099645
INFO: 2017-08-28 23:51:48: main.py:87 **  0 170 0 1.14111253811 0.578959822681
INFO: 2017-08-28 23:51:48: main.py:87 **  0 171 0 1.14019073442 0.579728793985
INFO: 2017-08-28 23:51:48: main.py:87 **  0 172 0 1.13959640023 0.580276123387
INFO: 2017-08-28 23:51:49: main.py:87 **  0 173 0 1.13879864106 0.5807730155
INFO: 2017-08-28 23:51:49: main.py:87 **  0 174 0 1.13693833896 0.582284761522
INFO: 2017-08-28 23:51:49: main.py:87 **  0 175 0 1.1364711943 0.582558200127
INFO: 2017-08-28 23:51:49: main.py:87 **  0 176 0 1.13731000922 0.582105606988
INFO: 2017-08-28 23:51:49: main.py:87 **  0 177 0 1.13753080636 0.582047609156
INFO: 2017-08-28 23:51:49: main.py:87 **  0 178 0 1.13699073765 0.58246025662
INFO: 2017-08-28 23:51:49: main.py:87 **  0 179 0 1.13413592246 0.583955647242
INFO: 2017-08-28 23:51:50: main.py:87 **  0 180 0 1.13566786742 0.582287509328
INFO: 2017-08-28 23:51:50: main.py:87 **  0 181 0 1.13318557497 0.583362227146
INFO: 2017-08-28 23:51:50: main.py:87 **  0 182 0 1.13190929779 0.583508503755
INFO: 2017-08-28 23:51:50: main.py:87 **  0 183 0 1.13462852168 0.581146467663
INFO: 2017-08-28 23:51:50: main.py:87 **  0 184 0 1.13288845049 0.580744195975
INFO: 2017-08-28 23:51:50: main.py:87 **  0 185 0 1.13243299979 0.580681214113
INFO: 2017-08-28 23:51:50: main.py:87 **  0 186 0 1.135828066 0.579417403393
INFO: 2017-08-28 23:51:51: main.py:87 **  0 187 0 1.134679956 0.578663470128
INFO: 2017-08-28 23:51:51: main.py:87 **  0 188 0 1.13783139716 0.575601871119
INFO: 2017-08-28 23:51:51: main.py:87 **  0 189 0 1.13582503545 0.576326903925
INFO: 2017-08-28 23:51:51: main.py:87 **  0 190 0 1.13530043407 0.576154639963
INFO: 2017-08-28 23:51:51: main.py:87 **  0 191 0 1.13506040039 0.576488195364
INFO: 2017-08-28 23:51:51: main.py:87 **  0 192 0 1.13303836238 0.577576057347
INFO: 2017-08-28 23:51:51: main.py:87 **  0 193 0 1.13067206249 0.578251260831
INFO: 2017-08-28 23:51:52: main.py:87 **  0 194 0 1.12859004553 0.578754980507
INFO: 2017-08-28 23:51:52: main.py:87 **  0 195 0 1.12614664223 0.579468207461
INFO: 2017-08-28 23:51:52: main.py:87 **  0 196 0 1.12831264795 0.579865791838
INFO: 2017-08-28 23:51:52: main.py:87 **  0 197 0 1.12576594678 0.58067423503
INFO: 2017-08-28 23:51:52: main.py:87 **  0 198 0 1.12331221301 0.58131626719
INFO: 2017-08-28 23:51:52: main.py:87 **  0 199 0 1.12177411884 0.581828408603
INFO: 2017-08-28 23:51:52: main.py:87 **  0 200 0 1.12158932763 0.582577835925
INFO: 2017-08-28 23:51:53: main.py:87 **  0 201 0 1.11949166892 0.583836003169
INFO: 2017-08-28 23:51:53: main.py:87 **  0 202 0 1.11845623566 0.584294720299
INFO: 2017-08-28 23:51:53: main.py:87 **  0 203 0 1.11602579437 0.585672540762
INFO: 2017-08-28 23:51:53: main.py:87 **  0 204 0 1.11534467529 0.585750634849
INFO: 2017-08-28 23:51:53: main.py:87 **  0 205 0 1.11261583561 0.587280209265
INFO: 2017-08-28 23:51:53: main.py:87 **  0 206 0 1.11032937258 0.588557623209
INFO: 2017-08-28 23:51:53: main.py:87 **  0 207 0 1.10747029747 0.590037094347
INFO: 2017-08-28 23:51:54: main.py:87 **  0 208 0 1.11123818131 0.587333715595
INFO: 2017-08-28 23:51:54: main.py:87 **  0 209 0 1.11038852646 0.587135020217
INFO: 2017-08-28 23:51:54: main.py:87 **  0 210 0 1.10988671746 0.587135434891
INFO: 2017-08-28 23:51:54: main.py:87 **  0 211 0 1.1108227822 0.586688448551
INFO: 2017-08-28 23:51:54: main.py:87 **  0 212 0 1.1146192629 0.583934107195
INFO: 2017-08-28 23:51:54: main.py:87 **  0 213 0 1.11826211883 0.58120550959
INFO: 2017-08-28 23:51:55: main.py:87 **  0 214 0 1.11696921532 0.582600700911
INFO: 2017-08-28 23:51:55: main.py:87 **  0 215 0 1.11724165937 0.583542155227
INFO: 2017-08-28 23:51:55: main.py:87 **  0 216 0 1.11726838735 0.584191298211
INFO: 2017-08-28 23:51:55: main.py:87 **  0 217 0 1.11743136201 0.583928162134
INFO: 2017-08-28 23:51:55: main.py:87 **  0 218 0 1.11486701089 0.585155023343
INFO: 2017-08-28 23:51:55: main.py:87 **  0 219 0 1.11805094724 0.582495298799
INFO: 2017-08-28 23:51:55: main.py:87 **  0 220 0 1.12040493418 0.580311493454
INFO: 2017-08-28 23:51:56: main.py:87 **  0 221 0 1.12001979485 0.580830686352
INFO: 2017-08-28 23:51:56: main.py:87 **  0 222 0 1.11918407625 0.581690440885
INFO: 2017-08-28 23:51:56: main.py:87 **  0 223 0 1.11727280569 0.582714214476
INFO: 2017-08-28 23:51:56: main.py:87 **  0 224 0 1.11477360646 0.583711653372
INFO: 2017-08-28 23:51:56: main.py:87 **  0 225 0 1.1156186712 0.584200872606
INFO: 2017-08-28 23:51:56: main.py:87 **  0 226 0 1.11447176613 0.584665692936
INFO: 2017-08-28 23:51:56: main.py:87 **  0 227 0 1.11607759119 0.583887722624
INFO: 2017-08-28 23:51:57: main.py:87 **  0 228 0 1.11537347714 0.584455574659
INFO: 2017-08-28 23:51:57: main.py:87 **  0 229 0 1.11303760746 0.585304456127
INFO: 2017-08-28 23:51:57: main.py:87 **  0 230 0 1.11193638034 0.58595904097
INFO: 2017-08-28 23:51:57: main.py:87 **  0 231 0 1.11053715595 0.586058264618
INFO: 2017-08-28 23:51:57: main.py:87 **  0 232 0 1.111742101 0.585423682464
INFO: 2017-08-28 23:51:57: main.py:87 **  0 233 0 1.10958117196 0.586350850908
INFO: 2017-08-28 23:51:57: main.py:87 **  0 234 0 1.11049709624 0.586215010431
INFO: 2017-08-28 23:51:58: main.py:87 **  0 235 0 1.10936024684 0.586633300712
INFO: 2017-08-28 23:51:58: main.py:87 **  0 236 0 1.10857897972 0.586933967767
INFO: 2017-08-28 23:51:58: main.py:87 **  0 237 0 1.10796944133 0.587162694592
INFO: 2017-08-28 23:51:58: main.py:87 **  0 238 0 1.10990037878 0.585104400288
INFO: 2017-08-28 23:51:58: main.py:87 **  0 239 0 1.11060756048 0.585394449923
INFO: 2017-08-28 23:51:58: main.py:87 **  0 240 0 1.10876760146 0.586510705645
INFO: 2017-08-28 23:51:58: main.py:87 **  0 241 0 1.10855834642 0.586482285856
INFO: 2017-08-28 23:51:59: main.py:87 **  0 242 0 1.10861673375 0.586763719614
INFO: 2017-08-28 23:51:59: main.py:87 **  0 243 0 1.10831281787 0.58754909077
INFO: 2017-08-28 23:51:59: main.py:87 **  0 244 0 1.10799684719 0.587622499849
INFO: 2017-08-28 23:51:59: main.py:87 **  0 245 0 1.10834839092 0.587811892007
INFO: 2017-08-28 23:51:59: main.py:87 **  0 246 0 1.10778293339 0.588095270085
INFO: 2017-08-28 23:51:59: main.py:87 **  0 247 0 1.10728886051 0.588363863666
INFO: 2017-08-28 23:52:00: main.py:87 **  0 248 0 1.1055886238 0.589419488346
INFO: 2017-08-28 23:52:00: main.py:87 **  0 249 0 1.10660023499 0.589488227982
INFO: 2017-08-28 23:52:00: main.py:87 **  0 250 0 1.10447191907 0.590409157897
INFO: 2017-08-28 23:52:00: main.py:87 **  0 251 0 1.10493178405 0.589856601796
INFO: 2017-08-28 23:52:00: main.py:87 **  0 252 0 1.10711839454 0.588112356535
INFO: 2017-08-28 23:52:00: main.py:87 **  0 253 0 1.10733538631 0.588370649734
INFO: 2017-08-28 23:52:00: main.py:87 **  0 254 0 1.1064329124 0.58895057313
INFO: 2017-08-28 23:52:01: main.py:87 **  0 255 0 1.10556583595 0.589519206336
INFO: 2017-08-28 23:52:01: main.py:87 **  0 256 0 1.10424980324 0.590412204733
INFO: 2017-08-28 23:52:01: main.py:87 **  0 257 0 1.10624359952 0.588840396616
INFO: 2017-08-28 23:52:01: main.py:87 **  0 258 0 1.10465731469 0.589481240321
INFO: 2017-08-28 23:52:01: main.py:87 **  0 259 0 1.10518400463 0.588629432269
INFO: 2017-08-28 23:52:01: main.py:87 **  0 260 0 1.10802753287 0.586374227093
INFO: 2017-08-28 23:52:01: main.py:87 **  0 261 0 1.10608699417 0.58752828311
INFO: 2017-08-28 23:52:02: main.py:87 **  0 262 0 1.10439187651 0.588243980803
INFO: 2017-08-28 23:52:02: main.py:87 **  0 263 0 1.10409015214 0.588533879659
INFO: 2017-08-28 23:52:02: main.py:87 **  0 264 0 1.10223256979 0.589667808988
INFO: 2017-08-28 23:52:02: main.py:87 **  0 265 0 1.10051947541 0.590731129141
INFO: 2017-08-28 23:52:02: main.py:87 **  0 266 0 1.0996697295 0.591170827303
INFO: 2017-08-28 23:52:02: main.py:87 **  0 267 0 1.09845408263 0.592001768169
INFO: 2017-08-28 23:52:02: main.py:87 **  0 268 0 1.09742652351 0.592527750235
INFO: 2017-08-28 23:52:03: main.py:87 **  0 269 0 1.09705475525 0.592473833849
INFO: 2017-08-28 23:52:03: main.py:87 **  0 270 0 1.09637556965 0.592788608291
INFO: 2017-08-28 23:52:03: main.py:87 **  0 271 0 1.09939681432 0.591555360026
INFO: 2017-08-28 23:52:03: main.py:87 **  0 272 0 1.10013499015 0.592026634291
INFO: 2017-08-28 23:52:03: main.py:87 **  0 273 0 1.09893428931 0.592559643232
INFO: 2017-08-28 23:52:03: main.py:87 **  0 274 0 1.09888176701 0.592499317235
INFO: 2017-08-28 23:52:03: main.py:87 **  0 275 0 1.10113885869 0.592158875583
INFO: 2017-08-28 23:52:04: main.py:87 **  0 276 0 1.10087633563 0.592205880761
INFO: 2017-08-28 23:52:04: main.py:87 **  0 277 0 1.0991029495 0.593148592973
INFO: 2017-08-28 23:52:04: main.py:87 **  0 278 0 1.10098193324 0.591865865593
INFO: 2017-08-28 23:52:04: main.py:87 **  0 279 0 1.09988619813 0.59231027219
INFO: 2017-08-28 23:52:04: main.py:87 **  0 280 0 1.0988787856 0.592759670981
INFO: 2017-08-28 23:52:04: main.py:87 **  0 281 0 1.09917283882 0.593319808582
INFO: 2017-08-28 23:52:05: main.py:87 **  0 282 0 1.09743576779 0.594470962651
INFO: 2017-08-28 23:52:05: main.py:87 **  0 283 0 1.09845506578 0.594550000352
INFO: 2017-08-28 23:52:05: main.py:87 **  0 284 0 1.09730774457 0.595514339555
INFO: 2017-08-28 23:52:05: main.py:87 **  0 285 0 1.09554807144 0.596723620949
INFO: 2017-08-28 23:52:05: main.py:87 **  0 286 0 1.09377601338 0.597588504473
INFO: 2017-08-28 23:52:05: main.py:87 **  0 287 0 1.09257203734 0.598350831236
INFO: 2017-08-28 23:52:05: main.py:87 **  0 288 0 1.09230864687 0.598328669004
INFO: 2017-08-28 23:52:06: main.py:87 **  0 289 0 1.0939705859 0.597146761141
INFO: 2017-08-28 23:52:06: main.py:87 **  0 290 0 1.09715987255 0.595094756457
INFO: 2017-08-28 23:52:06: main.py:87 **  0 291 0 1.09598859389 0.595485939236
INFO: 2017-08-28 23:52:06: main.py:87 **  0 292 0 1.09521215951 0.596200584234
INFO: 2017-08-28 23:52:06: main.py:87 **  0 293 0 1.0936786429 0.597107982238
INFO: 2017-08-28 23:52:06: main.py:87 **  0 294 0 1.09258922985 0.597833494097
INFO: 2017-08-28 23:52:06: main.py:87 **  0 295 0 1.09105882149 0.598705654901
INFO: 2017-08-28 23:52:07: main.py:87 **  0 296 0 1.089490977 0.599547602801
INFO: 2017-08-28 23:52:07: main.py:87 **  0 297 0 1.09022509672 0.598594173977
INFO: 2017-08-28 23:52:07: main.py:87 **  0 298 0 1.09048826659 0.599126209271
INFO: 2017-08-28 23:52:07: main.py:87 **  0 299 0 1.09115126948 0.598451065409
INFO: 2017-08-28 23:52:07: main.py:87 **  0 300 0 1.0906911431 0.599262837345
INFO: 2017-08-28 23:52:07: main.py:87 **  0 301 0 1.09069755417 0.599855347765
INFO: 2017-08-28 23:52:07: main.py:87 **  0 302 0 1.09016903832 0.600680107122
INFO: 2017-08-28 23:52:08: main.py:87 **  0 303 0 1.08872949959 0.60129703286
INFO: 2017-08-28 23:52:08: main.py:87 **  0 304 0 1.0882809125 0.601943140721
INFO: 2017-08-28 23:52:08: main.py:87 **  0 305 0 1.08739949266 0.602504507961
INFO: 2017-08-28 23:52:08: main.py:87 **  0 306 0 1.08625955108 0.603336250343
INFO: 2017-08-28 23:52:08: main.py:87 **  0 307 0 1.08587817306 0.604192243756
INFO: 2017-08-28 23:52:08: main.py:87 **  0 308 0 1.08432458365 0.604729868243
INFO: 2017-08-28 23:52:08: main.py:87 **  0 309 0 1.0837648061 0.605001999859
INFO: 2017-08-28 23:52:09: main.py:87 **  0 310 0 1.08319492854 0.605500187893
INFO: 2017-08-28 23:52:09: main.py:87 **  0 311 0 1.08408097923 0.605733437404
INFO: 2017-08-28 23:52:09: main.py:87 **  0 312 0 1.08408871245 0.606199121502
INFO: 2017-08-28 23:52:09: main.py:87 **  0 313 0 1.08502381243 0.605194974276
INFO: 2017-08-28 23:52:09: main.py:87 **  0 314 0 1.08359731663 0.606286187198
INFO: 2017-08-28 23:52:09: main.py:87 **  0 315 0 1.08352623465 0.606167793828
INFO: 2017-08-28 23:52:10: main.py:87 **  0 316 0 1.08252737255 0.606666205798
INFO: 2017-08-28 23:52:10: main.py:87 **  0 317 0 1.0810978347 0.607544755399
INFO: 2017-08-28 23:52:10: main.py:87 **  0 318 0 1.08165920435 0.607760172454
INFO: 2017-08-28 23:52:10: main.py:87 **  0 319 0 1.08428179901 0.606062733999
INFO: 2017-08-28 23:52:10: main.py:87 **  0 320 0 1.08402141362 0.606147496187
INFO: 2017-08-28 23:52:10: main.py:87 **  0 321 0 1.08267765293 0.60673758708
INFO: 2017-08-28 23:52:10: main.py:87 **  0 322 0 1.08475398925 0.605236432789
INFO: 2017-08-28 23:52:11: main.py:87 **  0 323 0 1.08379282186 0.606003082197
INFO: 2017-08-28 23:52:11: main.py:87 **  0 324 0 1.08328382235 0.606639099408
INFO: 2017-08-28 23:52:11: main.py:87 **  0 325 0 1.08412073178 0.605642997608
INFO: 2017-08-28 23:52:11: main.py:87 **  0 326 0 1.08428475981 0.606286696349
INFO: 2017-08-28 23:52:11: main.py:87 **  0 327 0 1.08414791924 0.605951743803
INFO: 2017-08-28 23:52:11: main.py:87 **  0 328 0 1.08304852773 0.606375503968
INFO: 2017-08-28 23:52:11: main.py:87 **  0 329 0 1.08441587939 0.605430198721
INFO: 2017-08-28 23:52:12: main.py:87 **  0 330 0 1.08414520562 0.605927539657
INFO: 2017-08-28 23:52:12: main.py:87 **  0 331 0 1.08470788454 0.606359570422
INFO: 2017-08-28 23:52:12: main.py:87 **  0 332 0 1.08340809403 0.60684000722
INFO: 2017-08-28 23:52:12: main.py:87 **  0 333 0 1.08249108306 0.607526423075
INFO: 2017-08-28 23:52:12: main.py:87 **  0 334 0 1.08106658672 0.608519494278
INFO: 2017-08-28 23:52:12: main.py:87 **  0 335 0 1.08142220513 0.609197438218
INFO: 2017-08-28 23:52:12: main.py:87 **  0 336 0 1.08203834136 0.608415988957
INFO: 2017-08-28 23:52:13: main.py:87 **  0 337 0 1.08079801403 0.608852850538
INFO: 2017-08-28 23:52:13: main.py:87 **  0 338 0 1.08007664877 0.609216811482
INFO: 2017-08-28 23:52:13: main.py:87 **  0 339 0 1.08239287138 0.607577418262
INFO: 2017-08-28 23:52:13: main.py:87 **  0 340 0 1.08123982757 0.608254828444
INFO: 2017-08-28 23:52:13: main.py:87 **  0 341 0 1.08119628199 0.607813979483
INFO: 2017-08-28 23:52:13: main.py:87 **  0 342 0 1.08062423753 0.608052765695
INFO: 2017-08-28 23:52:13: main.py:87 **  0 343 0 1.08167026088 0.607329141728
INFO: 2017-08-28 23:52:14: main.py:87 **  0 344 0 1.08170128577 0.607567964203
INFO: 2017-08-28 23:52:14: main.py:87 **  0 345 0 1.08067863146 0.607876345203
INFO: 2017-08-28 23:52:14: main.py:87 **  0 346 0 1.0827014886 0.606284677274
INFO: 2017-08-28 23:52:14: main.py:87 **  0 347 0 1.08269097134 0.60638025144
INFO: 2017-08-28 23:52:14: main.py:87 **  0 348 0 1.08247154456 0.606489186792
INFO: 2017-08-28 23:52:14: main.py:87 **  0 349 0 1.08154443877 0.606910632487
INFO: 2017-08-28 23:52:15: main.py:87 **  0 350 0 1.08010199335 0.607537210136
INFO: 2017-08-28 23:52:15: main.py:87 **  0 351 0 1.07981221683 0.607453411424
INFO: 2017-08-28 23:52:15: main.py:87 **  0 352 0 1.07895475558 0.607618127409
INFO: 2017-08-28 23:52:15: main.py:87 **  0 353 0 1.07847582974 0.607758602496
INFO: 2017-08-28 23:52:15: main.py:87 **  0 354 0 1.07822685023 0.607755776673
INFO: 2017-08-28 23:52:15: main.py:87 **  0 355 0 1.07802461356 0.607882757682
INFO: 2017-08-28 23:52:15: main.py:87 **  0 356 0 1.07687925508 0.60853512629
INFO: 2017-08-28 23:52:16: main.py:87 **  0 357 0 1.07556629031 0.609102076971
INFO: 2017-08-28 23:52:16: main.py:87 **  0 358 0 1.07511875141 0.609326560829
INFO: 2017-08-28 23:52:16: main.py:87 **  0 359 0 1.07527415405 0.609524447533
INFO: 2017-08-28 23:52:16: main.py:87 **  0 360 0 1.07523304257 0.609755801507
INFO: 2017-08-28 23:52:16: main.py:87 **  0 361 0 1.07753972635 0.608091551737
INFO: 2017-08-28 23:52:16: main.py:87 **  0 362 0 1.07659465189 0.608760918579
INFO: 2017-08-28 23:52:16: main.py:87 **  0 363 0 1.07678112781 0.60906696036
INFO: 2017-08-28 23:52:17: main.py:87 **  0 364 0 1.07717400087 0.609389758058
INFO: 2017-08-28 23:52:17: main.py:87 **  0 365 0 1.07617043765 0.610062599293
INFO: 2017-08-28 23:52:17: main.py:87 **  0 366 0 1.07582896704 0.610634065727
INFO: 2017-08-28 23:52:17: main.py:87 **  0 367 0 1.07812962649 0.609115225981
INFO: 2017-08-28 23:52:17: main.py:87 **  0 368 0 1.0776701681 0.609762955032
INFO: 2017-08-28 23:52:17: main.py:87 **  0 369 0 1.07834992087 0.609266026951
INFO: 2017-08-28 23:52:18: main.py:87 **  0 370 0 1.07732786547 0.609856093216
INFO: 2017-08-28 23:52:18: main.py:87 **  0 371 0 1.07703519012 0.609890219534
INFO: 2017-08-28 23:52:18: main.py:87 **  0 372 0 1.07551083824 0.610709252719
INFO: 2017-08-28 23:52:18: main.py:87 **  0 373 0 1.07820814131 0.609077393696
INFO: 2017-08-28 23:52:18: main.py:87 **  0 374 0 1.07947876946 0.608206797924
INFO: 2017-08-28 23:52:18: main.py:87 **  0 375 0 1.0788674261 0.608379832138
INFO: 2017-08-28 23:52:18: main.py:87 **  0 376 0 1.07797524167 0.608908651368
INFO: 2017-08-28 23:52:19: main.py:87 **  0 377 0 1.07659177853 0.60971412016
INFO: 2017-08-28 23:52:19: main.py:87 **  0 378 0 1.07532020295 0.610411653896
INFO: 2017-08-28 23:52:19: main.py:87 **  0 379 0 1.07675277904 0.609293526692
INFO: 2017-08-28 23:52:19: main.py:87 **  0 380 0 1.0790534517 0.607694364937
INFO: 2017-08-28 23:52:19: main.py:87 **  0 381 0 1.07827634992 0.607992829949
INFO: 2017-08-28 23:52:19: main.py:87 **  0 382 0 1.07898065751 0.607877344777
INFO: 2017-08-28 23:52:19: main.py:87 **  0 383 0 1.07781312661 0.608424196669
INFO: 2017-08-28 23:52:20: main.py:87 **  0 384 0 1.07772779666 0.608666893597
INFO: 2017-08-28 23:52:20: main.py:87 **  0 385 0 1.07718657274 0.608863975282
INFO: 2017-08-28 23:52:20: main.py:87 **  0 386 0 1.07660617754 0.609195845254
INFO: 2017-08-28 23:52:20: main.py:87 **  0 387 0 1.07549738116 0.609648041378
INFO: 2017-08-28 23:52:20: main.py:87 **  0 388 0 1.07546322971 0.60967372487
INFO: 2017-08-28 23:52:20: main.py:87 **  0 389 0 1.07445806555 0.610082645016
INFO: 2017-08-28 23:52:20: main.py:87 **  0 390 0 1.07598277446 0.608917453204
INFO: 2017-08-28 23:52:21: main.py:87 **  0 391 0 1.07575658101 0.609266095271
INFO: 2017-08-28 23:52:21: main.py:87 **  0 392 0 1.0755834769 0.609384625874
INFO: 2017-08-28 23:52:21: main.py:87 **  0 393 0 1.07505108862 0.60956386922
INFO: 2017-08-28 23:52:21: main.py:87 **  0 394 0 1.0753773985 0.608902582818
INFO: 2017-08-28 23:52:21: main.py:87 **  0 395 0 1.07431357918 0.609349327885
INFO: 2017-08-28 23:52:21: main.py:87 **  0 396 0 1.07374037753 0.609962401549
INFO: 2017-08-28 23:52:22: main.py:87 **  0 397 0 1.07378566055 0.610308682676
INFO: 2017-08-28 23:52:22: main.py:87 **  0 398 0 1.07300485973 0.610771827109
INFO: 2017-08-28 23:52:22: main.py:87 **  0 399 0 1.07342031449 0.610970102278
INFO: 2017-08-28 23:52:22: main.py:87 **  0 400 0 1.0724035118 0.611630005608
INFO: 2017-08-28 23:53:39: main.py:28 **  start
INFO: 2017-08-28 23:53:39: main.py:30 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-28 23:53:39: main.py:31 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-28 23:53:39: main.py:32 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-28 23:53:39: main.py:33 **  读取数据集...
INFO: 2017-08-28 23:53:42: main.py:44 **  train data sample counts 5088
INFO: 2017-08-28 23:53:42: main.py:45 **  train data set batch size 4
INFO: 2017-08-28 23:53:42: main.py:46 **  train data batch counts 1272
INFO: 2017-08-28 23:53:43: main.py:53 **  <class 'net.net.CarUNet'>


INFO: 2017-08-28 23:53:45: main.py:86 **  epoch: 0 batch_num: 0 lr: 0 loss: 1.26105856895 acc: 0.534521758556
INFO: 2017-08-28 23:53:45: main.py:86 **  epoch: 0 batch_num: 1 lr: 0 loss: 1.1654137373 acc: 0.642114669085
INFO: 2017-08-28 23:53:45: main.py:86 **  epoch: 0 batch_num: 2 lr: 0 loss: 1.16072591146 acc: 0.649253487587
INFO: 2017-08-28 23:53:46: main.py:86 **  epoch: 0 batch_num: 3 lr: 0 loss: 1.21706819534 acc: 0.581148527563
INFO: 2017-08-28 23:53:46: main.py:86 **  epoch: 0 batch_num: 4 lr: 0 loss: 1.20868406296 acc: 0.583390957117
INFO: 2017-08-28 23:53:47: main.py:86 **  epoch: 0 batch_num: 5 lr: 0 loss: 1.24525431792 acc: 0.5392366002
INFO: 2017-08-28 23:53:47: main.py:86 **  epoch: 0 batch_num: 6 lr: 0 loss: 1.27043734278 acc: 0.502851354224
INFO: 2017-08-28 23:53:47: main.py:86 **  epoch: 0 batch_num: 7 lr: 0 loss: 1.27948713303 acc: 0.482263986021
INFO: 2017-08-28 23:53:48: main.py:86 **  epoch: 0 batch_num: 8 lr: 0 loss: 1.27936726146 acc: 0.47640270657
INFO: 2017-08-28 23:53:48: main.py:86 **  epoch: 0 batch_num: 9 lr: 0 loss: 1.25223872662 acc: 0.505389004946
INFO: 2017-08-28 23:53:49: main.py:86 **  epoch: 0 batch_num: 10 lr: 0 loss: 1.23119065978 acc: 0.523405866189
INFO: 2017-08-28 23:53:49: main.py:86 **  epoch: 0 batch_num: 11 lr: 0 loss: 1.22131416202 acc: 0.527106205622
INFO: 2017-08-28 23:53:50: main.py:86 **  epoch: 0 batch_num: 12 lr: 0 loss: 1.20610119746 acc: 0.536695168569
INFO: 2017-08-28 23:53:50: main.py:86 **  epoch: 0 batch_num: 13 lr: 0 loss: 1.21136120387 acc: 0.528095866953
INFO: 2017-08-28 23:53:50: main.py:86 **  epoch: 0 batch_num: 14 lr: 0 loss: 1.19804118474 acc: 0.540020696322
INFO: 2017-08-28 23:53:51: main.py:86 **  epoch: 0 batch_num: 15 lr: 0 loss: 1.20631638169 acc: 0.531707502902
INFO: 2017-08-28 23:53:51: main.py:86 **  epoch: 0 batch_num: 16 lr: 0 loss: 1.20354392949 acc: 0.534300562213
INFO: 2017-08-28 23:53:52: main.py:86 **  epoch: 0 batch_num: 17 lr: 0 loss: 1.20649517907 acc: 0.536890609397
INFO: 2017-08-28 23:53:52: main.py:86 **  epoch: 0 batch_num: 18 lr: 0 loss: 1.20666786244 acc: 0.541509574965
INFO: 2017-08-28 23:53:53: main.py:86 **  epoch: 0 batch_num: 19 lr: 0 loss: 1.19927005768 acc: 0.539757761359
INFO: 2017-08-28 23:53:53: main.py:86 **  epoch: 0 batch_num: 20 lr: 0 loss: 1.1970798061 acc: 0.546717246373
INFO: 2017-08-28 23:53:53: main.py:86 **  epoch: 0 batch_num: 21 lr: 0 loss: 1.19911684231 acc: 0.550281819972
INFO: 2017-08-28 23:53:54: main.py:86 **  epoch: 0 batch_num: 22 lr: 0 loss: 1.18799529905 acc: 0.551158264927
INFO: 2017-08-28 23:53:54: main.py:86 **  epoch: 0 batch_num: 23 lr: 0 loss: 1.1867185384 acc: 0.55261190484
INFO: 2017-08-28 23:53:55: main.py:86 **  epoch: 0 batch_num: 24 lr: 0 loss: 1.17843524456 acc: 0.553883037567
INFO: 2017-08-28 23:53:55: main.py:86 **  epoch: 0 batch_num: 25 lr: 0 loss: 1.17107081413 acc: 0.557483297128
INFO: 2017-08-28 23:53:55: main.py:86 **  epoch: 0 batch_num: 26 lr: 0 loss: 1.1671653412 acc: 0.560516379498
INFO: 2017-08-28 23:53:56: main.py:86 **  epoch: 0 batch_num: 27 lr: 0 loss: 1.16728776268 acc: 0.561923065356
INFO: 2017-08-28 23:53:56: main.py:86 **  epoch: 0 batch_num: 28 lr: 0 loss: 1.16257084238 acc: 0.569470851586
INFO: 2017-08-28 23:53:57: main.py:86 **  epoch: 0 batch_num: 29 lr: 0 loss: 1.15858848492 acc: 0.575255141656
INFO: 2017-08-28 23:53:57: main.py:86 **  epoch: 0 batch_num: 30 lr: 0 loss: 1.15800199201 acc: 0.574667563362
INFO: 2017-08-28 23:53:57: main.py:86 **  epoch: 0 batch_num: 31 lr: 0 loss: 1.15725235268 acc: 0.576328858733
INFO: 2017-08-28 23:53:58: main.py:86 **  epoch: 0 batch_num: 32 lr: 0 loss: 1.15291661566 acc: 0.581882018031
INFO: 2017-08-28 23:53:58: main.py:86 **  epoch: 0 batch_num: 33 lr: 0 loss: 1.14824112899 acc: 0.581428938052
INFO: 2017-08-28 23:53:59: main.py:86 **  epoch: 0 batch_num: 34 lr: 0 loss: 1.14434431791 acc: 0.586811821801
INFO: 2017-08-28 23:53:59: main.py:86 **  epoch: 0 batch_num: 35 lr: 0 loss: 1.14332282709 acc: 0.589384327332
INFO: 2017-08-28 23:54:00: main.py:86 **  epoch: 0 batch_num: 36 lr: 0 loss: 1.13554441123 acc: 0.591922328279
INFO: 2017-08-28 23:54:00: main.py:86 **  epoch: 0 batch_num: 37 lr: 0 loss: 1.13069371958 acc: 0.593075521682
INFO: 2017-08-28 23:54:00: main.py:86 **  epoch: 0 batch_num: 38 lr: 0 loss: 1.12593177496 acc: 0.5967901777
INFO: 2017-08-28 23:54:01: main.py:86 **  epoch: 0 batch_num: 39 lr: 0 loss: 1.1206753239 acc: 0.602371810377
INFO: 2017-08-28 23:54:01: main.py:86 **  epoch: 0 batch_num: 40 lr: 0 loss: 1.11538700941 acc: 0.605902559874
INFO: 2017-08-28 23:54:02: main.py:86 **  epoch: 0 batch_num: 41 lr: 0 loss: 1.11404503527 acc: 0.606297124
INFO: 2017-08-28 23:54:02: main.py:86 **  epoch: 0 batch_num: 42 lr: 0 loss: 1.10819209731 acc: 0.610730459524
INFO: 2017-08-28 23:54:02: main.py:86 **  epoch: 0 batch_num: 43 lr: 0 loss: 1.10479783741 acc: 0.613648275083
INFO: 2017-08-28 23:54:03: main.py:86 **  epoch: 0 batch_num: 44 lr: 0 loss: 1.10472114616 acc: 0.615010154247
INFO: 2017-08-28 23:54:03: main.py:86 **  epoch: 0 batch_num: 45 lr: 0 loss: 1.10146527964 acc: 0.614189631265
INFO: 2017-08-28 23:54:04: main.py:86 **  epoch: 0 batch_num: 46 lr: 0 loss: 1.09638599386 acc: 0.615534040522
INFO: 2017-08-28 23:54:04: main.py:86 **  epoch: 0 batch_num: 47 lr: 0 loss: 1.0928064771 acc: 0.618379162004
INFO: 2017-08-28 23:54:05: main.py:86 **  epoch: 0 batch_num: 48 lr: 0 loss: 1.08950937524 acc: 0.62216519093
INFO: 2017-08-28 23:54:05: main.py:86 **  epoch: 0 batch_num: 49 lr: 0 loss: 1.08252321005 acc: 0.625729085207
INFO: 2017-08-28 23:54:05: main.py:86 **  epoch: 0 batch_num: 50 lr: 0 loss: 1.07791863236 acc: 0.629965443237
INFO: 2017-08-28 23:54:06: main.py:86 **  epoch: 0 batch_num: 51 lr: 0 loss: 1.07247801927 acc: 0.631056168905
INFO: 2017-08-28 23:54:06: main.py:86 **  epoch: 0 batch_num: 52 lr: 0 loss: 1.0699096547 acc: 0.633842139874
INFO: 2017-08-28 23:54:07: main.py:86 **  epoch: 0 batch_num: 53 lr: 0 loss: 1.06571266717 acc: 0.63707841988
INFO: 2017-08-28 23:54:07: main.py:86 **  epoch: 0 batch_num: 54 lr: 0 loss: 1.06086151708 acc: 0.641059422493
INFO: 2017-08-28 23:54:07: main.py:86 **  epoch: 0 batch_num: 55 lr: 0 loss: 1.05472440379 acc: 0.645235822669
INFO: 2017-08-28 23:54:08: main.py:86 **  epoch: 0 batch_num: 56 lr: 0 loss: 1.05040106439 acc: 0.647589118857
INFO: 2017-08-28 23:54:08: main.py:86 **  epoch: 0 batch_num: 57 lr: 0 loss: 1.0489271016 acc: 0.649454261722
INFO: 2017-08-28 23:54:09: main.py:86 **  epoch: 0 batch_num: 58 lr: 0 loss: 1.04342599744 acc: 0.653507436736
INFO: 2017-08-28 23:54:09: main.py:86 **  epoch: 0 batch_num: 59 lr: 0 loss: 1.04541709522 acc: 0.650400364399
INFO: 2017-08-28 23:54:09: main.py:86 **  epoch: 0 batch_num: 60 lr: 0 loss: 1.03891202563 acc: 0.655084926574
INFO: 2017-08-28 23:54:10: main.py:86 **  epoch: 0 batch_num: 61 lr: 0 loss: 1.03565650698 acc: 0.655908787443
INFO: 2017-08-28 23:54:10: main.py:86 **  epoch: 0 batch_num: 62 lr: 0 loss: 1.03013995999 acc: 0.659039496429
INFO: 2017-08-28 23:54:11: main.py:86 **  epoch: 0 batch_num: 63 lr: 0 loss: 1.02949680388 acc: 0.659398392774
INFO: 2017-08-28 23:54:11: main.py:86 **  epoch: 0 batch_num: 64 lr: 0 loss: 1.02881099169 acc: 0.658684549882
INFO: 2017-08-28 23:54:11: main.py:86 **  epoch: 0 batch_num: 65 lr: 0 loss: 1.02747102849 acc: 0.658666739861
INFO: 2017-08-28 23:54:12: main.py:86 **  epoch: 0 batch_num: 66 lr: 0 loss: 1.02202823625 acc: 0.661337858705
INFO: 2017-08-28 23:54:12: main.py:86 **  epoch: 0 batch_num: 67 lr: 0 loss: 1.01654953641 acc: 0.66451211449
INFO: 2017-08-28 23:54:13: main.py:86 **  epoch: 0 batch_num: 68 lr: 0 loss: 1.01313488725 acc: 0.666114365709
INFO: 2017-08-28 23:54:13: main.py:86 **  epoch: 0 batch_num: 69 lr: 0 loss: 1.00818139996 acc: 0.668490114382
INFO: 2017-08-28 23:54:14: main.py:86 **  epoch: 0 batch_num: 70 lr: 0 loss: 1.00546331641 acc: 0.668281305004
INFO: 2017-08-28 23:54:14: main.py:86 **  epoch: 0 batch_num: 71 lr: 0 loss: 1.00509804818 acc: 0.669343805148
INFO: 2017-08-28 23:54:14: main.py:86 **  epoch: 0 batch_num: 72 lr: 0 loss: 1.00132998048 acc: 0.671933078603
INFO: 2017-08-28 23:54:15: main.py:86 **  epoch: 0 batch_num: 73 lr: 0 loss: 0.999821351992 acc: 0.671473730255
INFO: 2017-08-28 23:54:15: main.py:86 **  epoch: 0 batch_num: 74 lr: 0 loss: 0.997071030935 acc: 0.672761397362
INFO: 2017-08-28 23:54:16: main.py:86 **  epoch: 0 batch_num: 75 lr: 0 loss: 0.992612296029 acc: 0.673869804332
INFO: 2017-08-28 23:54:16: main.py:86 **  epoch: 0 batch_num: 76 lr: 0 loss: 0.997995949411 acc: 0.671345636829
INFO: 2017-08-28 23:54:16: main.py:86 **  epoch: 0 batch_num: 77 lr: 0 loss: 0.994887782977 acc: 0.672053931997
INFO: 2017-08-28 23:54:17: main.py:86 **  epoch: 0 batch_num: 78 lr: 0 loss: 0.991577266138 acc: 0.673472249432
INFO: 2017-08-28 23:54:17: main.py:86 **  epoch: 0 batch_num: 79 lr: 0 loss: 0.986353631318 acc: 0.67599000521
INFO: 2017-08-28 23:54:18: main.py:86 **  epoch: 0 batch_num: 80 lr: 0 loss: 0.983048179267 acc: 0.677994478264
INFO: 2017-08-28 23:54:18: main.py:86 **  epoch: 0 batch_num: 81 lr: 0 loss: 0.979724299617 acc: 0.677936677889
INFO: 2017-08-28 23:54:18: main.py:86 **  epoch: 0 batch_num: 82 lr: 0 loss: 0.978211682963 acc: 0.679269769465
INFO: 2017-08-28 23:54:19: main.py:86 **  epoch: 0 batch_num: 83 lr: 0 loss: 0.973263111852 acc: 0.681860289758
INFO: 2017-08-28 23:54:19: main.py:86 **  epoch: 0 batch_num: 84 lr: 0 loss: 0.967320770025 acc: 0.684852286297
INFO: 2017-08-28 23:54:20: main.py:86 **  epoch: 0 batch_num: 85 lr: 0 loss: 0.961206150956 acc: 0.687751179864
INFO: 2017-08-28 23:54:20: main.py:86 **  epoch: 0 batch_num: 86 lr: 0 loss: 0.958329014052 acc: 0.689601232608
INFO: 2017-08-28 23:54:21: main.py:86 **  epoch: 0 batch_num: 87 lr: 0 loss: 0.953763465651 acc: 0.691762701693
INFO: 2017-08-28 23:54:21: main.py:86 **  epoch: 0 batch_num: 88 lr: 0 loss: 0.950986060868 acc: 0.693470072545
INFO: 2017-08-28 23:54:21: main.py:86 **  epoch: 0 batch_num: 89 lr: 0 loss: 0.949807348847 acc: 0.694247507056
INFO: 2017-08-28 23:54:22: main.py:86 **  epoch: 0 batch_num: 90 lr: 0 loss: 0.947263198567 acc: 0.695514679938
INFO: 2017-08-28 23:54:22: main.py:86 **  epoch: 0 batch_num: 91 lr: 0 loss: 0.943715677637 acc: 0.697381004367
INFO: 2017-08-28 23:54:23: main.py:86 **  epoch: 0 batch_num: 92 lr: 0 loss: 0.939339117658 acc: 0.699452529351
INFO: 2017-08-28 23:54:23: main.py:86 **  epoch: 0 batch_num: 93 lr: 0 loss: 0.94321915222 acc: 0.697815889691
INFO: 2017-08-28 23:54:23: main.py:86 **  epoch: 0 batch_num: 94 lr: 0 loss: 0.938357523241 acc: 0.69980416643
INFO: 2017-08-28 23:54:24: main.py:86 **  epoch: 0 batch_num: 95 lr: 0 loss: 0.937629096831 acc: 0.697460009096
INFO: 2017-08-28 23:54:24: main.py:86 **  epoch: 0 batch_num: 96 lr: 0 loss: 0.935129459986 acc: 0.697808422686
INFO: 2017-08-28 23:54:25: main.py:86 **  epoch: 0 batch_num: 97 lr: 0 loss: 0.935861370393 acc: 0.69751182533
INFO: 2017-08-28 23:54:25: main.py:86 **  epoch: 0 batch_num: 98 lr: 0 loss: 0.935471528106 acc: 0.69799666784
INFO: 2017-08-28 23:54:25: main.py:86 **  epoch: 0 batch_num: 99 lr: 0 loss: 0.935775183439 acc: 0.69861048609
INFO: 2017-08-28 23:54:26: main.py:86 **  epoch: 0 batch_num: 100 lr: 0 loss: 0.9333859328 acc: 0.700068587419
INFO: 2017-08-28 23:54:26: main.py:86 **  epoch: 0 batch_num: 101 lr: 0 loss: 0.933194785726 acc: 0.698398246777
INFO: 2017-08-28 23:54:27: main.py:86 **  epoch: 0 batch_num: 102 lr: 0 loss: 0.929612434026 acc: 0.699737620875
INFO: 2017-08-28 23:54:27: main.py:86 **  epoch: 0 batch_num: 103 lr: 0 loss: 0.925877646758 acc: 0.701408194521
INFO: 2017-08-28 23:54:28: main.py:86 **  epoch: 0 batch_num: 104 lr: 0 loss: 0.922346954119 acc: 0.702490938561
INFO: 2017-08-28 23:54:28: main.py:86 **  epoch: 0 batch_num: 105 lr: 0 loss: 0.92066378301 acc: 0.702318715318
INFO: 2017-08-28 23:54:28: main.py:86 **  epoch: 0 batch_num: 106 lr: 0 loss: 0.920389906268 acc: 0.700394095383
INFO: 2017-08-28 23:54:29: main.py:86 **  epoch: 0 batch_num: 107 lr: 0 loss: 0.920268024559 acc: 0.700499431679
INFO: 2017-08-28 23:54:29: main.py:86 **  epoch: 0 batch_num: 108 lr: 0 loss: 0.916684032033 acc: 0.70208181017
INFO: 2017-08-28 23:54:30: main.py:86 **  epoch: 0 batch_num: 109 lr: 0 loss: 0.914262994311 acc: 0.703241400556
INFO: 2017-08-28 23:54:30: main.py:86 **  epoch: 0 batch_num: 110 lr: 0 loss: 0.915418550238 acc: 0.701977080859
INFO: 2017-08-28 23:54:30: main.py:86 **  epoch: 0 batch_num: 111 lr: 0 loss: 0.911655823301 acc: 0.703411423468
INFO: 2017-08-28 23:54:31: main.py:86 **  epoch: 0 batch_num: 112 lr: 0 loss: 0.908204940832 acc: 0.70479580338
INFO: 2017-08-28 23:54:31: main.py:86 **  epoch: 0 batch_num: 113 lr: 0 loss: 0.906524136662 acc: 0.705307557656
INFO: 2017-08-28 23:54:32: main.py:86 **  epoch: 0 batch_num: 114 lr: 0 loss: 0.905457774712 acc: 0.706147133008
INFO: 2017-08-28 23:54:32: main.py:86 **  epoch: 0 batch_num: 115 lr: 0 loss: 0.901082187634 acc: 0.708113625389
INFO: 2017-08-28 23:54:33: main.py:86 **  epoch: 0 batch_num: 116 lr: 0 loss: 0.90171147641 acc: 0.708980542727
INFO: 2017-08-28 23:54:33: main.py:86 **  epoch: 0 batch_num: 117 lr: 0 loss: 0.898954194229 acc: 0.710062026725
INFO: 2017-08-28 23:54:33: main.py:86 **  epoch: 0 batch_num: 118 lr: 0 loss: 0.899740387662 acc: 0.709131475006
INFO: 2017-08-28 23:54:34: main.py:86 **  epoch: 0 batch_num: 119 lr: 0 loss: 0.89798004205 acc: 0.709418919434
INFO: 2017-08-28 23:54:34: main.py:86 **  epoch: 0 batch_num: 120 lr: 0 loss: 0.896147970079 acc: 0.710341594682
INFO: 2017-08-28 23:54:35: main.py:86 **  epoch: 0 batch_num: 121 lr: 0 loss: 0.892383344594 acc: 0.711621972625
INFO: 2017-08-28 23:54:35: main.py:86 **  epoch: 0 batch_num: 122 lr: 0 loss: 0.890443407181 acc: 0.712791201303
INFO: 2017-08-28 23:54:35: main.py:86 **  epoch: 0 batch_num: 123 lr: 0 loss: 0.895894377222 acc: 0.709546280724
INFO: 2017-08-28 23:54:36: main.py:86 **  epoch: 0 batch_num: 124 lr: 0 loss: 0.893850416899 acc: 0.709813229799
INFO: 2017-08-28 23:54:36: main.py:86 **  epoch: 0 batch_num: 125 lr: 0 loss: 0.890874317478 acc: 0.711237034864
INFO: 2017-08-28 23:54:37: main.py:86 **  epoch: 0 batch_num: 126 lr: 0 loss: 0.891321772431 acc: 0.709897807968
INFO: 2017-08-28 23:54:37: main.py:86 **  epoch: 0 batch_num: 127 lr: 0 loss: 0.88820113847 acc: 0.711076232838
INFO: 2017-08-28 23:54:38: main.py:86 **  epoch: 0 batch_num: 128 lr: 0 loss: 0.891593700694 acc: 0.709356397621
INFO: 2017-08-28 23:54:38: main.py:86 **  epoch: 0 batch_num: 129 lr: 0 loss: 0.891326234432 acc: 0.708743027999
INFO: 2017-08-28 23:54:38: main.py:86 **  epoch: 0 batch_num: 130 lr: 0 loss: 0.888109900114 acc: 0.709883411422
INFO: 2017-08-28 23:54:39: main.py:86 **  epoch: 0 batch_num: 131 lr: 0 loss: 0.885500956214 acc: 0.711114337047
INFO: 2017-08-28 23:54:39: main.py:86 **  epoch: 0 batch_num: 132 lr: 0 loss: 0.883099765258 acc: 0.712533897475
INFO: 2017-08-28 23:54:40: main.py:86 **  epoch: 0 batch_num: 133 lr: 0 loss: 0.887845238198 acc: 0.71217556409
INFO: 2017-08-28 23:54:40: main.py:86 **  epoch: 0 batch_num: 134 lr: 0 loss: 0.886104200063 acc: 0.712924305156
INFO: 2017-08-28 23:54:41: main.py:86 **  epoch: 0 batch_num: 135 lr: 0 loss: 0.889089974849 acc: 0.710783565088
INFO: 2017-08-28 23:54:41: main.py:86 **  epoch: 0 batch_num: 136 lr: 0 loss: 0.887674933802 acc: 0.711394058744
INFO: 2017-08-28 23:54:41: main.py:86 **  epoch: 0 batch_num: 137 lr: 0 loss: 0.884077650481 acc: 0.712810505344
INFO: 2017-08-28 23:54:42: main.py:86 **  epoch: 0 batch_num: 138 lr: 0 loss: 0.881744495828 acc: 0.714089979156
INFO: 2017-08-28 23:54:42: main.py:86 **  epoch: 0 batch_num: 139 lr: 0 loss: 0.880250744309 acc: 0.715116908082
INFO: 2017-08-28 23:54:43: main.py:86 **  epoch: 0 batch_num: 140 lr: 0 loss: 0.880133831755 acc: 0.713812235611
INFO: 2017-08-28 23:54:43: main.py:86 **  epoch: 0 batch_num: 141 lr: 0 loss: 0.87763113539 acc: 0.714659117267
INFO: 2017-08-28 23:54:44: main.py:86 **  epoch: 0 batch_num: 142 lr: 0 loss: 0.881200256881 acc: 0.712008402064
INFO: 2017-08-28 23:54:44: main.py:86 **  epoch: 0 batch_num: 143 lr: 0 loss: 0.878105476085 acc: 0.713315225724
INFO: 2017-08-28 23:54:44: main.py:86 **  epoch: 0 batch_num: 144 lr: 0 loss: 0.876880240851 acc: 0.716432525783
INFO: 2017-08-28 23:54:45: main.py:86 **  epoch: 0 batch_num: 145 lr: 0 loss: 0.884120464733 acc: 0.712146266274
INFO: 2017-08-28 23:54:45: main.py:86 **  epoch: 0 batch_num: 146 lr: 0 loss: 0.882637249775 acc: 0.712231802292
INFO: 2017-08-28 23:54:46: main.py:86 **  epoch: 0 batch_num: 147 lr: 0 loss: 0.881279106076 acc: 0.712529977431
INFO: 2017-08-28 23:54:46: main.py:86 **  epoch: 0 batch_num: 148 lr: 0 loss: 0.878659899603 acc: 0.713706923011
INFO: 2017-08-28 23:54:46: main.py:86 **  epoch: 0 batch_num: 149 lr: 0 loss: 0.875944339236 acc: 0.714632174969
INFO: 2017-08-28 23:54:47: main.py:86 **  epoch: 0 batch_num: 150 lr: 0 loss: 0.874158358534 acc: 0.715187915114
INFO: 2017-08-28 23:54:47: main.py:86 **  epoch: 0 batch_num: 151 lr: 0 loss: 0.872412999014 acc: 0.716009431764
INFO: 2017-08-28 23:54:48: main.py:86 **  epoch: 0 batch_num: 152 lr: 0 loss: 0.870561993005 acc: 0.716617659806
INFO: 2017-08-28 23:54:48: main.py:86 **  epoch: 0 batch_num: 153 lr: 0 loss: 0.868260146542 acc: 0.717694323558
INFO: 2017-08-28 23:54:49: main.py:86 **  epoch: 0 batch_num: 154 lr: 0 loss: 0.867249603618 acc: 0.717613354421
INFO: 2017-08-28 23:54:49: main.py:86 **  epoch: 0 batch_num: 155 lr: 0 loss: 0.866178700748 acc: 0.718424482605
INFO: 2017-08-28 23:54:49: main.py:86 **  epoch: 0 batch_num: 156 lr: 0 loss: 0.863508043585 acc: 0.71929136545
INFO: 2017-08-28 23:54:50: main.py:86 **  epoch: 0 batch_num: 157 lr: 0 loss: 0.860879916368 acc: 0.72013235394
INFO: 2017-08-28 23:54:50: main.py:86 **  epoch: 0 batch_num: 158 lr: 0 loss: 0.860088943123 acc: 0.719954681471
INFO: 2017-08-28 23:54:51: main.py:86 **  epoch: 0 batch_num: 159 lr: 0 loss: 0.858920397796 acc: 0.720619366691
INFO: 2017-08-28 23:54:51: main.py:86 **  epoch: 0 batch_num: 160 lr: 0 loss: 0.857397514286 acc: 0.721280067234
INFO: 2017-08-28 23:54:52: main.py:86 **  epoch: 0 batch_num: 161 lr: 0 loss: 0.856057135227 acc: 0.721561137909
INFO: 2017-08-28 23:54:52: main.py:86 **  epoch: 0 batch_num: 162 lr: 0 loss: 0.855181647042 acc: 0.721185437375
INFO: 2017-08-28 23:54:52: main.py:86 **  epoch: 0 batch_num: 163 lr: 0 loss: 0.856152923187 acc: 0.720376609666
INFO: 2017-08-28 23:54:53: main.py:86 **  epoch: 0 batch_num: 164 lr: 0 loss: 0.85379076275 acc: 0.720629553361
INFO: 2017-08-28 23:54:53: main.py:86 **  epoch: 0 batch_num: 165 lr: 0 loss: 0.853648361492 acc: 0.719199123691
INFO: 2017-08-28 23:54:54: main.py:86 **  epoch: 0 batch_num: 166 lr: 0 loss: 0.851014882326 acc: 0.720124608564
INFO: 2017-08-28 23:54:54: main.py:86 **  epoch: 0 batch_num: 167 lr: 0 loss: 0.850763094745 acc: 0.718899441085
INFO: 2017-08-28 23:54:55: main.py:86 **  epoch: 0 batch_num: 168 lr: 0 loss: 0.849400976882 acc: 0.719050993404
INFO: 2017-08-28 23:54:55: main.py:86 **  epoch: 0 batch_num: 169 lr: 0 loss: 0.848280246994 acc: 0.718665957276
INFO: 2017-08-28 23:54:56: main.py:86 **  epoch: 0 batch_num: 170 lr: 0 loss: 0.847122189769 acc: 0.720766035263
INFO: 2017-08-28 23:54:56: main.py:86 **  epoch: 0 batch_num: 171 lr: 0 loss: 0.845324938207 acc: 0.720369087575
INFO: 2017-08-28 23:54:56: main.py:86 **  epoch: 0 batch_num: 172 lr: 0 loss: 0.845139211485 acc: 0.720587450817
INFO: 2017-08-28 23:54:57: main.py:86 **  epoch: 0 batch_num: 173 lr: 0 loss: 0.843627710623 acc: 0.720734095608
INFO: 2017-08-28 23:54:57: main.py:86 **  epoch: 0 batch_num: 174 lr: 0 loss: 0.841951971224 acc: 0.72138097167
INFO: 2017-08-28 23:54:58: main.py:86 **  epoch: 0 batch_num: 175 lr: 0 loss: 0.840805284848 acc: 0.720841220665
INFO: 2017-08-28 23:54:58: main.py:86 **  epoch: 0 batch_num: 176 lr: 0 loss: 0.841346142487 acc: 0.719110005825
INFO: 2017-08-28 23:54:59: main.py:86 **  epoch: 0 batch_num: 177 lr: 0 loss: 0.841261470753 acc: 0.719291875034
INFO: 2017-08-28 23:54:59: main.py:86 **  epoch: 0 batch_num: 178 lr: 0 loss: 0.838749546079 acc: 0.720440793803
INFO: 2017-08-28 23:54:59: main.py:86 **  epoch: 0 batch_num: 179 lr: 0 loss: 0.836154578295 acc: 0.721538534264
INFO: 2017-08-28 23:55:00: main.py:86 **  epoch: 0 batch_num: 180 lr: 0 loss: 0.834259635012 acc: 0.72205070188
INFO: 2017-08-28 23:55:00: main.py:86 **  epoch: 0 batch_num: 181 lr: 0 loss: 0.832184125106 acc: 0.72294923614
INFO: 2017-08-28 23:55:01: main.py:86 **  epoch: 0 batch_num: 182 lr: 0 loss: 0.829339091081 acc: 0.724282724447
INFO: 2017-08-28 23:55:01: main.py:86 **  epoch: 0 batch_num: 183 lr: 0 loss: 0.826942162993 acc: 0.725216355499
INFO: 2017-08-28 23:55:02: main.py:86 **  epoch: 0 batch_num: 184 lr: 0 loss: 0.824441510278 acc: 0.726378595346
INFO: 2017-08-28 23:55:02: main.py:86 **  epoch: 0 batch_num: 185 lr: 0 loss: 0.824005678777 acc: 0.725621292508
INFO: 2017-08-28 23:55:02: main.py:86 **  epoch: 0 batch_num: 186 lr: 0 loss: 0.823267129653 acc: 0.725769286328
INFO: 2017-08-28 23:55:03: main.py:86 **  epoch: 0 batch_num: 187 lr: 0 loss: 0.820632717394 acc: 0.726765352836
INFO: 2017-08-28 23:55:03: main.py:86 **  epoch: 0 batch_num: 188 lr: 0 loss: 0.819401063301 acc: 0.727642377375
INFO: 2017-08-28 23:55:04: main.py:86 **  epoch: 0 batch_num: 189 lr: 0 loss: 0.817021010424 acc: 0.728358318461
INFO: 2017-08-28 23:55:04: main.py:86 **  epoch: 0 batch_num: 190 lr: 0 loss: 0.815923878036 acc: 0.728260960722
INFO: 2017-08-28 23:55:05: main.py:86 **  epoch: 0 batch_num: 191 lr: 0 loss: 0.815354043618 acc: 0.728257359161
INFO: 2017-08-28 23:55:05: main.py:86 **  epoch: 0 batch_num: 192 lr: 0 loss: 0.814273868509 acc: 0.727430631614
INFO: 2017-08-28 23:55:06: main.py:86 **  epoch: 0 batch_num: 193 lr: 0 loss: 0.812683451729 acc: 0.728494218791
INFO: 2017-08-28 23:55:06: main.py:86 **  epoch: 0 batch_num: 194 lr: 0 loss: 0.813844206089 acc: 0.729050649282
INFO: 2017-08-28 23:55:06: main.py:86 **  epoch: 0 batch_num: 195 lr: 0 loss: 0.81179006048 acc: 0.729364663819
INFO: 2017-08-28 23:55:07: main.py:86 **  epoch: 0 batch_num: 196 lr: 0 loss: 0.809517872364 acc: 0.730368847018
INFO: 2017-08-28 23:55:07: main.py:86 **  epoch: 0 batch_num: 197 lr: 0 loss: 0.809056935136 acc: 0.729521656729
INFO: 2017-08-28 23:55:08: main.py:86 **  epoch: 0 batch_num: 198 lr: 0 loss: 0.815313001823 acc: 0.726462592132
INFO: 2017-08-28 23:55:08: main.py:86 **  epoch: 0 batch_num: 199 lr: 0 loss: 0.813362350613 acc: 0.727297520041
INFO: 2017-08-28 23:55:09: main.py:86 **  epoch: 0 batch_num: 200 lr: 0 loss: 0.813084172817 acc: 0.726931794366
INFO: 2017-08-28 23:55:09: main.py:86 **  epoch: 0 batch_num: 201 lr: 0 loss: 0.812964800118 acc: 0.727689589309
INFO: 2017-08-28 23:55:09: main.py:86 **  epoch: 0 batch_num: 202 lr: 0 loss: 0.811726509791 acc: 0.728260725003
INFO: 2017-08-28 23:55:10: main.py:86 **  epoch: 0 batch_num: 203 lr: 0 loss: 0.81015231957 acc: 0.729077267296
INFO: 2017-08-28 23:55:10: main.py:86 **  epoch: 0 batch_num: 204 lr: 0 loss: 0.808814678977 acc: 0.729320334516
INFO: 2017-08-28 23:55:11: main.py:86 **  epoch: 0 batch_num: 205 lr: 0 loss: 0.808210458426 acc: 0.730026733817
INFO: 2017-08-28 23:55:11: main.py:86 **  epoch: 0 batch_num: 206 lr: 0 loss: 0.807326466421 acc: 0.730466129987
INFO: 2017-08-28 23:55:12: main.py:86 **  epoch: 0 batch_num: 207 lr: 0 loss: 0.805467077603 acc: 0.731192352393
INFO: 2017-08-28 23:55:12: main.py:86 **  epoch: 0 batch_num: 208 lr: 0 loss: 0.803900299888 acc: 0.731643822205
INFO: 2017-08-28 23:55:12: main.py:86 **  epoch: 0 batch_num: 209 lr: 0 loss: 0.802402816926 acc: 0.732363710233
INFO: 2017-08-28 23:55:13: main.py:86 **  epoch: 0 batch_num: 210 lr: 0 loss: 0.800760148261 acc: 0.732807220723
INFO: 2017-08-28 23:55:13: main.py:86 **  epoch: 0 batch_num: 211 lr: 0 loss: 0.800517443778 acc: 0.732237626359
INFO: 2017-08-28 23:55:14: main.py:86 **  epoch: 0 batch_num: 212 lr: 0 loss: 0.798958941245 acc: 0.732654990445
INFO: 2017-08-28 23:55:14: main.py:86 **  epoch: 0 batch_num: 213 lr: 0 loss: 0.798494308908 acc: 0.73245516308
INFO: 2017-08-28 23:55:15: main.py:86 **  epoch: 0 batch_num: 214 lr: 0 loss: 0.79715858282 acc: 0.732463534765
INFO: 2017-08-28 23:55:15: main.py:86 **  epoch: 0 batch_num: 215 lr: 0 loss: 0.795150965728 acc: 0.73315255951
INFO: 2017-08-28 23:55:16: main.py:86 **  epoch: 0 batch_num: 216 lr: 0 loss: 0.793182544994 acc: 0.733913228808
INFO: 2017-08-28 23:55:16: main.py:86 **  epoch: 0 batch_num: 217 lr: 0 loss: 0.795148604507 acc: 0.732069582567
INFO: 2017-08-28 23:55:16: main.py:86 **  epoch: 0 batch_num: 218 lr: 0 loss: 0.793742722028 acc: 0.732483524982
INFO: 2017-08-28 23:55:17: main.py:86 **  epoch: 0 batch_num: 219 lr: 0 loss: 0.791769791733 acc: 0.73327408162
INFO: 2017-08-28 23:55:17: main.py:86 **  epoch: 0 batch_num: 220 lr: 0 loss: 0.789822434138 acc: 0.733983534494
INFO: 2017-08-28 23:55:24: main.py:28 **  start
INFO: 2017-08-28 23:55:24: main.py:30 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-28 23:55:24: main.py:31 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-28 23:55:24: main.py:32 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-28 23:55:24: main.py:33 **  读取数据集...
INFO: 2017-08-28 23:55:27: main.py:44 **  train data sample counts 5088
INFO: 2017-08-28 23:55:27: main.py:45 **  train data set batch size 16
INFO: 2017-08-28 23:55:27: main.py:46 **  train data batch counts 318
INFO: 2017-08-28 23:55:29: main.py:53 **  <class 'net.net.CarUNet'>


INFO: 2017-08-28 23:55:44: main.py:28 **  start
INFO: 2017-08-28 23:55:44: main.py:30 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-28 23:55:44: main.py:31 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-28 23:55:44: main.py:32 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-28 23:55:44: main.py:33 **  读取数据集...
INFO: 2017-08-28 23:55:47: main.py:44 **  train data sample counts 5088
INFO: 2017-08-28 23:55:47: main.py:45 **  train data set batch size 8
INFO: 2017-08-28 23:55:47: main.py:46 **  train data batch counts 636
INFO: 2017-08-28 23:55:48: main.py:53 **  <class 'net.net.CarUNet'>


INFO: 2017-08-28 23:56:09: main.py:28 **  start
INFO: 2017-08-28 23:56:09: main.py:30 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-28 23:56:09: main.py:31 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-28 23:56:09: main.py:32 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-28 23:56:09: main.py:33 **  读取数据集...
INFO: 2017-08-28 23:56:12: main.py:44 **  train data sample counts 5088
INFO: 2017-08-28 23:56:12: main.py:45 **  train data set batch size 6
INFO: 2017-08-28 23:56:12: main.py:46 **  train data batch counts 848
INFO: 2017-08-28 23:56:13: main.py:53 **  <class 'net.net.CarUNet'>


INFO: 2017-08-28 23:56:15: main.py:86 **  epoch: 0 batch_num: 0 lr: 0 loss: 1.34152781963 acc: 0.484533846378
INFO: 2017-08-28 23:56:16: main.py:86 **  epoch: 0 batch_num: 1 lr: 0 loss: 1.3553134799 acc: 0.407298743725
INFO: 2017-08-28 23:56:16: main.py:86 **  epoch: 0 batch_num: 2 lr: 0 loss: 1.29304711024 acc: 0.472776830196
INFO: 2017-08-28 23:56:17: main.py:86 **  epoch: 0 batch_num: 3 lr: 0 loss: 1.28040236235 acc: 0.500600010157
INFO: 2017-08-28 23:56:17: main.py:86 **  epoch: 0 batch_num: 4 lr: 0 loss: 1.26421499252 acc: 0.5234557271
INFO: 2017-08-28 23:56:18: main.py:86 **  epoch: 0 batch_num: 5 lr: 0 loss: 1.25280336539 acc: 0.529240955909
INFO: 2017-08-28 23:56:19: main.py:86 **  epoch: 0 batch_num: 6 lr: 0 loss: 1.25570225716 acc: 0.531018214566
INFO: 2017-08-28 23:56:19: main.py:86 **  epoch: 0 batch_num: 7 lr: 0 loss: 1.23963391781 acc: 0.538070045412
INFO: 2017-08-28 23:56:20: main.py:86 **  epoch: 0 batch_num: 8 lr: 0 loss: 1.22530266974 acc: 0.554903010527
INFO: 2017-08-28 23:56:20: main.py:86 **  epoch: 0 batch_num: 9 lr: 0 loss: 1.21706722975 acc: 0.566364580393
INFO: 2017-08-28 23:56:21: main.py:86 **  epoch: 0 batch_num: 10 lr: 0 loss: 1.2176366611 acc: 0.558205951344
INFO: 2017-08-28 23:56:22: main.py:86 **  epoch: 0 batch_num: 11 lr: 0 loss: 1.22370931506 acc: 0.542244459192
INFO: 2017-08-28 23:56:22: main.py:86 **  epoch: 0 batch_num: 12 lr: 0 loss: 1.2196269127 acc: 0.543604043814
INFO: 2017-08-28 23:56:23: main.py:86 **  epoch: 0 batch_num: 13 lr: 0 loss: 1.22132634265 acc: 0.537907132081
INFO: 2017-08-28 23:56:23: main.py:86 **  epoch: 0 batch_num: 14 lr: 0 loss: 1.21825181643 acc: 0.534793591499
INFO: 2017-08-28 23:56:24: main.py:86 **  epoch: 0 batch_num: 15 lr: 0 loss: 1.21599618345 acc: 0.537015601993
INFO: 2017-08-28 23:56:25: main.py:86 **  epoch: 0 batch_num: 16 lr: 0 loss: 1.20771651408 acc: 0.537807247218
INFO: 2017-08-28 23:56:25: main.py:86 **  epoch: 0 batch_num: 17 lr: 0 loss: 1.20060212082 acc: 0.548089494308
INFO: 2017-08-28 23:56:26: main.py:86 **  epoch: 0 batch_num: 18 lr: 0 loss: 1.20028389128 acc: 0.551703820103
INFO: 2017-08-28 23:56:26: main.py:86 **  epoch: 0 batch_num: 19 lr: 0 loss: 1.19564185739 acc: 0.55666718483
INFO: 2017-08-28 23:56:27: main.py:86 **  epoch: 0 batch_num: 20 lr: 0 loss: 1.18921686922 acc: 0.559608487856
INFO: 2017-08-28 23:56:28: main.py:86 **  epoch: 0 batch_num: 21 lr: 0 loss: 1.18592028726 acc: 0.565118944103
INFO: 2017-08-28 23:56:28: main.py:86 **  epoch: 0 batch_num: 22 lr: 0 loss: 1.1807710347 acc: 0.568424375161
INFO: 2017-08-28 23:56:29: main.py:86 **  epoch: 0 batch_num: 23 lr: 0 loss: 1.17574451367 acc: 0.568018520872
INFO: 2017-08-28 23:56:29: main.py:86 **  epoch: 0 batch_num: 24 lr: 0 loss: 1.16870077133 acc: 0.574680695534
INFO: 2017-08-28 23:56:30: main.py:86 **  epoch: 0 batch_num: 25 lr: 0 loss: 1.16942155361 acc: 0.572772266773
INFO: 2017-08-28 23:56:31: main.py:86 **  epoch: 0 batch_num: 26 lr: 0 loss: 1.16974913632 acc: 0.57486243822
INFO: 2017-08-28 23:56:31: main.py:86 **  epoch: 0 batch_num: 27 lr: 0 loss: 1.16169825196 acc: 0.582651608757
INFO: 2017-08-28 23:56:32: main.py:86 **  epoch: 0 batch_num: 28 lr: 0 loss: 1.15587625421 acc: 0.583094126192
INFO: 2017-08-28 23:56:32: main.py:86 **  epoch: 0 batch_num: 29 lr: 0 loss: 1.15280441046 acc: 0.588795252641
INFO: 2017-08-28 23:56:33: main.py:86 **  epoch: 0 batch_num: 30 lr: 0 loss: 1.14424558224 acc: 0.595553869201
INFO: 2017-08-28 23:56:34: main.py:86 **  epoch: 0 batch_num: 31 lr: 0 loss: 1.14076272771 acc: 0.598364876583
INFO: 2017-08-28 23:56:34: main.py:86 **  epoch: 0 batch_num: 32 lr: 0 loss: 1.13402246345 acc: 0.604276006872
INFO: 2017-08-28 23:56:35: main.py:86 **  epoch: 0 batch_num: 33 lr: 0 loss: 1.12820066775 acc: 0.60809378063
INFO: 2017-08-28 23:56:36: main.py:86 **  epoch: 0 batch_num: 34 lr: 0 loss: 1.1294307879 acc: 0.604596602917
INFO: 2017-08-28 23:56:36: main.py:86 **  epoch: 0 batch_num: 35 lr: 0 loss: 1.12349206871 acc: 0.606396072441
INFO: 2017-08-28 23:56:37: main.py:86 **  epoch: 0 batch_num: 36 lr: 0 loss: 1.11725344368 acc: 0.611171332566
INFO: 2017-08-28 23:56:37: main.py:86 **  epoch: 0 batch_num: 37 lr: 0 loss: 1.1119725563 acc: 0.616526597425
INFO: 2017-08-28 23:56:38: main.py:86 **  epoch: 0 batch_num: 38 lr: 0 loss: 1.10702868761 acc: 0.623541268019
INFO: 2017-08-28 23:56:39: main.py:86 **  epoch: 0 batch_num: 39 lr: 0 loss: 1.10116646141 acc: 0.625819566846
INFO: 2017-08-28 23:56:39: main.py:86 **  epoch: 0 batch_num: 40 lr: 0 loss: 1.09477702147 acc: 0.630764286693
INFO: 2017-08-28 23:56:40: main.py:86 **  epoch: 0 batch_num: 41 lr: 0 loss: 1.09033290403 acc: 0.631799584343
INFO: 2017-08-28 23:56:40: main.py:86 **  epoch: 0 batch_num: 42 lr: 0 loss: 1.0850175633 acc: 0.64137673378
INFO: 2017-08-28 23:56:41: main.py:86 **  epoch: 0 batch_num: 43 lr: 0 loss: 1.0791609057 acc: 0.644611816515
INFO: 2017-08-28 23:56:42: main.py:86 **  epoch: 0 batch_num: 44 lr: 0 loss: 1.07510232528 acc: 0.645612920655
INFO: 2017-08-28 23:56:42: main.py:86 **  epoch: 0 batch_num: 45 lr: 0 loss: 1.0691800571 acc: 0.649918680606
INFO: 2017-08-28 23:56:43: main.py:86 **  epoch: 0 batch_num: 46 lr: 0 loss: 1.06040579588 acc: 0.654026532427
INFO: 2017-08-28 23:56:44: main.py:86 **  epoch: 0 batch_num: 47 lr: 0 loss: 1.055956278 acc: 0.655998869489
INFO: 2017-08-28 23:56:44: main.py:86 **  epoch: 0 batch_num: 48 lr: 0 loss: 1.05143856394 acc: 0.656840491052
INFO: 2017-08-28 23:56:45: main.py:86 **  epoch: 0 batch_num: 49 lr: 0 loss: 1.0484975183 acc: 0.659060468674
INFO: 2017-08-28 23:56:45: main.py:86 **  epoch: 0 batch_num: 50 lr: 0 loss: 1.04287129874 acc: 0.662480300548
INFO: 2017-08-28 23:56:46: main.py:86 **  epoch: 0 batch_num: 51 lr: 0 loss: 1.03896867197 acc: 0.663811289347
INFO: 2017-08-28 23:56:47: main.py:86 **  epoch: 0 batch_num: 52 lr: 0 loss: 1.03486931436 acc: 0.666496824543
INFO: 2017-08-28 23:56:47: main.py:86 **  epoch: 0 batch_num: 53 lr: 0 loss: 1.03228919042 acc: 0.67201476075
INFO: 2017-08-28 23:56:48: main.py:86 **  epoch: 0 batch_num: 54 lr: 0 loss: 1.03118681366 acc: 0.672283323245
INFO: 2017-08-28 23:56:49: main.py:86 **  epoch: 0 batch_num: 55 lr: 0 loss: 1.03201198684 acc: 0.67137364511
INFO: 2017-08-28 23:56:49: main.py:86 **  epoch: 0 batch_num: 56 lr: 0 loss: 1.03595089599 acc: 0.672608583643
INFO: 2017-08-28 23:56:50: main.py:86 **  epoch: 0 batch_num: 57 lr: 0 loss: 1.03143543313 acc: 0.678214460611
INFO: 2017-08-28 23:56:50: main.py:86 **  epoch: 0 batch_num: 58 lr: 0 loss: 1.02885083526 acc: 0.677581741648
INFO: 2017-08-28 23:56:51: main.py:86 **  epoch: 0 batch_num: 59 lr: 0 loss: 1.02362890343 acc: 0.677441010873
INFO: 2017-08-28 23:56:52: main.py:86 **  epoch: 0 batch_num: 60 lr: 0 loss: 1.02092423107 acc: 0.678778619063
INFO: 2017-08-28 23:56:52: main.py:86 **  epoch: 0 batch_num: 61 lr: 0 loss: 1.01658882345 acc: 0.678964365875
INFO: 2017-08-28 23:56:53: main.py:86 **  epoch: 0 batch_num: 62 lr: 0 loss: 1.0101103565 acc: 0.681061883767
INFO: 2017-08-28 23:56:54: main.py:86 **  epoch: 0 batch_num: 63 lr: 0 loss: 1.00972404983 acc: 0.679004780017
INFO: 2017-08-28 23:56:54: main.py:86 **  epoch: 0 batch_num: 64 lr: 0 loss: 1.00258915699 acc: 0.681954795581
INFO: 2017-08-28 23:56:55: main.py:86 **  epoch: 0 batch_num: 65 lr: 0 loss: 0.997413427541 acc: 0.687884813908
INFO: 2017-08-28 23:56:56: main.py:86 **  epoch: 0 batch_num: 66 lr: 0 loss: 0.99639845784 acc: 0.685575174752
INFO: 2017-08-28 23:56:56: main.py:86 **  epoch: 0 batch_num: 67 lr: 0 loss: 0.992668363978 acc: 0.687858818208
INFO: 2017-08-28 23:56:57: main.py:86 **  epoch: 0 batch_num: 68 lr: 0 loss: 0.98765663997 acc: 0.690034708251
INFO: 2017-08-28 23:56:57: main.py:86 **  epoch: 0 batch_num: 69 lr: 0 loss: 0.98202248386 acc: 0.69147522194
INFO: 2017-08-28 23:56:58: main.py:86 **  epoch: 0 batch_num: 70 lr: 0 loss: 0.977426069723 acc: 0.692892171128
INFO: 2017-08-28 23:56:59: main.py:86 **  epoch: 0 batch_num: 71 lr: 0 loss: 0.972112031447 acc: 0.69416530182
INFO: 2017-08-28 23:56:59: main.py:86 **  epoch: 0 batch_num: 72 lr: 0 loss: 0.968965759016 acc: 0.694480016623
INFO: 2017-08-28 23:57:00: main.py:86 **  epoch: 0 batch_num: 73 lr: 0 loss: 0.966649018429 acc: 0.694253050798
INFO: 2017-08-28 23:57:01: main.py:86 **  epoch: 0 batch_num: 74 lr: 0 loss: 0.959834468365 acc: 0.697124492327
INFO: 2017-08-28 23:57:01: main.py:86 **  epoch: 0 batch_num: 75 lr: 0 loss: 0.956993080283 acc: 0.698640690038
INFO: 2017-08-28 23:57:02: main.py:86 **  epoch: 0 batch_num: 76 lr: 0 loss: 0.953138909557 acc: 0.699592971957
INFO: 2017-08-28 23:57:03: main.py:86 **  epoch: 0 batch_num: 77 lr: 0 loss: 0.950078277252 acc: 0.699130247037
INFO: 2017-08-28 23:57:03: main.py:86 **  epoch: 0 batch_num: 78 lr: 0 loss: 0.947409615486 acc: 0.700406905217
INFO: 2017-08-28 23:57:04: main.py:86 **  epoch: 0 batch_num: 79 lr: 0 loss: 0.943291794509 acc: 0.701277776808
INFO: 2017-08-28 23:57:04: main.py:86 **  epoch: 0 batch_num: 80 lr: 0 loss: 0.938347011437 acc: 0.70207055189
INFO: 2017-08-28 23:57:05: main.py:86 **  epoch: 0 batch_num: 81 lr: 0 loss: 0.934004904293 acc: 0.703480657281
INFO: 2017-08-28 23:57:06: main.py:86 **  epoch: 0 batch_num: 82 lr: 0 loss: 0.931628067809 acc: 0.704497823514
INFO: 2017-08-28 23:57:06: main.py:86 **  epoch: 0 batch_num: 83 lr: 0 loss: 0.929052105972 acc: 0.70484712649
INFO: 2017-08-28 23:57:07: main.py:86 **  epoch: 0 batch_num: 84 lr: 0 loss: 0.924824522523 acc: 0.70682105387
INFO: 2017-08-28 23:57:08: main.py:86 **  epoch: 0 batch_num: 85 lr: 0 loss: 0.922034282324 acc: 0.70799350392
INFO: 2017-08-28 23:57:08: main.py:86 **  epoch: 0 batch_num: 86 lr: 0 loss: 0.91638664336 acc: 0.7101934607
INFO: 2017-08-28 23:57:09: main.py:86 **  epoch: 0 batch_num: 87 lr: 0 loss: 0.911345935003 acc: 0.712028661235
INFO: 2017-08-28 23:57:10: main.py:86 **  epoch: 0 batch_num: 88 lr: 0 loss: 0.90872363495 acc: 0.712556842338
INFO: 2017-08-28 23:57:10: main.py:86 **  epoch: 0 batch_num: 89 lr: 0 loss: 0.906385957532 acc: 0.713700779941
INFO: 2017-08-28 23:57:11: main.py:86 **  epoch: 0 batch_num: 90 lr: 0 loss: 0.902843273603 acc: 0.714663033302
INFO: 2017-08-28 23:57:11: main.py:86 **  epoch: 0 batch_num: 91 lr: 0 loss: 0.898371040173 acc: 0.714999469726
INFO: 2017-08-28 23:57:12: main.py:86 **  epoch: 0 batch_num: 92 lr: 0 loss: 0.897310858132 acc: 0.71539105523
INFO: 2017-08-28 23:57:13: main.py:86 **  epoch: 0 batch_num: 93 lr: 0 loss: 0.892763452961 acc: 0.716540182525
INFO: 2017-08-28 23:57:13: main.py:86 **  epoch: 0 batch_num: 94 lr: 0 loss: 0.89158682384 acc: 0.716032716475
INFO: 2017-08-28 23:57:14: main.py:86 **  epoch: 0 batch_num: 95 lr: 0 loss: 0.888178147996 acc: 0.717558622981
INFO: 2017-08-28 23:57:15: main.py:86 **  epoch: 0 batch_num: 96 lr: 0 loss: 0.885696037528 acc: 0.718150544412
INFO: 2017-08-28 23:57:15: main.py:86 **  epoch: 0 batch_num: 97 lr: 0 loss: 0.883167113577 acc: 0.719382899148
INFO: 2017-08-28 23:57:16: main.py:86 **  epoch: 0 batch_num: 98 lr: 0 loss: 0.881421964578 acc: 0.722168586471
INFO: 2017-08-28 23:57:17: main.py:86 **  epoch: 0 batch_num: 99 lr: 0 loss: 0.878122446537 acc: 0.723673155904
INFO: 2017-08-28 23:57:17: main.py:86 **  epoch: 0 batch_num: 100 lr: 0 loss: 0.877054153102 acc: 0.723310248096
INFO: 2017-08-28 23:57:18: main.py:86 **  epoch: 0 batch_num: 101 lr: 0 loss: 0.875442303863 acc: 0.723255502827
INFO: 2017-08-28 23:57:19: main.py:86 **  epoch: 0 batch_num: 102 lr: 0 loss: 0.873881397317 acc: 0.725526424288
INFO: 2017-08-28 23:57:19: main.py:86 **  epoch: 0 batch_num: 103 lr: 0 loss: 0.871194757521 acc: 0.725400592272
INFO: 2017-08-28 23:57:20: main.py:86 **  epoch: 0 batch_num: 104 lr: 0 loss: 0.869011716048 acc: 0.725325520833
INFO: 2017-08-28 23:57:21: main.py:86 **  epoch: 0 batch_num: 105 lr: 0 loss: 0.867918028584 acc: 0.724007525534
INFO: 2017-08-28 23:57:21: main.py:86 **  epoch: 0 batch_num: 106 lr: 0 loss: 0.864828181601 acc: 0.724956655057
INFO: 2017-08-28 23:57:22: main.py:86 **  epoch: 0 batch_num: 107 lr: 0 loss: 0.862304344773 acc: 0.72561616147
INFO: 2017-08-28 23:57:22: main.py:86 **  epoch: 0 batch_num: 108 lr: 0 loss: 0.860546411177 acc: 0.725047207754
INFO: 2017-08-28 23:57:23: main.py:86 **  epoch: 0 batch_num: 109 lr: 0 loss: 0.862006328865 acc: 0.721362118288
INFO: 2017-08-28 23:57:24: main.py:86 **  epoch: 0 batch_num: 110 lr: 0 loss: 0.860346425224 acc: 0.723475765001
INFO: 2017-08-28 23:57:24: main.py:86 **  epoch: 0 batch_num: 111 lr: 0 loss: 0.857113246407 acc: 0.723743929395
INFO: 2017-08-28 23:57:25: main.py:86 **  epoch: 0 batch_num: 112 lr: 0 loss: 0.857405136644 acc: 0.722372415846
INFO: 2017-08-28 23:57:26: main.py:86 **  epoch: 0 batch_num: 113 lr: 0 loss: 0.858862899375 acc: 0.721326163463
INFO: 2017-08-28 23:57:26: main.py:86 **  epoch: 0 batch_num: 114 lr: 0 loss: 0.85553634685 acc: 0.721903321536
INFO: 2017-08-28 23:57:27: main.py:86 **  epoch: 0 batch_num: 115 lr: 0 loss: 0.858019526662 acc: 0.720556636823
INFO: 2017-08-28 23:57:28: main.py:86 **  epoch: 0 batch_num: 116 lr: 0 loss: 0.855510726953 acc: 0.720882557906
INFO: 2017-08-28 23:57:28: main.py:86 **  epoch: 0 batch_num: 117 lr: 0 loss: 0.851656352564 acc: 0.722301999896
INFO: 2017-08-28 23:57:29: main.py:86 **  epoch: 0 batch_num: 118 lr: 0 loss: 0.849354881198 acc: 0.722373446497
INFO: 2017-08-28 23:57:30: main.py:86 **  epoch: 0 batch_num: 119 lr: 0 loss: 0.847151594857 acc: 0.723057518899
INFO: 2017-08-28 23:57:30: main.py:86 **  epoch: 0 batch_num: 120 lr: 0 loss: 0.845753197828 acc: 0.722685232143
INFO: 2017-08-28 23:57:31: main.py:86 **  epoch: 0 batch_num: 121 lr: 0 loss: 0.841536792087 acc: 0.723902625139
INFO: 2017-08-28 23:57:32: main.py:86 **  epoch: 0 batch_num: 122 lr: 0 loss: 0.840813583959 acc: 0.724158477008
INFO: 2017-08-28 23:57:32: main.py:86 **  epoch: 0 batch_num: 123 lr: 0 loss: 0.83840305238 acc: 0.726798886253
INFO: 2017-08-28 23:57:33: main.py:86 **  epoch: 0 batch_num: 124 lr: 0 loss: 0.837890106678 acc: 0.72624161005
INFO: 2017-08-28 23:57:34: main.py:86 **  epoch: 0 batch_num: 125 lr: 0 loss: 0.83728232081 acc: 0.726761583298
INFO: 2017-08-28 23:57:34: main.py:86 **  epoch: 0 batch_num: 126 lr: 0 loss: 0.838935347993 acc: 0.724741372067
INFO: 2017-08-28 23:57:35: main.py:86 **  epoch: 0 batch_num: 127 lr: 0 loss: 0.835876752622 acc: 0.725548397284
INFO: 2017-08-28 23:57:35: main.py:86 **  epoch: 0 batch_num: 128 lr: 0 loss: 0.836717937806 acc: 0.725623078124
INFO: 2017-08-28 23:57:36: main.py:86 **  epoch: 0 batch_num: 129 lr: 0 loss: 0.836637130609 acc: 0.724762021578
INFO: 2017-08-28 23:57:37: main.py:86 **  epoch: 0 batch_num: 130 lr: 0 loss: 0.835291588124 acc: 0.725002392103
INFO: 2017-08-28 23:57:37: main.py:86 **  epoch: 0 batch_num: 131 lr: 0 loss: 0.834282696699 acc: 0.725402060332
INFO: 2017-08-28 23:57:38: main.py:86 **  epoch: 0 batch_num: 132 lr: 0 loss: 0.834121719339 acc: 0.724253094734
INFO: 2017-08-28 23:57:39: main.py:86 **  epoch: 0 batch_num: 133 lr: 0 loss: 0.831770196779 acc: 0.726682432107
INFO: 2017-08-28 23:57:39: main.py:86 **  epoch: 0 batch_num: 134 lr: 0 loss: 0.829967308927 acc: 0.727110840215
INFO: 2017-08-28 23:57:40: main.py:86 **  epoch: 0 batch_num: 135 lr: 0 loss: 0.827010553111 acc: 0.727655479137
INFO: 2017-08-28 23:57:41: main.py:86 **  epoch: 0 batch_num: 136 lr: 0 loss: 0.825972585347 acc: 0.728106872444
INFO: 2017-08-28 23:57:41: main.py:86 **  epoch: 0 batch_num: 137 lr: 0 loss: 0.824270040661 acc: 0.728492017241
INFO: 2017-08-28 23:57:42: main.py:86 **  epoch: 0 batch_num: 138 lr: 0 loss: 0.822282765409 acc: 0.728660091651
INFO: 2017-08-28 23:57:43: main.py:86 **  epoch: 0 batch_num: 139 lr: 0 loss: 0.820587310621 acc: 0.729624798042
INFO: 2017-08-28 23:57:44: main.py:86 **  epoch: 0 batch_num: 140 lr: 0 loss: 0.819510685637 acc: 0.730226757256
INFO: 2017-08-28 23:57:44: main.py:86 **  epoch: 0 batch_num: 141 lr: 0 loss: 0.817405667104 acc: 0.731197679127
INFO: 2017-08-28 23:57:45: main.py:86 **  epoch: 0 batch_num: 142 lr: 0 loss: 0.814181892188 acc: 0.732591661957
INFO: 2017-08-28 23:57:45: main.py:86 **  epoch: 0 batch_num: 143 lr: 0 loss: 0.812707376149 acc: 0.733402633419
INFO: 2017-08-28 23:57:46: main.py:86 **  epoch: 0 batch_num: 144 lr: 0 loss: 0.8150922134 acc: 0.731804912255
INFO: 2017-08-28 23:57:47: main.py:86 **  epoch: 0 batch_num: 145 lr: 0 loss: 0.81307920242 acc: 0.734299775264
INFO: 2017-08-28 23:57:47: main.py:86 **  epoch: 0 batch_num: 146 lr: 0 loss: 0.811567324765 acc: 0.735100414072
INFO: 2017-08-28 23:57:48: main.py:86 **  epoch: 0 batch_num: 147 lr: 0 loss: 0.810374926071 acc: 0.735850061517
INFO: 2017-08-28 23:57:49: main.py:86 **  epoch: 0 batch_num: 148 lr: 0 loss: 0.808266415092 acc: 0.73671413628
INFO: 2017-08-28 23:57:49: main.py:86 **  epoch: 0 batch_num: 149 lr: 0 loss: 0.805693956812 acc: 0.737809423208
INFO: 2017-08-28 23:57:50: main.py:86 **  epoch: 0 batch_num: 150 lr: 0 loss: 0.803484329522 acc: 0.738010237154
INFO: 2017-08-28 23:57:51: main.py:86 **  epoch: 0 batch_num: 151 lr: 0 loss: 0.802230499489 acc: 0.73857270142
INFO: 2017-08-28 23:57:51: main.py:86 **  epoch: 0 batch_num: 152 lr: 0 loss: 0.799865348082 acc: 0.738791516014
INFO: 2017-08-28 23:57:52: main.py:86 **  epoch: 0 batch_num: 153 lr: 0 loss: 0.797891049803 acc: 0.739325965767
INFO: 2017-08-28 23:57:53: main.py:86 **  epoch: 0 batch_num: 154 lr: 0 loss: 0.798550846115 acc: 0.738950035264
INFO: 2017-08-28 23:57:53: main.py:86 **  epoch: 0 batch_num: 155 lr: 0 loss: 0.79602165673 acc: 0.739817468402
INFO: 2017-08-28 23:57:54: main.py:86 **  epoch: 0 batch_num: 156 lr: 0 loss: 0.794171167407 acc: 0.740546827863
INFO: 2017-08-28 23:57:55: main.py:86 **  epoch: 0 batch_num: 157 lr: 0 loss: 0.793759541421 acc: 0.73994451384
INFO: 2017-08-28 23:57:55: main.py:86 **  epoch: 0 batch_num: 158 lr: 0 loss: 0.791697848705 acc: 0.740577864197
INFO: 2017-08-28 23:57:56: main.py:86 **  epoch: 0 batch_num: 159 lr: 0 loss: 0.789954067953 acc: 0.741239418089
INFO: 2017-08-28 23:57:57: main.py:86 **  epoch: 0 batch_num: 160 lr: 0 loss: 0.788142691488 acc: 0.741926024419
INFO: 2017-08-28 23:57:57: main.py:86 **  epoch: 0 batch_num: 161 lr: 0 loss: 0.786366949111 acc: 0.742942236824
INFO: 2017-08-28 23:57:58: main.py:86 **  epoch: 0 batch_num: 162 lr: 0 loss: 0.785591508713 acc: 0.743217995196
INFO: 2017-08-28 23:57:59: main.py:86 **  epoch: 0 batch_num: 163 lr: 0 loss: 0.787787155407 acc: 0.742459992447
INFO: 2017-08-28 23:57:59: main.py:86 **  epoch: 0 batch_num: 164 lr: 0 loss: 0.784960438627 acc: 0.743579770941
INFO: 2017-08-28 23:58:00: main.py:86 **  epoch: 0 batch_num: 165 lr: 0 loss: 0.78374744072 acc: 0.744046856481
INFO: 2017-08-28 23:58:01: main.py:86 **  epoch: 0 batch_num: 166 lr: 0 loss: 0.781996707716 acc: 0.745945369769
INFO: 2017-08-28 23:58:01: main.py:86 **  epoch: 0 batch_num: 167 lr: 0 loss: 0.779247809024 acc: 0.746921518019
INFO: 2017-08-28 23:58:02: main.py:86 **  epoch: 0 batch_num: 168 lr: 0 loss: 0.781120247389 acc: 0.745657737438
INFO: 2017-08-28 23:58:03: main.py:86 **  epoch: 0 batch_num: 169 lr: 0 loss: 0.779563307061 acc: 0.746516969625
INFO: 2017-08-28 23:58:03: main.py:86 **  epoch: 0 batch_num: 170 lr: 0 loss: 0.778836567151 acc: 0.745634502835
INFO: 2017-08-28 23:58:04: main.py:86 **  epoch: 0 batch_num: 171 lr: 0 loss: 0.777157832544 acc: 0.746071080829
INFO: 2017-08-28 23:58:05: main.py:86 **  epoch: 0 batch_num: 172 lr: 0 loss: 0.776221567323 acc: 0.746367196816
INFO: 2017-08-28 23:58:05: main.py:86 **  epoch: 0 batch_num: 173 lr: 0 loss: 0.777485699318 acc: 0.74605977638
INFO: 2017-08-28 23:58:06: main.py:86 **  epoch: 0 batch_num: 174 lr: 0 loss: 0.775773480279 acc: 0.746762347221
INFO: 2017-08-28 23:58:07: main.py:86 **  epoch: 0 batch_num: 175 lr: 0 loss: 0.774139424109 acc: 0.747433028438
INFO: 2017-08-28 23:58:07: main.py:86 **  epoch: 0 batch_num: 176 lr: 0 loss: 0.7719736404 acc: 0.748352070313
INFO: 2017-08-28 23:58:08: main.py:86 **  epoch: 0 batch_num: 177 lr: 0 loss: 0.771366057269 acc: 0.749871893545
INFO: 2017-08-28 23:58:09: main.py:86 **  epoch: 0 batch_num: 178 lr: 0 loss: 0.7710637342 acc: 0.749274116322
INFO: 2017-08-28 23:58:09: main.py:86 **  epoch: 0 batch_num: 179 lr: 0 loss: 0.769193862213 acc: 0.750092592173
INFO: 2017-08-28 23:58:10: main.py:86 **  epoch: 0 batch_num: 180 lr: 0 loss: 0.76813362547 acc: 0.751831998812
INFO: 2017-08-28 23:58:11: main.py:86 **  epoch: 0 batch_num: 181 lr: 0 loss: 0.767331572024 acc: 0.752350902164
INFO: 2017-08-28 23:58:11: main.py:86 **  epoch: 0 batch_num: 182 lr: 0 loss: 0.765763875565 acc: 0.754165105481
INFO: 2017-08-28 23:58:12: main.py:86 **  epoch: 0 batch_num: 183 lr: 0 loss: 0.768163001408 acc: 0.75229468391
INFO: 2017-08-28 23:58:13: main.py:86 **  epoch: 0 batch_num: 184 lr: 0 loss: 0.765879272126 acc: 0.753097725881
INFO: 2017-08-28 23:58:13: main.py:86 **  epoch: 0 batch_num: 185 lr: 0 loss: 0.76433775614 acc: 0.753670517475
INFO: 2017-08-28 23:58:14: main.py:86 **  epoch: 0 batch_num: 186 lr: 0 loss: 0.763902236274 acc: 0.753683563222
INFO: 2017-08-28 23:58:15: main.py:86 **  epoch: 0 batch_num: 187 lr: 0 loss: 0.762076872302 acc: 0.754387085108
INFO: 2017-08-28 23:58:15: main.py:86 **  epoch: 0 batch_num: 188 lr: 0 loss: 0.762153469539 acc: 0.754378216607
INFO: 2017-08-28 23:58:16: main.py:86 **  epoch: 0 batch_num: 189 lr: 0 loss: 0.762360503642 acc: 0.754318732651
INFO: 2017-08-28 23:58:17: main.py:86 **  epoch: 0 batch_num: 190 lr: 0 loss: 0.760873215985 acc: 0.754892102399
INFO: 2017-08-28 23:58:17: main.py:86 **  epoch: 0 batch_num: 191 lr: 0 loss: 0.759594112635 acc: 0.756650516453
INFO: 2017-08-28 23:58:18: main.py:86 **  epoch: 0 batch_num: 192 lr: 0 loss: 0.759934978782 acc: 0.757501154057
INFO: 2017-08-28 23:58:19: main.py:86 **  epoch: 0 batch_num: 193 lr: 0 loss: 0.758739453001 acc: 0.757722999632
INFO: 2017-08-28 23:58:19: main.py:86 **  epoch: 0 batch_num: 194 lr: 0 loss: 0.758139837094 acc: 0.757665191247
INFO: 2017-08-28 23:58:20: main.py:86 **  epoch: 0 batch_num: 195 lr: 0 loss: 0.756871261767 acc: 0.757667846217
INFO: 2017-08-28 23:58:21: main.py:86 **  epoch: 0 batch_num: 196 lr: 0 loss: 0.756565112148 acc: 0.757543999834
INFO: 2017-08-28 23:58:21: main.py:86 **  epoch: 0 batch_num: 197 lr: 0 loss: 0.755803037171 acc: 0.757201222759
INFO: 2017-08-28 23:58:22: main.py:86 **  epoch: 0 batch_num: 198 lr: 0 loss: 0.756268253878 acc: 0.756820103331
INFO: 2017-08-28 23:58:23: main.py:86 **  epoch: 0 batch_num: 199 lr: 0 loss: 0.756271233559 acc: 0.756630714834
INFO: 2017-08-28 23:58:23: main.py:86 **  epoch: 0 batch_num: 200 lr: 0 loss: 0.754954791484 acc: 0.756741306972
INFO: 2017-08-28 23:58:24: main.py:86 **  epoch: 0 batch_num: 201 lr: 0 loss: 0.753863335541 acc: 0.756929564299
INFO: 2017-08-28 23:58:25: main.py:86 **  epoch: 0 batch_num: 202 lr: 0 loss: 0.752629700846 acc: 0.757004836216
INFO: 2017-08-28 23:58:25: main.py:86 **  epoch: 0 batch_num: 203 lr: 0 loss: 0.752144189734 acc: 0.757345676714
INFO: 2017-08-28 23:58:26: main.py:86 **  epoch: 0 batch_num: 204 lr: 0 loss: 0.750562394101 acc: 0.758029782191
INFO: 2017-08-29 00:06:53: main.py:28 **  start
INFO: 2017-08-29 00:06:53: main.py:30 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-29 00:06:53: main.py:31 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-29 00:06:53: main.py:32 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-29 00:06:53: main.py:33 **  读取数据集...
INFO: 2017-08-29 00:06:56: main.py:44 **  train data sample counts 5088
INFO: 2017-08-29 00:06:56: main.py:45 **  train data set batch size 6
INFO: 2017-08-29 00:06:56: main.py:46 **  train data batch counts 848
INFO: 2017-08-29 00:07:46: main.py:28 **  start
INFO: 2017-08-29 00:07:46: main.py:30 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-29 00:07:46: main.py:31 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-29 00:07:46: main.py:32 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-29 00:07:46: main.py:33 **  读取数据集...
INFO: 2017-08-29 00:07:49: main.py:44 **  train data sample counts 5088
INFO: 2017-08-29 00:07:49: main.py:45 **  train data set batch size 6
INFO: 2017-08-29 00:07:49: main.py:46 **  train data batch counts 848
INFO: 2017-08-29 00:08:09: main.py:28 **  start
INFO: 2017-08-29 00:08:09: main.py:30 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-29 00:08:09: main.py:31 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-29 00:08:09: main.py:32 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-29 00:08:09: main.py:33 **  读取数据集...
INFO: 2017-08-29 00:08:12: main.py:44 **  train data sample counts 5088
INFO: 2017-08-29 00:08:12: main.py:45 **  train data set batch size 4
INFO: 2017-08-29 00:08:12: main.py:46 **  train data batch counts 1272
INFO: 2017-08-29 00:08:21: main.py:28 **  start
INFO: 2017-08-29 00:08:21: main.py:30 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-29 00:08:21: main.py:31 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-29 00:08:21: main.py:32 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-29 00:08:21: main.py:33 **  读取数据集...
INFO: 2017-08-29 00:08:24: main.py:44 **  train data sample counts 5088
INFO: 2017-08-29 00:08:24: main.py:45 **  train data set batch size 1
INFO: 2017-08-29 00:08:24: main.py:46 **  train data batch counts 5088
INFO: 2017-08-29 00:18:15: main.py:28 **  start
INFO: 2017-08-29 00:18:15: main.py:30 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-29 00:18:15: main.py:31 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-29 00:18:15: main.py:32 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-29 00:18:15: main.py:33 **  读取数据集...
INFO: 2017-08-29 00:18:18: main.py:44 **  train data sample counts 5088
INFO: 2017-08-29 00:18:18: main.py:45 **  train data set batch size 1
INFO: 2017-08-29 00:18:18: main.py:46 **  train data batch counts 5088
INFO: 2017-08-29 00:18:20: main.py:53 **  <class 'net.net.CarUNet'>


INFO: 2017-08-29 00:18:45: main.py:28 **  start
INFO: 2017-08-29 00:18:45: main.py:30 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-29 00:18:45: main.py:31 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-29 00:18:45: main.py:32 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-29 00:18:45: main.py:33 **  读取数据集...
INFO: 2017-08-29 00:18:48: main.py:44 **  train data sample counts 5088
INFO: 2017-08-29 00:18:48: main.py:45 **  train data set batch size 8
INFO: 2017-08-29 00:18:48: main.py:46 **  train data batch counts 636
INFO: 2017-08-29 00:18:49: main.py:53 **  <class 'net.net.CarUNet'>


INFO: 2017-08-29 00:21:32: main.py:28 **  start
INFO: 2017-08-29 00:21:32: main.py:30 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-29 00:21:32: main.py:31 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-29 00:21:32: main.py:32 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-29 00:21:32: main.py:33 **  读取数据集...
INFO: 2017-08-29 00:21:35: main.py:44 **  train data sample counts 5088
INFO: 2017-08-29 00:21:35: main.py:45 **  train data set batch size 8
INFO: 2017-08-29 00:21:35: main.py:46 **  train data batch counts 636
INFO: 2017-08-29 00:21:36: main.py:53 **  <class 'net.net.CarUNet'>


INFO: 2017-08-29 00:22:11: main.py:28 **  start
INFO: 2017-08-29 00:22:11: main.py:30 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-29 00:22:11: main.py:31 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-29 00:22:11: main.py:32 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-29 00:22:11: main.py:33 **  读取数据集...
INFO: 2017-08-29 00:22:14: main.py:44 **  train data sample counts 5088
INFO: 2017-08-29 00:22:14: main.py:45 **  train data set batch size 8
INFO: 2017-08-29 00:22:14: main.py:46 **  train data batch counts 636
INFO: 2017-08-29 00:22:15: main.py:53 **  <class 'net.net.CarUNet'>


INFO: 2017-08-29 00:22:17: main.py:100 **  epoch: 0 batch_num: 0 lr: 0 loss: 1.32574439049 acc: 0.00668253609911train_loss: 1.3257 train_acc: 0.67%
INFO: 2017-08-29 00:22:18: main.py:100 **  epoch: 0 batch_num: 1 lr: 0 loss: 1.32574439049 acc: 0.00668253609911train_loss: 1.2577 train_acc: 0.68%
INFO: 2017-08-29 00:22:19: main.py:100 **  epoch: 0 batch_num: 2 lr: 0 loss: 1.32574439049 acc: 0.00668253609911train_loss: 1.3425 train_acc: 1.46%
INFO: 2017-08-29 00:22:19: main.py:100 **  epoch: 0 batch_num: 3 lr: 0 loss: 1.32574439049 acc: 0.00668253609911train_loss: 1.2798 train_acc: 2.46%
INFO: 2017-08-29 00:22:20: main.py:100 **  epoch: 0 batch_num: 4 lr: 0 loss: 1.32574439049 acc: 0.00668253609911train_loss: 1.2708 train_acc: 4.16%
INFO: 2017-08-29 00:22:21: main.py:100 **  epoch: 0 batch_num: 5 lr: 0 loss: 1.32574439049 acc: 0.00668253609911train_loss: 1.2577 train_acc: 10.4%
INFO: 2017-08-29 00:22:22: main.py:100 **  epoch: 0 batch_num: 6 lr: 0 loss: 1.32574439049 acc: 0.00668253609911train_loss: 1.3741 train_acc: 26.37%
INFO: 2017-08-29 00:22:22: main.py:100 **  epoch: 0 batch_num: 7 lr: 0 loss: 1.32574439049 acc: 0.00668253609911train_loss: 1.22 train_acc: 21.01%
INFO: 2017-08-29 00:22:23: main.py:100 **  epoch: 0 batch_num: 8 lr: 0 loss: 1.32574439049 acc: 0.00668253609911train_loss: 1.2593 train_acc: 39.85%
INFO: 2017-08-29 00:22:24: main.py:100 **  epoch: 0 batch_num: 9 lr: 0 loss: 1.32574439049 acc: 0.00668253609911train_loss: 1.2974 train_acc: 32.84%
INFO: 2017-08-29 00:22:25: main.py:100 **  epoch: 0 batch_num: 10 lr: 0 loss: 1.32574439049 acc: 0.00668253609911train_loss: 1.2484 train_acc: 30.13%
INFO: 2017-08-29 00:22:25: main.py:100 **  epoch: 0 batch_num: 11 lr: 0 loss: 1.32574439049 acc: 0.00668253609911train_loss: 1.2851 train_acc: 46.28%
INFO: 2017-08-29 00:22:26: main.py:100 **  epoch: 0 batch_num: 12 lr: 0 loss: 1.32574439049 acc: 0.00668253609911train_loss: 1.1708 train_acc: 47.23%
INFO: 2017-08-29 00:22:27: main.py:100 **  epoch: 0 batch_num: 13 lr: 0 loss: 1.32574439049 acc: 0.00668253609911train_loss: 1.1564 train_acc: 59.99%
INFO: 2017-08-29 00:22:28: main.py:100 **  epoch: 0 batch_num: 14 lr: 0 loss: 1.32574439049 acc: 0.00668253609911train_loss: 1.211 train_acc: 62.27%
INFO: 2017-08-29 00:22:28: main.py:100 **  epoch: 0 batch_num: 15 lr: 0 loss: 1.32574439049 acc: 0.00668253609911train_loss: 1.1613 train_acc: 61.84%
INFO: 2017-08-29 00:22:29: main.py:100 **  epoch: 0 batch_num: 16 lr: 0 loss: 1.32574439049 acc: 0.00668253609911train_loss: 1.1773 train_acc: 54.46%
INFO: 2017-08-29 00:22:30: main.py:100 **  epoch: 0 batch_num: 17 lr: 0 loss: 1.32574439049 acc: 0.00668253609911train_loss: 1.24 train_acc: 46.64%
INFO: 2017-08-29 00:22:31: main.py:100 **  epoch: 0 batch_num: 18 lr: 0 loss: 1.32574439049 acc: 0.00668253609911train_loss: 1.1772 train_acc: 61.02%
INFO: 2017-08-29 00:22:32: main.py:100 **  epoch: 0 batch_num: 19 lr: 0 loss: 1.32574439049 acc: 0.00668253609911train_loss: 1.1547 train_acc: 59.86%
INFO: 2017-08-29 00:22:32: main.py:100 **  epoch: 0 batch_num: 20 lr: 0 loss: 1.32574439049 acc: 0.00668253609911train_loss: 1.1701 train_acc: 68.23%
INFO: 2017-08-29 00:22:33: main.py:100 **  epoch: 0 batch_num: 21 lr: 0 loss: 1.32574439049 acc: 0.00668253609911train_loss: 1.1197 train_acc: 67.96%
INFO: 2017-08-29 00:22:34: main.py:100 **  epoch: 0 batch_num: 22 lr: 0 loss: 1.32574439049 acc: 0.00668253609911train_loss: 1.1246 train_acc: 76.53%
INFO: 2017-08-29 00:22:35: main.py:100 **  epoch: 0 batch_num: 23 lr: 0 loss: 1.32574439049 acc: 0.00668253609911train_loss: 1.0803 train_acc: 73.54%
INFO: 2017-08-29 00:22:35: main.py:100 **  epoch: 0 batch_num: 24 lr: 0 loss: 1.32574439049 acc: 0.00668253609911train_loss: 1.1078 train_acc: 68.92%
INFO: 2017-08-29 00:22:36: main.py:100 **  epoch: 0 batch_num: 25 lr: 0 loss: 1.32574439049 acc: 0.00668253609911train_loss: 1.0387 train_acc: 71.48%
INFO: 2017-08-29 00:22:37: main.py:100 **  epoch: 0 batch_num: 26 lr: 0 loss: 1.32574439049 acc: 0.00668253609911train_loss: 1.0401 train_acc: 80.74%
INFO: 2017-08-29 00:22:38: main.py:100 **  epoch: 0 batch_num: 27 lr: 0 loss: 1.32574439049 acc: 0.00668253609911train_loss: 1.0182 train_acc: 70.29%
INFO: 2017-08-29 00:22:39: main.py:100 **  epoch: 0 batch_num: 28 lr: 0 loss: 1.32574439049 acc: 0.00668253609911train_loss: 0.9801 train_acc: 74.45%
INFO: 2017-08-29 00:22:39: main.py:100 **  epoch: 0 batch_num: 29 lr: 0 loss: 1.32574439049 acc: 0.00668253609911train_loss: 1.0603 train_acc: 69.14%
INFO: 2017-08-29 00:22:40: main.py:100 **  epoch: 0 batch_num: 30 lr: 0 loss: 1.17797092597 acc: 0.48959178091train_loss: 1.0576 train_acc: 78.55%
INFO: 2017-08-29 00:22:41: main.py:100 **  epoch: 0 batch_num: 31 lr: 0 loss: 1.17797092597 acc: 0.48959178091train_loss: 0.9615 train_acc: 79.45%
INFO: 2017-08-29 00:22:42: main.py:100 **  epoch: 0 batch_num: 32 lr: 0 loss: 1.17797092597 acc: 0.48959178091train_loss: 0.9751 train_acc: 78.34%
INFO: 2017-08-29 00:22:42: main.py:100 **  epoch: 0 batch_num: 33 lr: 0 loss: 1.17797092597 acc: 0.48959178091train_loss: 0.9694 train_acc: 84.15%
INFO: 2017-08-29 00:22:43: main.py:100 **  epoch: 0 batch_num: 34 lr: 0 loss: 1.17797092597 acc: 0.48959178091train_loss: 0.9562 train_acc: 71.92%
INFO: 2017-08-29 00:22:44: main.py:100 **  epoch: 0 batch_num: 35 lr: 0 loss: 1.17797092597 acc: 0.48959178091train_loss: 0.939 train_acc: 77.79%
INFO: 2017-08-29 00:22:45: main.py:100 **  epoch: 0 batch_num: 36 lr: 0 loss: 1.17797092597 acc: 0.48959178091train_loss: 0.8316 train_acc: 78.35%
INFO: 2017-08-29 00:22:45: main.py:100 **  epoch: 0 batch_num: 37 lr: 0 loss: 1.17797092597 acc: 0.48959178091train_loss: 0.9462 train_acc: 74.96%
INFO: 2017-08-29 00:22:46: main.py:100 **  epoch: 0 batch_num: 38 lr: 0 loss: 1.17797092597 acc: 0.48959178091train_loss: 0.9533 train_acc: 72.09%
INFO: 2017-08-29 00:22:47: main.py:100 **  epoch: 0 batch_num: 39 lr: 0 loss: 1.17797092597 acc: 0.48959178091train_loss: 0.8604 train_acc: 71.99%
INFO: 2017-08-29 00:22:48: main.py:100 **  epoch: 0 batch_num: 40 lr: 0 loss: 1.17797092597 acc: 0.48959178091train_loss: 0.8869 train_acc: 73.02%
INFO: 2017-08-29 00:22:49: main.py:100 **  epoch: 0 batch_num: 41 lr: 0 loss: 1.17797092597 acc: 0.48959178091train_loss: 0.8966 train_acc: 74.75%
INFO: 2017-08-29 00:22:49: main.py:100 **  epoch: 0 batch_num: 42 lr: 0 loss: 1.17797092597 acc: 0.48959178091train_loss: 0.7679 train_acc: 81.85%
INFO: 2017-08-29 00:22:50: main.py:100 **  epoch: 0 batch_num: 43 lr: 0 loss: 1.17797092597 acc: 0.48959178091train_loss: 0.7315 train_acc: 89.15%
INFO: 2017-08-29 00:22:51: main.py:100 **  epoch: 0 batch_num: 44 lr: 0 loss: 1.17797092597 acc: 0.48959178091train_loss: 0.6975 train_acc: 86.49%
INFO: 2017-08-29 00:22:52: main.py:100 **  epoch: 0 batch_num: 45 lr: 0 loss: 1.17797092597 acc: 0.48959178091train_loss: 0.7414 train_acc: 85.85%
INFO: 2017-08-29 00:22:53: main.py:100 **  epoch: 0 batch_num: 46 lr: 0 loss: 1.17797092597 acc: 0.48959178091train_loss: 0.8657 train_acc: 72.41%
INFO: 2017-08-29 00:22:53: main.py:100 **  epoch: 0 batch_num: 47 lr: 0 loss: 1.17797092597 acc: 0.48959178091train_loss: 0.9381 train_acc: 70.61%
INFO: 2017-08-29 00:22:54: main.py:100 **  epoch: 0 batch_num: 48 lr: 0 loss: 1.17797092597 acc: 0.48959178091train_loss: 0.6656 train_acc: 91.44%
INFO: 2017-08-29 00:22:55: main.py:100 **  epoch: 0 batch_num: 49 lr: 0 loss: 1.17797092597 acc: 0.48959178091train_loss: 0.7229 train_acc: 77.19%
INFO: 2017-08-29 00:22:56: main.py:100 **  epoch: 0 batch_num: 50 lr: 0 loss: 1.17797092597 acc: 0.48959178091train_loss: 0.7884 train_acc: 71.46%
INFO: 2017-08-29 00:22:56: main.py:100 **  epoch: 0 batch_num: 51 lr: 0 loss: 1.17797092597 acc: 0.48959178091train_loss: 0.8839 train_acc: 66.84%
INFO: 2017-08-29 00:22:57: main.py:100 **  epoch: 0 batch_num: 52 lr: 0 loss: 1.17797092597 acc: 0.48959178091train_loss: 0.8111 train_acc: 76.44%
INFO: 2017-08-29 00:22:58: main.py:100 **  epoch: 0 batch_num: 53 lr: 0 loss: 1.17797092597 acc: 0.48959178091train_loss: 0.7395 train_acc: 77.93%
INFO: 2017-08-29 00:22:59: main.py:100 **  epoch: 0 batch_num: 54 lr: 0 loss: 1.17797092597 acc: 0.48959178091train_loss: 0.8148 train_acc: 84.09%
INFO: 2017-08-29 00:23:00: main.py:100 **  epoch: 0 batch_num: 55 lr: 0 loss: 1.17797092597 acc: 0.48959178091train_loss: 0.7041 train_acc: 84.42%
INFO: 2017-08-29 00:23:00: main.py:100 **  epoch: 0 batch_num: 56 lr: 0 loss: 1.17797092597 acc: 0.48959178091train_loss: 0.8973 train_acc: 71.25%
INFO: 2017-08-29 00:23:01: main.py:100 **  epoch: 0 batch_num: 57 lr: 0 loss: 1.17797092597 acc: 0.48959178091train_loss: 0.7472 train_acc: 79.66%
INFO: 2017-08-29 00:23:02: main.py:100 **  epoch: 0 batch_num: 58 lr: 0 loss: 1.17797092597 acc: 0.48959178091train_loss: 0.706 train_acc: 79.75%
INFO: 2017-08-29 00:23:03: main.py:100 **  epoch: 0 batch_num: 59 lr: 0 loss: 1.17797092597 acc: 0.48959178091train_loss: 0.7815 train_acc: 68.18%
INFO: 2017-08-29 00:23:03: main.py:100 **  epoch: 0 batch_num: 60 lr: 0 loss: 0.826363853614 acc: 0.777560685078train_loss: 0.6102 train_acc: 80.87%
INFO: 2017-08-29 00:23:04: main.py:100 **  epoch: 0 batch_num: 61 lr: 0 loss: 0.826363853614 acc: 0.777560685078train_loss: 0.8273 train_acc: 76.74%
INFO: 2017-08-29 00:23:05: main.py:100 **  epoch: 0 batch_num: 62 lr: 0 loss: 0.826363853614 acc: 0.777560685078train_loss: 0.647 train_acc: 82.47%
INFO: 2017-08-29 00:23:06: main.py:100 **  epoch: 0 batch_num: 63 lr: 0 loss: 0.826363853614 acc: 0.777560685078train_loss: 0.6208 train_acc: 89.53%
INFO: 2017-08-29 00:23:07: main.py:100 **  epoch: 0 batch_num: 64 lr: 0 loss: 0.826363853614 acc: 0.777560685078train_loss: 0.4932 train_acc: 90.04%
INFO: 2017-08-29 00:23:07: main.py:100 **  epoch: 0 batch_num: 65 lr: 0 loss: 0.826363853614 acc: 0.777560685078train_loss: 0.5064 train_acc: 82.81%
INFO: 2017-08-29 00:23:08: main.py:100 **  epoch: 0 batch_num: 66 lr: 0 loss: 0.826363853614 acc: 0.777560685078train_loss: 0.7088 train_acc: 79.84%
INFO: 2017-08-29 00:23:09: main.py:100 **  epoch: 0 batch_num: 67 lr: 0 loss: 0.826363853614 acc: 0.777560685078train_loss: 0.804 train_acc: 68.42%
INFO: 2017-08-29 00:23:10: main.py:100 **  epoch: 0 batch_num: 68 lr: 0 loss: 0.826363853614 acc: 0.777560685078train_loss: 0.6186 train_acc: 82.0%
INFO: 2017-08-29 00:23:11: main.py:100 **  epoch: 0 batch_num: 69 lr: 0 loss: 0.826363853614 acc: 0.777560685078train_loss: 0.5256 train_acc: 86.98%
INFO: 2017-08-29 00:23:11: main.py:100 **  epoch: 0 batch_num: 70 lr: 0 loss: 0.826363853614 acc: 0.777560685078train_loss: 0.5158 train_acc: 86.38%
INFO: 2017-08-29 00:23:12: main.py:100 **  epoch: 0 batch_num: 71 lr: 0 loss: 0.826363853614 acc: 0.777560685078train_loss: 0.6089 train_acc: 76.32%
INFO: 2017-08-29 00:23:13: main.py:100 **  epoch: 0 batch_num: 72 lr: 0 loss: 0.826363853614 acc: 0.777560685078train_loss: 1.1493 train_acc: 51.68%
INFO: 2017-08-29 00:23:14: main.py:100 **  epoch: 0 batch_num: 73 lr: 0 loss: 0.826363853614 acc: 0.777560685078train_loss: 0.5806 train_acc: 78.3%
INFO: 2017-08-29 00:23:14: main.py:100 **  epoch: 0 batch_num: 74 lr: 0 loss: 0.826363853614 acc: 0.777560685078train_loss: 0.5227 train_acc: 83.42%
INFO: 2017-08-29 00:23:15: main.py:100 **  epoch: 0 batch_num: 75 lr: 0 loss: 0.826363853614 acc: 0.777560685078train_loss: 0.5422 train_acc: 80.44%
INFO: 2017-08-29 00:23:16: main.py:100 **  epoch: 0 batch_num: 76 lr: 0 loss: 0.826363853614 acc: 0.777560685078train_loss: 0.4747 train_acc: 91.19%
INFO: 2017-08-29 00:23:17: main.py:100 **  epoch: 0 batch_num: 77 lr: 0 loss: 0.826363853614 acc: 0.777560685078train_loss: 0.6858 train_acc: 71.4%
INFO: 2017-08-29 00:23:18: main.py:100 **  epoch: 0 batch_num: 78 lr: 0 loss: 0.826363853614 acc: 0.777560685078train_loss: 0.7605 train_acc: 82.72%
INFO: 2017-08-29 00:23:18: main.py:100 **  epoch: 0 batch_num: 79 lr: 0 loss: 0.826363853614 acc: 0.777560685078train_loss: 0.7474 train_acc: 58.16%
INFO: 2017-08-29 00:23:19: main.py:100 **  epoch: 0 batch_num: 80 lr: 0 loss: 0.826363853614 acc: 0.777560685078train_loss: 0.4653 train_acc: 83.62%
INFO: 2017-08-29 00:23:20: main.py:100 **  epoch: 0 batch_num: 81 lr: 0 loss: 0.826363853614 acc: 0.777560685078train_loss: 0.6798 train_acc: 72.43%
INFO: 2017-08-29 00:23:21: main.py:100 **  epoch: 0 batch_num: 82 lr: 0 loss: 0.826363853614 acc: 0.777560685078train_loss: 0.531 train_acc: 87.55%
INFO: 2017-08-29 00:23:21: main.py:100 **  epoch: 0 batch_num: 83 lr: 0 loss: 0.826363853614 acc: 0.777560685078train_loss: 0.5353 train_acc: 79.79%
INFO: 2017-08-29 00:23:22: main.py:100 **  epoch: 0 batch_num: 84 lr: 0 loss: 0.826363853614 acc: 0.777560685078train_loss: 0.5528 train_acc: 79.83%
INFO: 2017-08-29 00:23:23: main.py:100 **  epoch: 0 batch_num: 85 lr: 0 loss: 0.826363853614 acc: 0.777560685078train_loss: 0.4365 train_acc: 81.34%
INFO: 2017-08-29 00:23:24: main.py:100 **  epoch: 0 batch_num: 86 lr: 0 loss: 0.826363853614 acc: 0.777560685078train_loss: 0.6118 train_acc: 75.16%
INFO: 2017-08-29 00:23:25: main.py:100 **  epoch: 0 batch_num: 87 lr: 0 loss: 0.826363853614 acc: 0.777560685078train_loss: 0.6449 train_acc: 81.39%
INFO: 2017-08-29 00:23:25: main.py:100 **  epoch: 0 batch_num: 88 lr: 0 loss: 0.826363853614 acc: 0.777560685078train_loss: 0.8499 train_acc: 63.36%
INFO: 2017-08-29 00:23:26: main.py:100 **  epoch: 0 batch_num: 89 lr: 0 loss: 0.826363853614 acc: 0.777560685078train_loss: 0.9156 train_acc: 62.25%
INFO: 2017-08-29 00:23:27: main.py:100 **  epoch: 0 batch_num: 90 lr: 0 loss: 0.641488252083 acc: 0.778811105092train_loss: 0.6822 train_acc: 70.86%
INFO: 2017-08-29 00:23:28: main.py:100 **  epoch: 0 batch_num: 91 lr: 0 loss: 0.641488252083 acc: 0.778811105092train_loss: 0.5304 train_acc: 80.65%
INFO: 2017-08-29 00:23:29: main.py:100 **  epoch: 0 batch_num: 92 lr: 0 loss: 0.641488252083 acc: 0.778811105092train_loss: 0.7496 train_acc: 70.69%
INFO: 2017-08-29 00:23:29: main.py:100 **  epoch: 0 batch_num: 93 lr: 0 loss: 0.641488252083 acc: 0.778811105092train_loss: 0.3938 train_acc: 91.0%
INFO: 2017-08-29 00:23:30: main.py:100 **  epoch: 0 batch_num: 94 lr: 0 loss: 0.641488252083 acc: 0.778811105092train_loss: 0.5331 train_acc: 83.95%
INFO: 2017-08-29 00:23:31: main.py:100 **  epoch: 0 batch_num: 95 lr: 0 loss: 0.641488252083 acc: 0.778811105092train_loss: 0.6542 train_acc: 64.58%
INFO: 2017-08-29 00:23:32: main.py:100 **  epoch: 0 batch_num: 96 lr: 0 loss: 0.641488252083 acc: 0.778811105092train_loss: 0.617 train_acc: 68.37%
INFO: 2017-08-29 00:23:33: main.py:100 **  epoch: 0 batch_num: 97 lr: 0 loss: 0.641488252083 acc: 0.778811105092train_loss: 0.4628 train_acc: 85.91%
INFO: 2017-08-29 00:23:33: main.py:100 **  epoch: 0 batch_num: 98 lr: 0 loss: 0.641488252083 acc: 0.778811105092train_loss: 0.4217 train_acc: 88.14%
INFO: 2017-08-29 00:23:34: main.py:100 **  epoch: 0 batch_num: 99 lr: 0 loss: 0.641488252083 acc: 0.778811105092train_loss: 0.5998 train_acc: 81.5%
INFO: 2017-08-29 00:23:35: main.py:100 **  epoch: 0 batch_num: 100 lr: 0 loss: 0.641488252083 acc: 0.778811105092train_loss: 0.5546 train_acc: 76.57%
INFO: 2017-08-29 00:23:36: main.py:100 **  epoch: 0 batch_num: 101 lr: 0 loss: 0.641488252083 acc: 0.778811105092train_loss: 0.4338 train_acc: 88.37%
INFO: 2017-08-29 00:23:37: main.py:100 **  epoch: 0 batch_num: 102 lr: 0 loss: 0.641488252083 acc: 0.778811105092train_loss: 0.643 train_acc: 74.9%
INFO: 2017-08-29 00:23:38: main.py:100 **  epoch: 0 batch_num: 103 lr: 0 loss: 0.641488252083 acc: 0.778811105092train_loss: 1.1437 train_acc: 55.15%
INFO: 2017-08-29 00:23:38: main.py:100 **  epoch: 0 batch_num: 104 lr: 0 loss: 0.641488252083 acc: 0.778811105092train_loss: 0.6149 train_acc: 74.23%
INFO: 2017-08-29 00:23:39: main.py:100 **  epoch: 0 batch_num: 105 lr: 0 loss: 0.641488252083 acc: 0.778811105092train_loss: 0.3908 train_acc: 90.51%
INFO: 2017-08-29 00:23:40: main.py:100 **  epoch: 0 batch_num: 106 lr: 0 loss: 0.641488252083 acc: 0.778811105092train_loss: 0.4064 train_acc: 92.64%
INFO: 2017-08-29 00:23:41: main.py:100 **  epoch: 0 batch_num: 107 lr: 0 loss: 0.641488252083 acc: 0.778811105092train_loss: 0.5477 train_acc: 78.99%
INFO: 2017-08-29 00:23:42: main.py:100 **  epoch: 0 batch_num: 108 lr: 0 loss: 0.641488252083 acc: 0.778811105092train_loss: 0.3554 train_acc: 89.82%
INFO: 2017-08-29 00:23:42: main.py:100 **  epoch: 0 batch_num: 109 lr: 0 loss: 0.641488252083 acc: 0.778811105092train_loss: 0.2965 train_acc: 92.36%
INFO: 2017-08-29 00:23:43: main.py:100 **  epoch: 0 batch_num: 110 lr: 0 loss: 0.641488252083 acc: 0.778811105092train_loss: 0.6077 train_acc: 87.68%
INFO: 2017-08-29 00:23:44: main.py:100 **  epoch: 0 batch_num: 111 lr: 0 loss: 0.641488252083 acc: 0.778811105092train_loss: 0.5973 train_acc: 73.12%
INFO: 2017-08-29 00:23:45: main.py:100 **  epoch: 0 batch_num: 112 lr: 0 loss: 0.641488252083 acc: 0.778811105092train_loss: 0.4071 train_acc: 84.65%
INFO: 2017-08-29 00:23:46: main.py:100 **  epoch: 0 batch_num: 113 lr: 0 loss: 0.641488252083 acc: 0.778811105092train_loss: 0.4412 train_acc: 82.6%
INFO: 2017-08-29 00:23:47: main.py:100 **  epoch: 0 batch_num: 114 lr: 0 loss: 0.641488252083 acc: 0.778811105092train_loss: 0.4577 train_acc: 80.52%
INFO: 2017-08-29 00:23:47: main.py:100 **  epoch: 0 batch_num: 115 lr: 0 loss: 0.641488252083 acc: 0.778811105092train_loss: 0.7167 train_acc: 63.33%
INFO: 2017-08-29 00:23:48: main.py:100 **  epoch: 0 batch_num: 116 lr: 0 loss: 0.641488252083 acc: 0.778811105092train_loss: 1.0208 train_acc: 53.41%
INFO: 2017-08-29 00:23:49: main.py:100 **  epoch: 0 batch_num: 117 lr: 0 loss: 0.641488252083 acc: 0.778811105092train_loss: 0.823 train_acc: 54.9%
INFO: 2017-08-29 00:23:50: main.py:100 **  epoch: 0 batch_num: 118 lr: 0 loss: 0.641488252083 acc: 0.778811105092train_loss: 0.888 train_acc: 75.05%
INFO: 2017-08-29 00:23:51: main.py:100 **  epoch: 0 batch_num: 119 lr: 0 loss: 0.641488252083 acc: 0.778811105092train_loss: 0.5773 train_acc: 74.26%
INFO: 2017-08-29 00:23:51: main.py:100 **  epoch: 0 batch_num: 120 lr: 0 loss: 0.578264071544 acc: 0.78167437911train_loss: 0.4618 train_acc: 87.2%
INFO: 2017-08-29 00:23:52: main.py:100 **  epoch: 0 batch_num: 121 lr: 0 loss: 0.578264071544 acc: 0.78167437911train_loss: 0.883 train_acc: 63.74%
INFO: 2017-08-29 00:23:53: main.py:100 **  epoch: 0 batch_num: 122 lr: 0 loss: 0.578264071544 acc: 0.78167437911train_loss: 0.5033 train_acc: 101.61%
INFO: 2017-08-29 00:23:54: main.py:100 **  epoch: 0 batch_num: 123 lr: 0 loss: 0.578264071544 acc: 0.78167437911train_loss: 0.673 train_acc: 83.43%
INFO: 2017-08-29 00:23:55: main.py:100 **  epoch: 0 batch_num: 124 lr: 0 loss: 0.578264071544 acc: 0.78167437911train_loss: 0.4025 train_acc: 82.1%
INFO: 2017-08-29 00:23:56: main.py:100 **  epoch: 0 batch_num: 125 lr: 0 loss: 0.578264071544 acc: 0.78167437911train_loss: 0.5184 train_acc: 83.2%
INFO: 2017-08-29 00:23:56: main.py:100 **  epoch: 0 batch_num: 126 lr: 0 loss: 0.578264071544 acc: 0.78167437911train_loss: 1.081 train_acc: 55.06%
INFO: 2017-08-29 00:23:57: main.py:100 **  epoch: 0 batch_num: 127 lr: 0 loss: 0.578264071544 acc: 0.78167437911train_loss: 0.3979 train_acc: 82.47%
INFO: 2017-08-29 00:23:58: main.py:100 **  epoch: 0 batch_num: 128 lr: 0 loss: 0.578264071544 acc: 0.78167437911train_loss: 0.5149 train_acc: 84.8%
INFO: 2017-08-29 00:23:59: main.py:100 **  epoch: 0 batch_num: 129 lr: 0 loss: 0.578264071544 acc: 0.78167437911train_loss: 0.3882 train_acc: 85.6%
INFO: 2017-08-29 00:24:00: main.py:100 **  epoch: 0 batch_num: 130 lr: 0 loss: 0.578264071544 acc: 0.78167437911train_loss: 0.5199 train_acc: 79.27%
INFO: 2017-08-29 00:24:01: main.py:100 **  epoch: 0 batch_num: 131 lr: 0 loss: 0.578264071544 acc: 0.78167437911train_loss: 0.785 train_acc: 68.24%
INFO: 2017-08-29 00:24:01: main.py:100 **  epoch: 0 batch_num: 132 lr: 0 loss: 0.578264071544 acc: 0.78167437911train_loss: 0.5221 train_acc: 80.22%
INFO: 2017-08-29 00:24:02: main.py:100 **  epoch: 0 batch_num: 133 lr: 0 loss: 0.578264071544 acc: 0.78167437911train_loss: 0.711 train_acc: 72.9%
INFO: 2017-08-29 00:24:03: main.py:100 **  epoch: 0 batch_num: 134 lr: 0 loss: 0.578264071544 acc: 0.78167437911train_loss: 0.6087 train_acc: 81.7%
INFO: 2017-08-29 00:24:04: main.py:100 **  epoch: 0 batch_num: 135 lr: 0 loss: 0.578264071544 acc: 0.78167437911train_loss: 0.6085 train_acc: 73.14%
INFO: 2017-08-29 00:24:05: main.py:100 **  epoch: 0 batch_num: 136 lr: 0 loss: 0.578264071544 acc: 0.78167437911train_loss: 0.3928 train_acc: 81.45%
INFO: 2017-08-29 00:24:06: main.py:100 **  epoch: 0 batch_num: 137 lr: 0 loss: 0.578264071544 acc: 0.78167437911train_loss: 0.4451 train_acc: 85.49%
INFO: 2017-08-29 00:24:06: main.py:100 **  epoch: 0 batch_num: 138 lr: 0 loss: 0.578264071544 acc: 0.78167437911train_loss: 0.3423 train_acc: 88.45%
INFO: 2017-08-29 00:24:07: main.py:100 **  epoch: 0 batch_num: 139 lr: 0 loss: 0.578264071544 acc: 0.78167437911train_loss: 0.3398 train_acc: 92.07%
INFO: 2017-08-29 00:24:08: main.py:100 **  epoch: 0 batch_num: 140 lr: 0 loss: 0.578264071544 acc: 0.78167437911train_loss: 0.4092 train_acc: 87.95%
INFO: 2017-08-29 00:24:09: main.py:100 **  epoch: 0 batch_num: 141 lr: 0 loss: 0.578264071544 acc: 0.78167437911train_loss: 0.5562 train_acc: 98.76%
INFO: 2017-08-29 00:24:10: main.py:100 **  epoch: 0 batch_num: 142 lr: 0 loss: 0.578264071544 acc: 0.78167437911train_loss: 0.5584 train_acc: 82.11%
INFO: 2017-08-29 00:24:11: main.py:100 **  epoch: 0 batch_num: 143 lr: 0 loss: 0.578264071544 acc: 0.78167437911train_loss: 0.5041 train_acc: 71.31%
INFO: 2017-08-29 00:24:12: main.py:100 **  epoch: 0 batch_num: 144 lr: 0 loss: 0.578264071544 acc: 0.78167437911train_loss: 0.5293 train_acc: 85.97%
INFO: 2017-08-29 00:24:12: main.py:100 **  epoch: 0 batch_num: 145 lr: 0 loss: 0.578264071544 acc: 0.78167437911train_loss: 0.7121 train_acc: 56.52%
INFO: 2017-08-29 00:24:13: main.py:100 **  epoch: 0 batch_num: 146 lr: 0 loss: 0.578264071544 acc: 0.78167437911train_loss: 0.4253 train_acc: 86.13%
INFO: 2017-08-29 00:24:14: main.py:100 **  epoch: 0 batch_num: 147 lr: 0 loss: 0.578264071544 acc: 0.78167437911train_loss: 0.479 train_acc: 103.04%
INFO: 2017-08-29 00:24:15: main.py:100 **  epoch: 0 batch_num: 148 lr: 0 loss: 0.578264071544 acc: 0.78167437911train_loss: 0.4479 train_acc: 85.15%
INFO: 2017-08-29 00:24:16: main.py:100 **  epoch: 0 batch_num: 149 lr: 0 loss: 0.578264071544 acc: 0.78167437911train_loss: 0.4883 train_acc: 83.14%
INFO: 2017-08-29 00:24:17: main.py:100 **  epoch: 0 batch_num: 150 lr: 0 loss: 0.536088689168 acc: 0.818423587084train_loss: 0.3353 train_acc: 90.28%
INFO: 2017-08-29 00:24:17: main.py:100 **  epoch: 0 batch_num: 151 lr: 0 loss: 0.536088689168 acc: 0.818423587084train_loss: 0.3615 train_acc: 85.47%
INFO: 2017-08-29 00:24:18: main.py:100 **  epoch: 0 batch_num: 152 lr: 0 loss: 0.536088689168 acc: 0.818423587084train_loss: 0.415 train_acc: 77.75%
INFO: 2017-08-29 00:24:19: main.py:100 **  epoch: 0 batch_num: 153 lr: 0 loss: 0.536088689168 acc: 0.818423587084train_loss: 0.6109 train_acc: 74.93%
INFO: 2017-08-29 00:24:20: main.py:100 **  epoch: 0 batch_num: 154 lr: 0 loss: 0.536088689168 acc: 0.818423587084train_loss: 0.5654 train_acc: 96.71%
INFO: 2017-08-29 00:24:21: main.py:100 **  epoch: 0 batch_num: 155 lr: 0 loss: 0.536088689168 acc: 0.818423587084train_loss: 0.4666 train_acc: 96.29%
INFO: 2017-08-29 00:24:22: main.py:100 **  epoch: 0 batch_num: 156 lr: 0 loss: 0.536088689168 acc: 0.818423587084train_loss: 0.6837 train_acc: 63.92%
INFO: 2017-08-29 00:24:22: main.py:100 **  epoch: 0 batch_num: 157 lr: 0 loss: 0.536088689168 acc: 0.818423587084train_loss: 0.5282 train_acc: 81.23%
INFO: 2017-08-29 00:24:23: main.py:100 **  epoch: 0 batch_num: 158 lr: 0 loss: 0.536088689168 acc: 0.818423587084train_loss: 0.39 train_acc: 106.25%
INFO: 2017-08-29 00:26:12: main.py:28 **  start
INFO: 2017-08-29 00:26:12: main.py:30 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-29 00:26:12: main.py:31 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-29 00:26:12: main.py:32 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-29 00:26:12: main.py:33 **  读取数据集...
INFO: 2017-08-29 00:26:14: main.py:44 **  train data sample counts 5088
INFO: 2017-08-29 00:26:15: main.py:45 **  train data set batch size 8
INFO: 2017-08-29 00:26:15: main.py:46 **  train data batch counts 636
INFO: 2017-08-29 00:26:16: main.py:53 **  <class 'net.net.CarUNet'>


INFO: 2017-08-29 00:26:18: main.py:101 **  epoch: 0 batch_num: 0 lr: 0 loss: 1.15972852707 acc: 0.651173710823train_loss: 1.1597 train_acc: 65.12%
INFO: 2017-08-29 00:27:18: main.py:28 **  start
INFO: 2017-08-29 00:27:18: main.py:30 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-29 00:27:18: main.py:31 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-29 00:27:18: main.py:32 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-29 00:27:18: main.py:33 **  读取数据集...
INFO: 2017-08-29 00:27:21: main.py:44 **  train data sample counts 5088
INFO: 2017-08-29 00:27:21: main.py:45 **  train data set batch size 16
INFO: 2017-08-29 00:27:21: main.py:46 **  train data batch counts 318
INFO: 2017-08-29 00:27:23: main.py:53 **  <class 'net.net.CarUNet'>


INFO: 2017-08-29 00:27:37: main.py:28 **  start
INFO: 2017-08-29 00:27:37: main.py:30 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-08-29 00:27:37: main.py:31 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-08-29 00:27:37: main.py:32 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-08-29 00:27:37: main.py:33 **  读取数据集...
INFO: 2017-08-29 00:27:40: main.py:44 **  train data sample counts 5088
INFO: 2017-08-29 00:27:40: main.py:45 **  train data set batch size 8
INFO: 2017-08-29 00:27:40: main.py:46 **  train data batch counts 636
INFO: 2017-08-29 00:27:41: main.py:53 **  <class 'net.net.CarUNet'>


INFO: 2017-08-29 00:27:43: main.py:101 **  epoch: 0 batch_num: 0 lr: 0 loss: 1.2745 acc: 0.426  train_loss: 1.2745 train_acc: 42.6%
INFO: 2017-08-29 00:28:06: main.py:101 **  epoch: 0 batch_num: 30 lr: 0 loss: 1.1396 acc: 0.6438  train_loss: 1.0329 train_acc: 76.0%
INFO: 2017-08-29 00:28:29: main.py:101 **  epoch: 0 batch_num: 60 lr: 0 loss: 0.8729 acc: 0.777  train_loss: 0.7614 train_acc: 76.98%
INFO: 2017-08-29 00:28:54: main.py:101 **  epoch: 0 batch_num: 90 lr: 0 loss: 0.6828 acc: 0.8018  train_loss: 0.6682 train_acc: 77.64%
INFO: 2017-08-29 00:29:19: main.py:101 **  epoch: 0 batch_num: 120 lr: 0 loss: 0.6827 acc: 0.7597  train_loss: 1.2406 train_acc: 46.36%
INFO: 2017-08-29 00:29:44: main.py:101 **  epoch: 0 batch_num: 150 lr: 0 loss: 0.5657 acc: 0.8316  train_loss: 0.549 train_acc: 104.31%
INFO: 2017-08-29 00:30:10: main.py:101 **  epoch: 0 batch_num: 180 lr: 0 loss: 0.5914 acc: 0.7801  train_loss: 0.4945 train_acc: 73.58%
INFO: 2017-08-29 00:30:37: main.py:101 **  epoch: 0 batch_num: 210 lr: 0 loss: 0.5126 acc: 0.8406  train_loss: 0.7354 train_acc: 66.31%
INFO: 2017-08-29 00:31:04: main.py:101 **  epoch: 0 batch_num: 240 lr: 0 loss: 0.4312 acc: 0.8546  train_loss: 0.2832 train_acc: 84.37%
INFO: 2017-08-29 00:31:32: main.py:101 **  epoch: 0 batch_num: 270 lr: 0 loss: 0.4134 acc: 0.8689  train_loss: 0.3139 train_acc: 85.62%
INFO: 2017-08-29 00:31:59: main.py:101 **  epoch: 0 batch_num: 300 lr: 0 loss: 0.4104 acc: 0.8692  train_loss: 0.5676 train_acc: 74.57%
INFO: 2017-08-29 00:32:26: main.py:101 **  epoch: 0 batch_num: 330 lr: 0 loss: 0.3774 acc: 0.8784  train_loss: 0.3415 train_acc: 85.65%
INFO: 2017-08-29 00:32:53: main.py:101 **  epoch: 0 batch_num: 360 lr: 0 loss: 0.394 acc: 0.8775  train_loss: 0.3584 train_acc: 82.41%
INFO: 2017-08-29 00:33:19: main.py:101 **  epoch: 0 batch_num: 390 lr: 0 loss: 0.3709 acc: 0.8772  train_loss: 0.1809 train_acc: 95.3%
INFO: 2017-08-29 00:33:46: main.py:101 **  epoch: 0 batch_num: 420 lr: 0 loss: 0.3431 acc: 0.8985  train_loss: 0.2454 train_acc: 89.84%
INFO: 2017-08-29 00:34:12: main.py:101 **  epoch: 0 batch_num: 450 lr: 0 loss: 0.3883 acc: 0.9123  train_loss: 0.3706 train_acc: 82.69%
INFO: 2017-08-29 00:34:38: main.py:101 **  epoch: 0 batch_num: 480 lr: 0 loss: 0.3304 acc: 0.892  train_loss: 0.2693 train_acc: 91.19%
INFO: 2017-08-29 00:35:05: main.py:101 **  epoch: 0 batch_num: 510 lr: 0 loss: 0.3927 acc: 0.8935  train_loss: 0.1693 train_acc: 95.4%
INFO: 2017-08-29 00:35:31: main.py:101 **  epoch: 0 batch_num: 540 lr: 0 loss: 0.3431 acc: 0.9057  train_loss: 0.1345 train_acc: 96.29%
INFO: 2017-08-29 00:35:57: main.py:101 **  epoch: 0 batch_num: 570 lr: 0 loss: 0.2865 acc: 0.9266  train_loss: 0.1968 train_acc: 92.75%
INFO: 2017-08-29 00:36:24: main.py:101 **  epoch: 0 batch_num: 600 lr: 0 loss: 0.3105 acc: 0.9383  train_loss: 0.319 train_acc: 89.68%
INFO: 2017-08-29 00:36:51: main.py:101 **  epoch: 0 batch_num: 630 lr: 0 loss: 0.2921 acc: 0.9471  train_loss: 0.2307 train_acc: 93.59%
INFO: 2017-08-29 00:36:55: main.py:101 **  epoch: 0 batch_num: 635 lr: 0 loss: 0.2921 acc: 0.9471  train_loss: 0.335 train_acc: 84.84%
INFO: 2017-08-29 00:36:57: main.py:101 **  epoch: 1 batch_num: 0 lr: 0 loss: 0.2143 acc: 0.9144  train_loss: 0.1637 train_acc: 94.85%
INFO: 2017-08-29 00:37:24: main.py:101 **  epoch: 1 batch_num: 30 lr: 0 loss: 0.2347 acc: 0.9279  train_loss: 0.1969 train_acc: 94.52%
INFO: 2017-08-29 00:37:50: main.py:101 **  epoch: 1 batch_num: 60 lr: 0 loss: 0.2943 acc: 0.9254  train_loss: 0.2931 train_acc: 88.27%
INFO: 2017-08-29 00:38:16: main.py:101 **  epoch: 1 batch_num: 90 lr: 0 loss: 0.277 acc: 0.933  train_loss: 0.2978 train_acc: 105.58%
INFO: 2017-08-29 00:38:42: main.py:101 **  epoch: 1 batch_num: 120 lr: 0 loss: 0.2755 acc: 0.9357  train_loss: 0.302 train_acc: 105.46%
INFO: 2017-08-29 00:39:09: main.py:101 **  epoch: 1 batch_num: 150 lr: 0 loss: 0.2428 acc: 0.943  train_loss: 0.2182 train_acc: 91.92%
INFO: 2017-08-29 00:39:35: main.py:101 **  epoch: 1 batch_num: 180 lr: 0 loss: 0.2299 acc: 0.9524  train_loss: 0.385 train_acc: 81.2%
INFO: 2017-08-29 00:40:02: main.py:101 **  epoch: 1 batch_num: 210 lr: 0 loss: 0.2589 acc: 0.9408  train_loss: 0.4051 train_acc: 72.36%
INFO: 2017-08-29 00:40:28: main.py:101 **  epoch: 1 batch_num: 240 lr: 0 loss: 0.2219 acc: 0.9435  train_loss: 0.1575 train_acc: 95.51%
INFO: 2017-08-29 00:40:55: main.py:101 **  epoch: 1 batch_num: 270 lr: 0 loss: 0.2448 acc: 0.9262  train_loss: 0.49 train_acc: 81.93%
INFO: 2017-08-29 00:41:22: main.py:101 **  epoch: 1 batch_num: 300 lr: 0 loss: 0.297 acc: 0.9119  train_loss: 0.2441 train_acc: 91.37%
INFO: 2017-08-29 00:41:48: main.py:101 **  epoch: 1 batch_num: 330 lr: 0 loss: 0.2542 acc: 0.9344  train_loss: 0.1233 train_acc: 95.61%
INFO: 2017-08-29 00:42:14: main.py:101 **  epoch: 1 batch_num: 360 lr: 0 loss: 0.2351 acc: 0.967  train_loss: 0.2149 train_acc: 109.1%
INFO: 2017-08-29 00:42:41: main.py:101 **  epoch: 1 batch_num: 390 lr: 0 loss: 0.2688 acc: 0.9313  train_loss: 0.2497 train_acc: 88.08%
INFO: 2017-08-29 00:43:08: main.py:101 **  epoch: 1 batch_num: 420 lr: 0 loss: 0.2051 acc: 0.9286  train_loss: 0.2989 train_acc: 107.63%
INFO: 2017-08-29 00:43:35: main.py:101 **  epoch: 1 batch_num: 450 lr: 0 loss: 0.2225 acc: 0.9391  train_loss: 0.128 train_acc: 94.31%
INFO: 2017-08-29 00:44:02: main.py:101 **  epoch: 1 batch_num: 480 lr: 0 loss: 0.2207 acc: 0.938  train_loss: 0.1537 train_acc: 93.47%
INFO: 2017-08-29 00:44:28: main.py:101 **  epoch: 1 batch_num: 510 lr: 0 loss: 0.2118 acc: 0.9746  train_loss: 0.2561 train_acc: 91.07%
INFO: 2017-09-02 16:50:24: main.py:28 **  start
INFO: 2017-09-02 16:50:24: main.py:30 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-09-02 16:50:24: main.py:31 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-09-02 16:50:24: main.py:32 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-09-02 16:50:24: main.py:33 **  读取数据集...
INFO: 2017-09-02 16:56:24: main.py:28 **  start
INFO: 2017-09-02 16:56:24: main.py:30 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-09-02 16:56:24: main.py:31 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-09-02 16:56:24: main.py:32 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-09-02 16:56:24: main.py:33 **  读取数据集...
INFO: 2017-09-02 16:56:27: utils.py:58 **  All data size is 5088
INFO: 2017-09-02 16:56:27: utils.py:59 **  All data size is 5088
INFO: 2017-09-02 16:56:27: utils.py:61 **  Valid data size is 1017
INFO: 2017-09-02 16:56:27: main.py:54 **  all data sample counts 5088
INFO: 2017-09-02 16:56:27: main.py:55 **  train data set batch size 8
INFO: 2017-09-02 16:56:27: main.py:56 **  train data batch counts 508
INFO: 2017-09-02 16:56:27: main.py:57 **  valid data batch counts 1018
INFO: 2017-09-02 16:57:19: main.py:28 **  start
INFO: 2017-09-02 16:57:19: main.py:30 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-09-02 16:57:19: main.py:31 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-09-02 16:57:19: main.py:32 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-09-02 16:57:19: main.py:33 **  读取数据集...
INFO: 2017-09-02 16:57:22: utils.py:58 **  All data size is 5088
INFO: 2017-09-02 16:57:22: utils.py:59 **  All data size is 5088
INFO: 2017-09-02 16:57:22: utils.py:61 **  Valid data size is 1017
INFO: 2017-09-02 16:57:22: main.py:54 **  all data sample counts 5088
INFO: 2017-09-02 16:57:22: main.py:55 **  train data set batch size 8
INFO: 2017-09-02 16:57:22: main.py:56 **  train data sample counts 4064
INFO: 2017-09-02 16:57:22: main.py:57 **  valid data sample counts 1018
INFO: 2017-09-02 17:08:54: main.py:67 **  start
INFO: 2017-09-02 17:08:54: main.py:69 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-09-02 17:08:54: main.py:70 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-09-02 17:08:54: main.py:71 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-09-02 17:08:54: main.py:72 **  读取数据集...
INFO: 2017-09-02 17:08:57: utils.py:58 **  All data size is 5088
INFO: 2017-09-02 17:08:57: utils.py:59 **  All data size is 5088
INFO: 2017-09-02 17:08:57: utils.py:61 **  Valid data size is 1017
INFO: 2017-09-02 17:08:57: main.py:93 **  all data sample counts 5088
INFO: 2017-09-02 17:08:57: main.py:94 **  train data set batch size 8
INFO: 2017-09-02 17:08:57: main.py:95 **  train data sample counts 4064
INFO: 2017-09-02 17:08:57: main.py:96 **  valid data sample counts 1018
INFO: 2017-09-02 17:08:59: main.py:103 **  <class 'net.net.CarUNet'>


INFO: 2017-09-02 17:09:01: main.py:150 **  epoch: 0 batch_num: 0 lr: 0 loss: 1.2879 acc: 11.1%  train_loss: 1.2879 train_acc: 11.1%
INFO: 2017-09-02 17:09:24: main.py:150 **  epoch: 0 batch_num: 30 lr: 0 loss: 1.1796 acc: 53.5%  train_loss: 1.1742 train_acc: 58.02%
INFO: 2017-09-02 17:09:47: main.py:150 **  epoch: 0 batch_num: 60 lr: 0 loss: 0.8365 acc: 78.61%  train_loss: 0.6419 train_acc: 83.42%
INFO: 2017-09-02 17:10:10: main.py:150 **  epoch: 0 batch_num: 90 lr: 0 loss: 0.6141 acc: 79.66%  train_loss: 0.6896 train_acc: 73.64%
INFO: 2017-09-02 17:10:34: main.py:150 **  epoch: 0 batch_num: 120 lr: 0 loss: 0.5402 acc: 80.49%  train_loss: 1.4354 train_acc: 63.94%
INFO: 2017-09-02 17:10:58: main.py:150 **  epoch: 0 batch_num: 150 lr: 0 loss: 0.4835 acc: 83.04%  train_loss: 0.4288 train_acc: 89.65%
INFO: 2017-09-02 17:11:24: main.py:150 **  epoch: 0 batch_num: 180 lr: 0 loss: 0.5245 acc: 81.99%  train_loss: 0.3594 train_acc: 89.09%
INFO: 2017-09-02 17:11:49: main.py:150 **  epoch: 0 batch_num: 210 lr: 0 loss: 0.4966 acc: 85.75%  train_loss: 0.441 train_acc: 120.82%
INFO: 2017-09-02 17:12:15: main.py:150 **  epoch: 0 batch_num: 240 lr: 0 loss: 0.4615 acc: 84.42%  train_loss: 0.3316 train_acc: 88.93%
INFO: 2017-09-02 17:12:41: main.py:150 **  epoch: 0 batch_num: 270 lr: 0 loss: 0.3979 acc: 88.38%  train_loss: 0.526 train_acc: 66.93%
INFO: 2017-09-02 17:13:07: main.py:150 **  epoch: 0 batch_num: 300 lr: 0 loss: 0.5722 acc: 81.92%  train_loss: 0.6894 train_acc: 83.02%
INFO: 2017-09-02 17:13:33: main.py:150 **  epoch: 0 batch_num: 330 lr: 0 loss: 0.4158 acc: 85.36%  train_loss: 0.3114 train_acc: 84.52%
INFO: 2017-09-02 17:13:58: main.py:150 **  epoch: 0 batch_num: 360 lr: 0 loss: 0.4029 acc: 89.01%  train_loss: 0.351 train_acc: 80.96%
INFO: 2017-09-02 17:14:24: main.py:150 **  epoch: 0 batch_num: 390 lr: 0 loss: 0.4312 acc: 86.11%  train_loss: 0.268 train_acc: 90.83%
INFO: 2017-09-02 17:14:50: main.py:150 **  epoch: 0 batch_num: 420 lr: 0 loss: 0.3435 acc: 87.09%  train_loss: 0.3472 train_acc: 82.1%
INFO: 2017-09-02 17:15:16: main.py:150 **  epoch: 0 batch_num: 450 lr: 0 loss: 0.36 acc: 88.52%  train_loss: 0.1697 train_acc: 93.51%
INFO: 2017-09-02 17:15:42: main.py:150 **  epoch: 0 batch_num: 480 lr: 0 loss: 0.3904 acc: 87.84%  train_loss: 0.308 train_acc: 89.97%
INFO: 2017-09-02 17:16:05: main.py:150 **  epoch: 0 batch_num: 507 lr: 0 loss: 0.3904 acc: 87.84%  train_loss: 0.3424 train_acc: 91.38%
INFO: 2017-09-02 17:25:13: main.py:53 **  start
INFO: 2017-09-02 17:25:13: main.py:55 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-09-02 17:25:13: main.py:56 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-09-02 17:25:13: main.py:57 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-09-02 17:25:13: main.py:58 **  读取数据集...
INFO: 2017-09-02 17:25:16: utils.py:58 **  All data size is 5088
INFO: 2017-09-02 17:25:16: utils.py:59 **  All data size is 5088
INFO: 2017-09-02 17:25:16: utils.py:61 **  Valid data size is 1017
INFO: 2017-09-02 17:25:16: main.py:79 **  all data sample counts 5088
INFO: 2017-09-02 17:25:16: main.py:80 **  train data set batch size 8
INFO: 2017-09-02 17:25:16: main.py:81 **  train data sample counts 0
INFO: 2017-09-02 17:25:16: main.py:82 **  valid data sample counts 1018
INFO: 2017-09-02 17:25:17: main.py:89 **  <class 'net.net.CarUNet'>


INFO: 2017-09-02 17:27:17: main.py:53 **  start
INFO: 2017-09-02 17:27:17: main.py:55 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-09-02 17:27:17: main.py:56 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-09-02 17:27:17: main.py:57 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-09-02 17:27:17: main.py:58 **  读取数据集...
INFO: 2017-09-02 17:27:20: utils.py:58 **  All data size is 5088
INFO: 2017-09-02 17:27:20: utils.py:59 **  All data size is 5088
INFO: 2017-09-02 17:27:20: utils.py:61 **  Valid data size is 1017
INFO: 2017-09-02 17:27:20: main.py:79 **  all data sample counts 5088
INFO: 2017-09-02 17:27:20: main.py:80 **  train data set batch size 8
INFO: 2017-09-02 17:27:20: main.py:81 **  train data sample counts 0
INFO: 2017-09-02 17:27:20: main.py:82 **  valid data sample counts 1018
INFO: 2017-09-02 17:27:21: main.py:89 **  <class 'net.net.CarUNet'>


INFO: 2017-09-02 17:27:56: main.py:143 **  1.298090430630.54182844285
INFO: 2017-09-02 17:28:57: main.py:53 **  start
INFO: 2017-09-02 17:28:57: main.py:55 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-09-02 17:28:57: main.py:56 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-09-02 17:28:57: main.py:57 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-09-02 17:28:57: main.py:58 **  读取数据集...
INFO: 2017-09-02 17:29:00: utils.py:58 **  All data size is 5088
INFO: 2017-09-02 17:29:00: utils.py:59 **  All data size is 5088
INFO: 2017-09-02 17:29:00: utils.py:61 **  Valid data size is 1017
INFO: 2017-09-02 17:29:00: main.py:79 **  all data sample counts 5088
INFO: 2017-09-02 17:29:00: main.py:80 **  train data set batch size 8
INFO: 2017-09-02 17:29:00: main.py:81 **  train data sample counts 96
INFO: 2017-09-02 17:29:00: main.py:82 **  valid data sample counts 1018
INFO: 2017-09-02 17:29:01: main.py:89 **  <class 'net.net.CarUNet'>


INFO: 2017-09-02 17:29:03: main.py:136 **  epoch: 0 batch_num: 0 lr: 0 loss: 1.19 acc: 64.6%  train_loss: 1.19 train_acc: 64.6%
INFO: 2017-09-02 17:29:12: main.py:136 **  epoch: 0 batch_num: 11 lr: 0 loss: 1.19 acc: 64.6%  train_loss: 1.3182 train_acc: 44.48%
INFO: 2017-09-02 17:29:47: main.py:143 **  epoch: 0valid_loss: 1.32299167339valid_acc: 0.506428274678
INFO: 2017-09-02 17:29:49: main.py:136 **  epoch: 1 batch_num: 0 lr: 0 loss: 1.2606 acc: 52.45%  train_loss: 1.255 train_acc: 48.01%
INFO: 2017-09-02 17:29:58: main.py:136 **  epoch: 1 batch_num: 11 lr: 0 loss: 1.2606 acc: 52.45%  train_loss: 1.177 train_acc: 46.69%
INFO: 2017-09-02 17:30:34: main.py:143 **  epoch: 1valid_loss: 1.28368828731valid_acc: 0.540727836953
INFO: 2017-09-02 17:30:35: main.py:136 **  epoch: 2 batch_num: 0 lr: 0 loss: 1.1988 acc: 51.19%  train_loss: 1.2481 train_acc: 40.96%
INFO: 2017-09-02 17:31:02: main.py:53 **  start
INFO: 2017-09-02 17:31:02: main.py:55 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-09-02 17:31:02: main.py:56 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-09-02 17:31:02: main.py:57 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-09-02 17:31:02: main.py:58 **  读取数据集...
INFO: 2017-09-02 17:31:05: utils.py:58 **  All data size is 5088
INFO: 2017-09-02 17:31:05: utils.py:59 **  All data size is 5088
INFO: 2017-09-02 17:31:05: utils.py:61 **  Valid data size is 1017
INFO: 2017-09-02 17:31:05: main.py:79 **  all data sample counts 5088
INFO: 2017-09-02 17:31:05: main.py:80 **  train data set batch size 8
INFO: 2017-09-02 17:31:05: main.py:81 **  train data sample counts 4064
INFO: 2017-09-02 17:31:05: main.py:82 **  valid data sample counts 1018
INFO: 2017-09-02 17:31:06: main.py:89 **  <class 'net.net.CarUNet'>


INFO: 2017-09-02 17:31:08: main.py:136 **  epoch: 0 batch_num: 0 lr: 0 loss: 1.3224 acc: 50.74%  train_loss: 1.3224 train_acc: 50.74%
INFO: 2017-09-02 17:31:32: main.py:136 **  epoch: 0 batch_num: 30 lr: 0 loss: 1.3005 acc: 52.55%  train_loss: 1.2992 train_acc: 57.19%
INFO: 2017-09-02 17:31:58: main.py:136 **  epoch: 0 batch_num: 60 lr: 0 loss: 0.9804 acc: 76.82%  train_loss: 0.685 train_acc: 86.72%
INFO: 2017-09-02 17:32:24: main.py:136 **  epoch: 0 batch_num: 90 lr: 0 loss: 0.6682 acc: 79.16%  train_loss: 0.5902 train_acc: 75.03%
INFO: 2017-09-02 17:32:50: main.py:136 **  epoch: 0 batch_num: 120 lr: 0 loss: 0.5994 acc: 79.36%  train_loss: 0.4548 train_acc: 86.17%
INFO: 2017-09-02 17:33:15: main.py:136 **  epoch: 0 batch_num: 150 lr: 0 loss: 0.5278 acc: 80.74%  train_loss: 0.4444 train_acc: 80.38%
INFO: 2017-09-02 17:33:41: main.py:136 **  epoch: 0 batch_num: 180 lr: 0 loss: 0.4731 acc: 84.2%  train_loss: 0.4969 train_acc: 84.29%
INFO: 2017-09-02 17:34:07: main.py:136 **  epoch: 0 batch_num: 210 lr: 0 loss: 0.5686 acc: 80.78%  train_loss: 0.6109 train_acc: 67.8%
INFO: 2017-09-02 17:34:33: main.py:136 **  epoch: 0 batch_num: 240 lr: 0 loss: 0.5683 acc: 79.62%  train_loss: 0.3575 train_acc: 82.83%
INFO: 2017-09-02 17:34:59: main.py:136 **  epoch: 0 batch_num: 270 lr: 0 loss: 0.4557 acc: 85.57%  train_loss: 0.3893 train_acc: 88.65%
INFO: 2017-09-02 17:35:25: main.py:136 **  epoch: 0 batch_num: 300 lr: 0 loss: 0.3507 acc: 91.27%  train_loss: 0.2915 train_acc: 109.24%
INFO: 2017-09-02 17:35:51: main.py:136 **  epoch: 0 batch_num: 330 lr: 0 loss: 0.4108 acc: 87.65%  train_loss: 0.2827 train_acc: 93.72%
INFO: 2017-09-02 17:36:17: main.py:136 **  epoch: 0 batch_num: 360 lr: 0 loss: 0.384 acc: 88.53%  train_loss: 0.2638 train_acc: 88.61%
INFO: 2017-09-02 17:36:43: main.py:136 **  epoch: 0 batch_num: 390 lr: 0 loss: 0.4189 acc: 89.56%  train_loss: 0.37 train_acc: 87.32%
INFO: 2017-09-02 17:37:09: main.py:136 **  epoch: 0 batch_num: 420 lr: 0 loss: 0.3685 acc: 87.93%  train_loss: 0.4548 train_acc: 83.9%
INFO: 2017-09-02 17:37:34: main.py:136 **  epoch: 0 batch_num: 450 lr: 0 loss: 0.2885 acc: 90.58%  train_loss: 0.2356 train_acc: 92.68%
INFO: 2017-09-02 17:38:01: main.py:136 **  epoch: 0 batch_num: 480 lr: 0 loss: 0.3023 acc: 91.29%  train_loss: 0.2198 train_acc: 92.66%
INFO: 2017-09-02 17:38:24: main.py:136 **  epoch: 0 batch_num: 507 lr: 0 loss: 0.3023 acc: 91.29%  train_loss: 0.2904 train_acc: 90.36%
INFO: 2017-09-02 17:39:02: main.py:144 **  epoch: 0 valid_loss: 0.2695 valid_acc: 95.54%
INFO: 2017-09-02 17:39:03: main.py:136 **  epoch: 1 batch_num: 0 lr: 0 loss: 0.3527 acc: 88.95%  train_loss: 0.2903 train_acc: 83.65%
INFO: 2017-09-02 17:39:29: main.py:136 **  epoch: 1 batch_num: 30 lr: 0 loss: 0.428 acc: 87.51%  train_loss: 0.3667 train_acc: 83.07%
INFO: 2017-09-02 17:39:55: main.py:136 **  epoch: 1 batch_num: 60 lr: 0 loss: 0.4275 acc: 87.51%  train_loss: 0.4527 train_acc: 70.07%
INFO: 2017-09-02 17:40:21: main.py:136 **  epoch: 1 batch_num: 90 lr: 0 loss: 0.3392 acc: 89.02%  train_loss: 0.2612 train_acc: 91.82%
INFO: 2017-09-02 17:40:47: main.py:136 **  epoch: 1 batch_num: 120 lr: 0 loss: 0.2821 acc: 91.02%  train_loss: 0.3193 train_acc: 87.87%
INFO: 2017-09-02 17:41:13: main.py:136 **  epoch: 1 batch_num: 150 lr: 0 loss: 0.3114 acc: 90.24%  train_loss: 0.2856 train_acc: 108.02%
INFO: 2017-09-02 17:41:39: main.py:136 **  epoch: 1 batch_num: 180 lr: 0 loss: 0.2632 acc: 89.67%  train_loss: 0.1348 train_acc: 97.14%
INFO: 2017-09-02 17:42:05: main.py:136 **  epoch: 1 batch_num: 210 lr: 0 loss: 0.2684 acc: 93.97%  train_loss: 0.2144 train_acc: 91.27%
INFO: 2017-09-02 17:42:31: main.py:136 **  epoch: 1 batch_num: 240 lr: 0 loss: 0.2851 acc: 93.95%  train_loss: 0.5452 train_acc: 85.22%
INFO: 2017-09-02 17:42:57: main.py:136 **  epoch: 1 batch_num: 270 lr: 0 loss: 0.2892 acc: 91.99%  train_loss: 0.1413 train_acc: 96.97%
INFO: 2017-09-02 17:43:23: main.py:136 **  epoch: 1 batch_num: 300 lr: 0 loss: 0.2733 acc: 92.81%  train_loss: 0.2311 train_acc: 108.21%
INFO: 2017-09-02 17:43:48: main.py:136 **  epoch: 1 batch_num: 330 lr: 0 loss: 0.1922 acc: 94.27%  train_loss: 0.2135 train_acc: 110.09%
INFO: 2017-09-02 17:44:14: main.py:136 **  epoch: 1 batch_num: 360 lr: 0 loss: 0.2286 acc: 93.59%  train_loss: 0.1925 train_acc: 93.9%
INFO: 2017-09-02 17:44:40: main.py:136 **  epoch: 1 batch_num: 390 lr: 0 loss: 0.2718 acc: 92.4%  train_loss: 0.4519 train_acc: 81.92%
INFO: 2017-09-02 17:45:06: main.py:136 **  epoch: 1 batch_num: 420 lr: 0 loss: 0.253 acc: 96.37%  train_loss: 0.2718 train_acc: 89.02%
INFO: 2017-09-02 17:45:32: main.py:136 **  epoch: 1 batch_num: 450 lr: 0 loss: 0.2387 acc: 98.01%  train_loss: 0.2048 train_acc: 90.48%
INFO: 2017-09-02 17:45:58: main.py:136 **  epoch: 1 batch_num: 480 lr: 0 loss: 0.2131 acc: 97.43%  train_loss: 0.1088 train_acc: 96.86%
INFO: 2017-09-02 17:46:21: main.py:136 **  epoch: 1 batch_num: 507 lr: 0 loss: 0.2131 acc: 97.43%  train_loss: 0.2009 train_acc: 85.28%
INFO: 2017-09-02 17:46:58: main.py:144 **  epoch: 1 valid_loss: 0.2257 valid_acc: 96.35%
INFO: 2017-09-02 17:47:00: main.py:136 **  epoch: 2 batch_num: 0 lr: 0 loss: 0.2095 acc: 93.95%  train_loss: 0.1504 train_acc: 95.44%
INFO: 2017-09-02 17:47:26: main.py:136 **  epoch: 2 batch_num: 30 lr: 0 loss: 0.2202 acc: 96.71%  train_loss: 0.1156 train_acc: 95.96%
INFO: 2017-09-02 17:47:52: main.py:136 **  epoch: 2 batch_num: 60 lr: 0 loss: 0.2434 acc: 94.76%  train_loss: 0.0755 train_acc: 98.19%
INFO: 2017-09-02 17:48:18: main.py:136 **  epoch: 2 batch_num: 90 lr: 0 loss: 0.1813 acc: 94.9%  train_loss: 0.1032 train_acc: 97.36%
INFO: 2017-09-02 17:48:44: main.py:136 **  epoch: 2 batch_num: 120 lr: 0 loss: 0.1942 acc: 95.57%  train_loss: 0.1138 train_acc: 97.4%
INFO: 2017-09-02 17:49:10: main.py:136 **  epoch: 2 batch_num: 150 lr: 0 loss: 0.1695 acc: 94.39%  train_loss: 0.0996 train_acc: 97.57%
INFO: 2017-09-02 17:49:35: main.py:136 **  epoch: 2 batch_num: 180 lr: 0 loss: 0.2203 acc: 94.81%  train_loss: 0.3156 train_acc: 94.16%
INFO: 2017-09-02 17:50:01: main.py:136 **  epoch: 2 batch_num: 210 lr: 0 loss: 0.2531 acc: 95.65%  train_loss: 0.3714 train_acc: 120.18%
INFO: 2017-09-02 17:50:27: main.py:136 **  epoch: 2 batch_num: 240 lr: 0 loss: 0.1643 acc: 95.38%  train_loss: 0.0857 train_acc: 96.8%
INFO: 2017-09-02 17:50:53: main.py:136 **  epoch: 2 batch_num: 270 lr: 0 loss: 0.1685 acc: 95.45%  train_loss: 0.1056 train_acc: 96.36%
INFO: 2017-09-02 17:51:19: main.py:136 **  epoch: 2 batch_num: 300 lr: 0 loss: 0.1917 acc: 96.29%  train_loss: 0.1623 train_acc: 92.31%
INFO: 2017-09-02 17:51:45: main.py:136 **  epoch: 2 batch_num: 330 lr: 0 loss: 0.1814 acc: 99.01%  train_loss: 0.0944 train_acc: 96.78%
INFO: 2017-09-02 17:52:11: main.py:136 **  epoch: 2 batch_num: 360 lr: 0 loss: 0.1551 acc: 96.91%  train_loss: 0.1215 train_acc: 95.16%
INFO: 2017-09-02 17:52:37: main.py:136 **  epoch: 2 batch_num: 390 lr: 0 loss: 0.1526 acc: 96.6%  train_loss: 0.0556 train_acc: 98.38%
INFO: 2017-09-02 17:53:03: main.py:136 **  epoch: 2 batch_num: 420 lr: 0 loss: 0.1813 acc: 94.74%  train_loss: 0.3315 train_acc: 89.1%
INFO: 2017-09-02 17:53:29: main.py:136 **  epoch: 2 batch_num: 450 lr: 0 loss: 0.1531 acc: 96.83%  train_loss: 0.0537 train_acc: 98.3%
INFO: 2017-09-02 17:53:54: main.py:136 **  epoch: 2 batch_num: 480 lr: 0 loss: 0.1347 acc: 98.06%  train_loss: 0.1286 train_acc: 96.64%
INFO: 2017-09-02 17:54:18: main.py:136 **  epoch: 2 batch_num: 507 lr: 0 loss: 0.1347 acc: 98.06%  train_loss: 0.1059 train_acc: 96.35%
INFO: 2017-09-02 17:54:55: main.py:144 **  epoch: 2 valid_loss: 0.1302 valid_acc: 99.49%
INFO: 2017-09-02 22:07:26: main.py:53 **  Start
INFO: 2017-09-02 22:07:26: main.py:55 **  训练数据集--图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-09-02 22:07:26: main.py:56 **  训练数据集--掩膜图片存储路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-09-02 22:07:26: main.py:57 **  训练数据集--标签csv路径是/home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-09-02 22:07:26: main.py:58 **  读取数据集...
INFO: 2017-09-02 22:07:29: utils.py:58 **  All data size is 5088
INFO: 2017-09-02 22:07:29: utils.py:59 **  All data size is 5088
INFO: 2017-09-02 22:07:29: utils.py:61 **  Valid data size is 1017
INFO: 2017-09-02 22:07:29: main.py:79 **  all data sample counts 5088
INFO: 2017-09-02 22:07:29: main.py:80 **  train data set batch size 8
INFO: 2017-09-02 22:07:29: main.py:81 **  train data sample counts 4064
INFO: 2017-09-02 22:07:29: main.py:82 **  valid data sample counts 1018
INFO: 2017-09-02 22:07:31: main.py:89 **  <class 'net.net.CarUNet'>


INFO: 2017-09-02 22:12:56: main.py:53 **  Start
INFO: 2017-09-02 22:12:56: main.py:55 **  训练数据集--图片存储路径是 /home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-09-02 22:12:56: main.py:56 **  训练数据集--掩膜图片存储路径是 /home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-09-02 22:12:56: main.py:57 **  训练数据集--标签csv路径是 /home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-09-02 22:12:56: main.py:58 **  读取数据集...
INFO: 2017-09-02 22:26:57: main.py:98 **  Start
INFO: 2017-09-02 22:26:57: main.py:100 **  训练数据集--图片存储路径是 /home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-09-02 22:26:57: main.py:101 **  训练数据集--掩膜图片存储路径是 /home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-09-02 22:26:57: main.py:102 **  训练数据集--标签csv路径是 /home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-09-02 22:26:57: main.py:103 **  读取数据集...
INFO: 2017-09-02 22:27:00: utils.py:58 **  All dataset size is 5088
INFO: 2017-09-02 22:27:00: utils.py:59 **  Train dataset size is 4071
INFO: 2017-09-02 22:27:00: utils.py:61 **  Valid dataset size is 1017
INFO: 2017-09-02 22:27:00: main.py:124 **  all data sample counts 5088
INFO: 2017-09-02 22:27:00: main.py:125 **  train data set batch size 8
INFO: 2017-09-02 22:27:00: main.py:126 **  train data sample counts 4064
INFO: 2017-09-02 22:27:00: main.py:127 **  valid data sample counts 1018
INFO: 2017-09-02 22:27:01: main.py:134 **  <class 'net.net.CarUNet'>


INFO: 2017-09-02 22:27:04: main.py:89 **  epoch: 0 batch_num: 0 lr: 0 loss: 1.3346 acc: 3.43%  train_loss: 1.3346 train_acc: 3.43%
INFO: 2017-09-02 22:27:26: main.py:89 **  epoch: 0 batch_num: 30 lr: 0 loss: 1.2347 acc: 44.61%  train_loss: 1.1679 train_acc: 75.62%
INFO: 2017-09-02 22:27:49: main.py:89 **  epoch: 0 batch_num: 60 lr: 0 loss: 0.9303 acc: 78.85%  train_loss: 0.7944 train_acc: 79.83%
INFO: 2017-09-02 23:56:05: main.py:98 **  Start
INFO: 2017-09-02 23:56:05: main.py:100 **  训练数据集--图片存储路径是 /home/wuliang/wuliang/CIMC/wuliang/dataset/train
INFO: 2017-09-02 23:56:05: main.py:101 **  训练数据集--掩膜图片存储路径是 /home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks
INFO: 2017-09-02 23:56:05: main.py:102 **  训练数据集--标签csv路径是 /home/wuliang/wuliang/CIMC/wuliang/dataset/train_masks.csv
INFO: 2017-09-02 23:56:05: main.py:103 **  读取数据集...
INFO: 2017-09-02 23:56:08: utils.py:58 **  All dataset size is 5088
INFO: 2017-09-02 23:56:08: utils.py:59 **  Train dataset size is 4071
INFO: 2017-09-02 23:56:08: utils.py:61 **  Valid dataset size is 1017
INFO: 2017-09-02 23:56:08: main.py:124 **  all data sample counts 5088
INFO: 2017-09-02 23:56:08: main.py:125 **  train data set batch size 8
INFO: 2017-09-02 23:56:08: main.py:126 **  train data sample counts 4064
INFO: 2017-09-02 23:56:08: main.py:127 **  valid data sample counts 1018
INFO: 2017-09-02 23:56:09: main.py:134 **  <class 'net.net.CarUNet'>


INFO: 2017-09-02 23:56:11: main.py:89 **  epoch: 0 batch_num: 0 lr: 0 loss: 1.2415 acc: 54.77%  train_loss: 1.2415 train_acc: 54.77%
INFO: 2017-09-02 23:56:34: main.py:89 **  epoch: 0 batch_num: 30 lr: 0 loss: 1.2619 acc: 60.53%  train_loss: 1.1707 train_acc: 75.32%
INFO: 2017-09-02 23:56:57: main.py:89 **  epoch: 0 batch_num: 60 lr: 0 loss: 1.1005 acc: 71.72%  train_loss: 0.9544 train_acc: 81.98%
INFO: 2017-09-02 23:57:20: main.py:89 **  epoch: 0 batch_num: 90 lr: 0 loss: 0.879 acc: 76.7%  train_loss: 0.737 train_acc: 82.62%
INFO: 2017-09-02 23:57:44: main.py:89 **  epoch: 0 batch_num: 120 lr: 0 loss: 0.74 acc: 78.44%  train_loss: 0.5799 train_acc: 91.99%
INFO: 2017-09-02 23:58:08: main.py:89 **  epoch: 0 batch_num: 150 lr: 0 loss: 0.6284 acc: 77.23%  train_loss: 0.5457 train_acc: 78.02%
INFO: 2017-09-02 23:58:32: main.py:89 **  epoch: 0 batch_num: 180 lr: 0 loss: 0.6323 acc: 79.46%  train_loss: 0.5709 train_acc: 79.06%
INFO: 2017-09-02 23:58:57: main.py:89 **  epoch: 0 batch_num: 210 lr: 0 loss: 0.5817 acc: 79.3%  train_loss: 0.5677 train_acc: 74.14%
INFO: 2017-09-02 23:59:24: main.py:89 **  epoch: 0 batch_num: 240 lr: 0 loss: 0.6082 acc: 80.73%  train_loss: 0.6244 train_acc: 70.12%
INFO: 2017-09-02 23:59:51: main.py:89 **  epoch: 0 batch_num: 270 lr: 0 loss: 0.4958 acc: 87.7%  train_loss: 0.2739 train_acc: 93.6%
INFO: 2017-09-03 00:00:18: main.py:89 **  epoch: 0 batch_num: 300 lr: 0 loss: 0.4965 acc: 82.06%  train_loss: 0.5927 train_acc: 70.74%
INFO: 2017-09-03 00:00:44: main.py:89 **  epoch: 0 batch_num: 330 lr: 0 loss: 0.4039 acc: 87.89%  train_loss: 0.2355 train_acc: 93.65%
INFO: 2017-09-03 00:01:10: main.py:89 **  epoch: 0 batch_num: 360 lr: 0 loss: 0.5494 acc: 82.15%  train_loss: 0.9946 train_acc: 48.61%
INFO: 2017-09-03 00:01:38: main.py:89 **  epoch: 0 batch_num: 390 lr: 0 loss: 0.479 acc: 84.43%  train_loss: 0.3292 train_acc: 87.98%
INFO: 2017-09-03 00:02:05: main.py:89 **  epoch: 0 batch_num: 420 lr: 0 loss: 0.3995 acc: 87.0%  train_loss: 0.4795 train_acc: 96.79%
INFO: 2017-09-03 00:02:32: main.py:89 **  epoch: 0 batch_num: 450 lr: 0 loss: 0.4324 acc: 85.9%  train_loss: 0.239 train_acc: 92.09%
INFO: 2017-09-03 00:03:00: main.py:89 **  epoch: 0 batch_num: 480 lr: 0 loss: 0.4639 acc: 84.65%  train_loss: 0.7628 train_acc: 69.7%
INFO: 2017-09-03 00:03:23: main.py:89 **  epoch: 0 batch_num: 507 lr: 0 loss: 0.4639 acc: 84.65%  train_loss: 1.2034 train_acc: 62.37%
INFO: 2017-09-03 00:04:00: main.py:147 **  epoch: 0 valid_loss: 0.5948 valid_acc: 86.84%
INFO: 2017-09-03 00:04:01: main.py:89 **  epoch: 1 batch_num: 0 lr: 0 loss: 0.8046 acc: 74.49%  train_loss: 0.8046 train_acc: 74.49%
INFO: 2017-09-03 00:04:28: main.py:89 **  epoch: 1 batch_num: 30 lr: 0 loss: 0.4292 acc: 87.23%  train_loss: 0.7416 train_acc: 100.3%
INFO: 2017-09-03 00:04:55: main.py:89 **  epoch: 1 batch_num: 60 lr: 0 loss: 0.3064 acc: 92.34%  train_loss: 0.194 train_acc: 94.05%
INFO: 2017-09-03 00:05:22: main.py:89 **  epoch: 1 batch_num: 90 lr: 0 loss: 0.3673 acc: 88.99%  train_loss: 0.4151 train_acc: 76.52%
INFO: 2017-09-03 00:05:49: main.py:89 **  epoch: 1 batch_num: 120 lr: 0 loss: 0.3422 acc: 90.36%  train_loss: 0.2167 train_acc: 95.4%
INFO: 2017-09-03 00:06:17: main.py:89 **  epoch: 1 batch_num: 150 lr: 0 loss: 0.3674 acc: 88.11%  train_loss: 0.288 train_acc: 88.33%
INFO: 2017-09-03 00:06:44: main.py:89 **  epoch: 1 batch_num: 180 lr: 0 loss: 0.2968 acc: 91.82%  train_loss: 0.1178 train_acc: 96.86%
INFO: 2017-09-03 00:07:11: main.py:89 **  epoch: 1 batch_num: 210 lr: 0 loss: 0.278 acc: 92.4%  train_loss: 0.2754 train_acc: 85.7%
INFO: 2017-09-03 00:07:38: main.py:89 **  epoch: 1 batch_num: 240 lr: 0 loss: 0.2492 acc: 94.29%  train_loss: 0.4525 train_acc: 79.39%
INFO: 2017-09-03 00:08:04: main.py:89 **  epoch: 1 batch_num: 270 lr: 0 loss: 0.3116 acc: 91.27%  train_loss: 0.4219 train_acc: 107.63%
INFO: 2017-09-03 00:08:31: main.py:89 **  epoch: 1 batch_num: 300 lr: 0 loss: 0.2617 acc: 92.85%  train_loss: 0.1058 train_acc: 96.76%
INFO: 2017-09-03 00:08:58: main.py:89 **  epoch: 1 batch_num: 330 lr: 0 loss: 0.2405 acc: 94.09%  train_loss: 0.131 train_acc: 97.13%
INFO: 2017-09-03 00:09:25: main.py:89 **  epoch: 1 batch_num: 360 lr: 0 loss: 0.2545 acc: 93.52%  train_loss: 0.222 train_acc: 91.87%
INFO: 2017-09-03 00:09:53: main.py:89 **  epoch: 1 batch_num: 390 lr: 0 loss: 0.3348 acc: 90.15%  train_loss: 0.6335 train_acc: 87.79%
INFO: 2017-09-03 00:10:21: main.py:89 **  epoch: 1 batch_num: 420 lr: 0 loss: 0.2811 acc: 91.68%  train_loss: 0.3805 train_acc: 91.26%
INFO: 2017-09-03 00:10:48: main.py:89 **  epoch: 1 batch_num: 450 lr: 0 loss: 0.3464 acc: 90.42%  train_loss: 0.1204 train_acc: 96.66%
INFO: 2017-09-03 00:11:16: main.py:89 **  epoch: 1 batch_num: 480 lr: 0 loss: 0.3029 acc: 91.92%  train_loss: 0.4454 train_acc: 97.15%
INFO: 2017-09-03 00:11:42: main.py:89 **  epoch: 1 batch_num: 507 lr: 0 loss: 0.3029 acc: 91.92%  train_loss: 0.2241 train_acc: 88.9%
INFO: 2017-09-03 00:12:23: main.py:147 **  epoch: 1 valid_loss: 0.2086 valid_acc: 96.86%
INFO: 2017-09-03 00:12:25: main.py:89 **  epoch: 2 batch_num: 0 lr: 0 loss: 0.1899 acc: 92.21%  train_loss: 0.1899 train_acc: 92.21%
INFO: 2017-09-03 00:12:53: main.py:89 **  epoch: 2 batch_num: 30 lr: 0 loss: 0.2799 acc: 92.68%  train_loss: 0.1508 train_acc: 93.31%
INFO: 2017-09-03 16:35:12: utils.py:58 **  All dataset size is 5088
INFO: 2017-09-03 16:35:12: utils.py:59 **  Train dataset size is 4071
INFO: 2017-09-03 16:35:12: utils.py:61 **  Valid dataset size is 1017
INFO: 2017-09-03 16:36:50: utils.py:58 **  All dataset size is 5088
INFO: 2017-09-03 16:36:50: utils.py:59 **  Train dataset size is 4071
INFO: 2017-09-03 16:36:50: utils.py:61 **  Valid dataset size is 1017
INFO: 2017-09-03 16:36:50: main.py:206 **  All data sample counts 5088
INFO: 2017-09-03 16:36:50: main.py:207 **  Train data batch size 8
INFO: 2017-09-03 16:36:50: main.py:208 **  Train data sample counts 4064
INFO: 2017-09-03 16:36:50: main.py:209 **  Valid data sample counts 1018
INFO: 2017-09-03 16:37:45: utils.py:58 **  All dataset size is 5088
INFO: 2017-09-03 16:37:45: utils.py:59 **  Train dataset size is 4071
INFO: 2017-09-03 16:37:45: utils.py:61 **  Valid dataset size is 1017
INFO: 2017-09-03 16:37:45: main.py:206 **  All data sample counts 5088
INFO: 2017-09-03 16:37:45: main.py:207 **  Train data batch size 8
INFO: 2017-09-03 16:37:45: main.py:208 **  Train data sample counts 4064
INFO: 2017-09-03 16:37:45: main.py:209 **  Valid data sample counts 1018
INFO: 2017-09-03 16:38:14: utils.py:58 **  All dataset size is 5088
INFO: 2017-09-03 16:38:14: utils.py:59 **  Train dataset size is 4071
INFO: 2017-09-03 16:38:14: utils.py:61 **  Valid dataset size is 1017
INFO: 2017-09-03 16:38:14: main.py:206 **  All data sample counts 5088
INFO: 2017-09-03 16:38:14: main.py:207 **  Train data batch size 8
INFO: 2017-09-03 16:38:14: main.py:208 **  Train data sample counts 4064
INFO: 2017-09-03 16:38:14: main.py:209 **  Valid data sample counts 1018
INFO: 2017-09-03 16:38:38: utils.py:58 **  All dataset size is 5088
INFO: 2017-09-03 16:38:38: utils.py:59 **  Train dataset size is 4071
INFO: 2017-09-03 16:38:38: utils.py:61 **  Valid dataset size is 1017
INFO: 2017-09-03 16:38:38: main.py:206 **  All data sample counts 5088
INFO: 2017-09-03 16:38:38: main.py:207 **  Train data batch size 8
INFO: 2017-09-03 16:38:38: main.py:208 **  Train data sample counts 4064
INFO: 2017-09-03 16:38:38: main.py:209 **  Valid data sample counts 1018
INFO: 2017-09-03 17:00:02: utils.py:58 **  All dataset size is 5088
INFO: 2017-09-03 17:00:02: utils.py:59 **  Train dataset size is 4071
INFO: 2017-09-03 17:00:02: utils.py:61 **  Valid dataset size is 1017
INFO: 2017-09-03 17:00:02: main.py:195 **  Loading dataset...
INFO: 2017-09-03 17:00:02: main.py:209 **  All data sample counts 5088
INFO: 2017-09-03 17:00:02: main.py:210 **  Train data batch size 8
INFO: 2017-09-03 17:00:02: main.py:211 **  Train data sample counts 4064
INFO: 2017-09-03 17:00:02: main.py:212 **  Valid data sample counts 1018
INFO: 2017-09-03 17:00:05: main.py:119 **  epoch: 0 batch_num: 0 lr: 0 loss: 1.2685 acc: 14.28%  train_loss: 1.2685 train_acc: 14.28%
INFO: 2017-09-03 18:13:35: utils.py:58 **  All dataset size is 5088
INFO: 2017-09-03 18:13:35: utils.py:59 **  Train dataset size is 4071
INFO: 2017-09-03 18:13:35: utils.py:61 **  Valid dataset size is 1017
INFO: 2017-09-03 18:13:35: main.py:214 **  Loading dataset...
INFO: 2017-09-03 18:13:35: main.py:228 **  All data sample counts 5088
INFO: 2017-09-03 18:13:35: main.py:229 **  Train data batch size 8
INFO: 2017-09-03 18:13:35: main.py:230 **  Train data sample counts 4064
INFO: 2017-09-03 18:13:35: main.py:231 **  Valid data sample counts 1018
INFO: 2017-09-03 18:13:38: main.py:138 **  Epoch: 0	
                           Batch_num: 0	
                           lr: 0.000	
                           loss: 1.278	
                           acc: 10.600%	
                           train_loss: 1.2785	
                           train_acc: 10.6%
INFO: 2017-09-03 18:14:01: main.py:138 **  Epoch: 0	
                           Batch_num: 30	
                           lr: 0.000	
                           loss: 1.185	
                           acc: 48.220%	
                           train_loss: 1.0196	
                           train_acc: 80.0%
INFO: 2017-09-03 18:14:19: utils.py:58 **  All dataset size is 5088
INFO: 2017-09-03 18:14:19: utils.py:59 **  Train dataset size is 4071
INFO: 2017-09-03 18:14:19: utils.py:61 **  Valid dataset size is 1017
INFO: 2017-09-03 18:14:19: main.py:214 **  Loading dataset...
INFO: 2017-09-03 18:14:19: main.py:228 **  All data sample counts 5088
INFO: 2017-09-03 18:14:19: main.py:229 **  Train data batch size 8
INFO: 2017-09-03 18:14:19: main.py:230 **  Train data sample counts 4064
INFO: 2017-09-03 18:14:19: main.py:231 **  Valid data sample counts 1018
INFO: 2017-09-03 18:14:23: main.py:138 **  
Epoch: 0	
                           Batch_num: 0	
                           lr: 0.000	
                           loss: 1.403	
                           acc: 37.430%	
                           train_loss: 1.4028	
                           train_acc: 37.43%
INFO: 2017-09-03 18:14:46: main.py:138 **  
Epoch: 0	
                           Batch_num: 30	
                           lr: 0.000	
                           loss: 1.156	
                           acc: 61.800%	
                           train_loss: 1.0686	
                           train_acc: 58.62%
INFO: 2017-09-03 18:15:10: utils.py:58 **  All dataset size is 5088
INFO: 2017-09-03 18:15:10: utils.py:59 **  Train dataset size is 4071
INFO: 2017-09-03 18:15:10: utils.py:61 **  Valid dataset size is 1017
INFO: 2017-09-03 18:15:10: main.py:214 **  Loading dataset...
INFO: 2017-09-03 18:15:10: main.py:228 **  All data sample counts 5088
INFO: 2017-09-03 18:15:10: main.py:229 **  Train data batch size 8
INFO: 2017-09-03 18:15:10: main.py:230 **  Train data sample counts 4064
INFO: 2017-09-03 18:15:10: main.py:231 **  Valid data sample counts 1018
INFO: 2017-09-03 18:15:13: main.py:138 **  Epoch: 0\t
                           Batch_num: 0\t
                           lr: 0.000\t
                           loss: 1.183\t
                           acc: 66.170%\t
                           train_loss: 1.1825\t
                           train_acc: 66.17%
INFO: 2017-09-03 18:16:30: utils.py:58 **  All dataset size is 5088
INFO: 2017-09-03 18:16:30: utils.py:59 **  Train dataset size is 4071
INFO: 2017-09-03 18:16:30: utils.py:61 **  Valid dataset size is 1017
INFO: 2017-09-03 18:16:30: main.py:214 **  Loading dataset...
INFO: 2017-09-03 18:16:30: main.py:228 **  All data sample counts 5088
INFO: 2017-09-03 18:16:30: main.py:229 **  Train data batch size 8
INFO: 2017-09-03 18:16:30: main.py:230 **  Train data sample counts 4064
INFO: 2017-09-03 18:16:30: main.py:231 **  Valid data sample counts 1018
INFO: 2017-09-03 18:16:33: main.py:138 **  Epoch: 0	Batch_num: 0	lr: 0.000	loss: 1.315	acc: 33.130%	train_loss: 1.3153	train_acc: 33.13%
INFO: 2017-09-03 18:16:56: main.py:138 **  Epoch: 0	Batch_num: 30	lr: 0.000	loss: 1.233	acc: 54.760%	train_loss: 1.0521	train_acc: 76.5%
INFO: 2017-09-03 18:17:19: main.py:138 **  Epoch: 0	Batch_num: 60	lr: 0.000	loss: 0.855	acc: 77.190%	train_loss: 0.6051	train_acc: 85.47%
INFO: 2017-09-03 18:17:27: utils.py:58 **  All dataset size is 5088
INFO: 2017-09-03 18:17:27: utils.py:59 **  Train dataset size is 4071
INFO: 2017-09-03 18:17:27: utils.py:61 **  Valid dataset size is 1017
INFO: 2017-09-03 18:17:27: main.py:214 **  Loading dataset...
INFO: 2017-09-03 18:17:27: main.py:228 **  All data sample counts 5088
INFO: 2017-09-03 18:17:27: main.py:229 **  Train data batch size 8
INFO: 2017-09-03 18:17:27: main.py:230 **  Train data sample counts 4064
INFO: 2017-09-03 18:17:27: main.py:231 **  Valid data sample counts 1018
INFO: 2017-09-03 18:17:30: main.py:138 **  
Epoch: 0	Batch_num: 0	lr: 0.000	loss: 1.320	acc: 0.670%	train_loss: 1.3199	train_acc: 0.67%
INFO: 2017-09-03 18:17:54: main.py:138 **  
Epoch: 0	Batch_num: 30	lr: 0.000	loss: 1.096	acc: 50.490%	train_loss: 0.9669	train_acc: 66.76%
INFO: 2017-09-03 18:18:20: main.py:138 **  
Epoch: 0	Batch_num: 60	lr: 0.000	loss: 0.769	acc: 76.320%	train_loss: 0.8222	train_acc: 54.75%
INFO: 2017-09-03 18:18:31: utils.py:58 **  All dataset size is 5088
INFO: 2017-09-03 18:18:31: utils.py:59 **  Train dataset size is 4071
INFO: 2017-09-03 18:18:31: utils.py:61 **  Valid dataset size is 1017
INFO: 2017-09-03 18:18:31: main.py:216 **  Loading dataset...
INFO: 2017-09-03 18:18:31: main.py:230 **  All data sample counts 5088
INFO: 2017-09-03 18:18:31: main.py:231 **  Train data batch size 8
INFO: 2017-09-03 18:18:31: main.py:232 **  Train data sample counts 4064
INFO: 2017-09-03 18:18:31: main.py:233 **  Valid data sample counts 1018
INFO: 2017-09-03 18:18:34: main.py:140 **  
Train=>	Epoch: 0	Batch_num: 0	lr: 0.000	loss: 1.279	acc: 4.080%	train_loss: 1.2787	train_acc: 4.08%
INFO: 2017-09-03 18:18:59: main.py:140 **  
Train=>	Epoch: 0	Batch_num: 30	lr: 0.000	loss: 1.202	acc: 43.440%	train_loss: 0.9908	train_acc: 79.03%
INFO: 2017-09-03 18:19:25: main.py:140 **  
Train=>	Epoch: 0	Batch_num: 60	lr: 0.000	loss: 0.840	acc: 76.080%	train_loss: 0.6992	train_acc: 73.41%
INFO: 2017-09-03 18:19:53: utils.py:58 **  All dataset size is 5088
INFO: 2017-09-03 18:19:53: utils.py:59 **  Train dataset size is 4071
INFO: 2017-09-03 18:19:53: utils.py:61 **  Valid dataset size is 1017
INFO: 2017-09-03 18:19:53: main.py:215 **  Loading dataset...
INFO: 2017-09-03 18:19:53: main.py:229 **  All data sample counts 5088
INFO: 2017-09-03 18:19:53: main.py:230 **  Train data batch size 8
INFO: 2017-09-03 18:19:53: main.py:231 **  Train data sample counts 4064
INFO: 2017-09-03 18:19:53: main.py:232 **  Valid data sample counts 1018
INFO: 2017-09-03 18:19:57: main.py:139 **  Train=>
Epoch: 0	Batch_num: 0	lr: 0.000	loss: 1.182	acc: 13.320%	train_loss: 1.1819	train_acc: 13.32%
INFO: 2017-09-03 18:20:23: main.py:139 **  Train=>
Epoch: 0	Batch_num: 30	lr: 0.000	loss: 1.172	acc: 58.120%	train_loss: 1.0197	train_acc: 74.14%
INFO: 2017-09-03 18:20:49: main.py:139 **  Train=>
Epoch: 0	Batch_num: 60	lr: 0.000	loss: 0.822	acc: 74.050%	train_loss: 0.6657	train_acc: 75.4%
INFO: 2017-09-03 18:35:43: utils.py:58 **  All dataset size is 5088
INFO: 2017-09-03 18:35:43: utils.py:59 **  Train dataset size is 4071
INFO: 2017-09-03 18:35:43: utils.py:61 **  Valid dataset size is 1017
INFO: 2017-09-03 18:35:43: main.py:165 **  Loading dataset...
INFO: 2017-09-03 18:35:43: main.py:179 **  All data sample counts 5088
INFO: 2017-09-03 18:35:43: main.py:180 **  Train data batch size 8
INFO: 2017-09-03 18:35:43: main.py:181 **  Train data sample counts 4064
INFO: 2017-09-03 18:35:43: main.py:182 **  Valid data sample counts 1018
INFO: 2017-09-03 18:35:47: main.py:141 **  Train=>
Epoch: 0	Batch_num: 0	lr: 0.000	loss: 1.237	acc: 20.060%	train_loss: 1.2368	train_acc: 20.06%
INFO: 2017-09-03 18:36:11: main.py:141 **  Train=>
Epoch: 0	Batch_num: 30	lr: 0.000	loss: 1.157	acc: 60.120%	train_loss: 1.0959	train_acc: 73.9%
INFO: 2017-09-03 18:36:34: main.py:141 **  Train=>
Epoch: 0	Batch_num: 60	lr: 0.000	loss: 0.884	acc: 73.650%	train_loss: 0.8312	train_acc: 70.58%
INFO: 2017-09-03 18:36:58: main.py:141 **  Train=>
Epoch: 0	Batch_num: 90	lr: 0.000	loss: 0.587	acc: 82.400%	train_loss: 0.4065	train_acc: 90.33%
INFO: 2017-09-03 18:37:22: main.py:141 **  Train=>
Epoch: 0	Batch_num: 120	lr: 0.000	loss: 0.605	acc: 78.610%	train_loss: 0.4557	train_acc: 86.05%
INFO: 2017-09-03 18:37:46: main.py:141 **  Train=>
Epoch: 0	Batch_num: 150	lr: 0.000	loss: 0.499	acc: 80.280%	train_loss: 0.8843	train_acc: 53.59%
INFO: 2017-09-03 18:38:11: main.py:141 **  Train=>
Epoch: 0	Batch_num: 180	lr: 0.000	loss: 0.496	acc: 81.210%	train_loss: 0.2488	train_acc: 93.83%
INFO: 2017-09-03 18:38:37: main.py:141 **  Train=>
Epoch: 0	Batch_num: 210	lr: 0.000	loss: 0.454	acc: 81.400%	train_loss: 0.3421	train_acc: 87.8%
INFO: 2017-09-03 18:39:03: main.py:141 **  Train=>
Epoch: 0	Batch_num: 240	lr: 0.000	loss: 0.415	acc: 85.320%	train_loss: 0.2313	train_acc: 93.09%
INFO: 2017-09-03 18:39:29: main.py:141 **  Train=>
Epoch: 0	Batch_num: 270	lr: 0.000	loss: 0.448	acc: 84.490%	train_loss: 0.2764	train_acc: 91.05%
INFO: 2017-09-03 18:39:55: main.py:141 **  Train=>
Epoch: 0	Batch_num: 300	lr: 0.000	loss: 0.338	acc: 88.550%	train_loss: 0.228	train_acc: 89.43%
INFO: 2017-09-03 18:40:20: main.py:141 **  Train=>
Epoch: 0	Batch_num: 330	lr: 0.000	loss: 0.318	acc: 91.760%	train_loss: 0.2666	train_acc: 93.28%
INFO: 2017-09-03 18:40:46: main.py:141 **  Train=>
Epoch: 0	Batch_num: 360	lr: 0.000	loss: 0.369	acc: 88.990%	train_loss: 0.4416	train_acc: 86.14%
INFO: 2017-09-03 18:41:12: main.py:141 **  Train=>
Epoch: 0	Batch_num: 390	lr: 0.000	loss: 0.385	acc: 85.480%	train_loss: 0.155	train_acc: 95.44%
INFO: 2017-09-03 18:41:38: main.py:141 **  Train=>
Epoch: 0	Batch_num: 420	lr: 0.000	loss: 0.293	acc: 90.490%	train_loss: 0.3173	train_acc: 95.04%
INFO: 2017-09-03 18:42:04: main.py:141 **  Train=>
Epoch: 0	Batch_num: 450	lr: 0.000	loss: 0.320	acc: 90.940%	train_loss: 0.3695	train_acc: 83.29%
INFO: 2017-09-03 18:42:30: main.py:141 **  Train=>
Epoch: 0	Batch_num: 480	lr: 0.000	loss: 0.327	acc: 91.000%	train_loss: 0.1606	train_acc: 95.37%
INFO: 2017-09-03 18:42:53: main.py:141 **  Train=>
Epoch: 0	Batch_num: 507	lr: 0.000	loss: 0.327	acc: 91.000%	train_loss: 0.3373	train_acc: 87.25%
INFO: 2017-09-03 18:43:31: main.py:80 **  Validate=>
Epoch: 0	Valid_loss: 0.424	Valid_acc: 84.420%
INFO: 2017-09-03 18:50:48: utils.py:58 **  All dataset size is 5088
INFO: 2017-09-03 18:50:48: utils.py:59 **  Train dataset size is 4071
INFO: 2017-09-03 18:50:48: utils.py:61 **  Valid dataset size is 1017
INFO: 2017-09-03 18:50:48: main.py:166 **  Loading dataset...
INFO: 2017-09-03 18:50:48: main.py:180 **  All data sample counts 5088
INFO: 2017-09-03 18:50:48: main.py:181 **  Train data batch size 8
INFO: 2017-09-03 18:50:48: main.py:182 **  Train data sample counts 4064
INFO: 2017-09-03 18:50:48: main.py:183 **  Valid data sample counts 1018
INFO: 2017-09-03 18:50:51: main.py:142 **  Train=>
Epoch: 0	Batch_num: 0	lr: 0.000	loss: 1.333	acc: 0.000%	train_loss: 1.3332	train_acc: 0.0%
INFO: 2017-09-03 18:51:14: main.py:142 **  Train=>
Epoch: 0	Batch_num: 30	lr: 0.000	loss: 1.218	acc: 31.210%	train_loss: 0.9843	train_acc: 83.49%
INFO: 2017-09-03 18:51:37: main.py:142 **  Train=>
Epoch: 0	Batch_num: 60	lr: 0.000	loss: 0.871	acc: 74.520%	train_loss: 0.6525	train_acc: 79.77%
INFO: 2017-09-03 18:52:01: main.py:142 **  Train=>
Epoch: 0	Batch_num: 90	lr: 0.000	loss: 0.671	acc: 77.520%	train_loss: 0.5888	train_acc: 83.47%
INFO: 2017-09-03 18:52:25: main.py:142 **  Train=>
Epoch: 0	Batch_num: 120	lr: 0.000	loss: 0.535	acc: 83.270%	train_loss: 0.4434	train_acc: 83.09%
INFO: 2017-09-03 18:52:50: main.py:142 **  Train=>
Epoch: 0	Batch_num: 150	lr: 0.000	loss: 0.468	acc: 86.390%	train_loss: 0.3363	train_acc: 106.88%
INFO: 2017-09-03 18:53:15: main.py:142 **  Train=>
Epoch: 0	Batch_num: 180	lr: 0.000	loss: 0.517	acc: 82.740%	train_loss: 0.4229	train_acc: 79.69%
INFO: 2017-09-03 18:53:41: main.py:142 **  Train=>
Epoch: 0	Batch_num: 210	lr: 0.000	loss: 0.424	acc: 86.940%	train_loss: 0.5025	train_acc: 97.17%
INFO: 2017-09-03 18:54:07: main.py:142 **  Train=>
Epoch: 0	Batch_num: 240	lr: 0.000	loss: 0.450	acc: 86.350%	train_loss: 0.242	train_acc: 89.41%
INFO: 2017-09-03 18:54:33: main.py:142 **  Train=>
Epoch: 0	Batch_num: 270	lr: 0.000	loss: 0.434	acc: 83.450%	train_loss: 0.3136	train_acc: 84.51%
INFO: 2017-09-03 18:54:59: main.py:142 **  Train=>
Epoch: 0	Batch_num: 300	lr: 0.000	loss: 0.402	acc: 85.720%	train_loss: 0.2236	train_acc: 92.92%
INFO: 2017-09-03 18:55:24: main.py:142 **  Train=>
Epoch: 0	Batch_num: 330	lr: 0.000	loss: 0.490	acc: 84.050%	train_loss: 0.3512	train_acc: 78.11%
INFO: 2017-09-03 18:55:50: main.py:142 **  Train=>
Epoch: 0	Batch_num: 360	lr: 0.000	loss: 0.412	acc: 83.530%	train_loss: 0.3734	train_acc: 78.78%
INFO: 2017-09-03 18:56:16: main.py:142 **  Train=>
Epoch: 0	Batch_num: 390	lr: 0.000	loss: 0.349	acc: 88.410%	train_loss: 0.2211	train_acc: 94.98%
INFO: 2017-09-03 18:56:42: main.py:142 **  Train=>
Epoch: 0	Batch_num: 420	lr: 0.000	loss: 0.407	acc: 88.080%	train_loss: 0.4185	train_acc: 71.44%
INFO: 2017-09-03 18:57:08: main.py:142 **  Train=>
Epoch: 0	Batch_num: 450	lr: 0.000	loss: 0.289	acc: 92.100%	train_loss: 0.2258	train_acc: 91.19%
INFO: 2017-09-03 18:57:34: main.py:142 **  Train=>
Epoch: 0	Batch_num: 480	lr: 0.000	loss: 0.333	acc: 91.230%	train_loss: 0.4893	train_acc: 93.75%
INFO: 2017-09-03 18:57:58: main.py:142 **  Train=>
Epoch: 0	Batch_num: 507	lr: 0.000	loss: 0.333	acc: 91.230%	train_loss: 0.2319	train_acc: 94.39%
INFO: 2017-09-03 18:58:35: main.py:81 **  Validate=>
Epoch: 0	Valid_loss: 0.299	Valid_acc: 90.420%
INFO: 2017-09-03 18:58:37: main.py:142 **  Train=>
Epoch: 1	Batch_num: 0	lr: 0.000	loss: 0.180	acc: 95.950%	train_loss: 0.1804	train_acc: 95.95%
INFO: 2017-09-03 18:59:02: main.py:142 **  Train=>
Epoch: 1	Batch_num: 30	lr: 0.000	loss: 1.029	acc: 63.990%	train_loss: 1.2726	train_acc: 0.0%
INFO: 2017-09-03 18:59:26: main.py:142 **  Train=>
Epoch: 1	Batch_num: 60	lr: 0.000	loss: 1.324	acc: 29.240%	train_loss: 1.344	train_acc: 51.36%
INFO: 2017-09-03 18:59:50: main.py:142 **  Train=>
Epoch: 1	Batch_num: 90	lr: 0.000	loss: 1.324	acc: 27.390%	train_loss: 1.3257	train_acc: 8.35%
INFO: 2017-09-03 21:17:54: utils.py:58 **  All dataset size is 5088
INFO: 2017-09-03 21:17:54: utils.py:59 **  Train dataset size is 4071
INFO: 2017-09-03 21:17:54: utils.py:61 **  Valid dataset size is 1017
INFO: 2017-09-03 21:17:54: main.py:166 **  Loading dataset...
INFO: 2017-09-03 21:17:54: main.py:180 **  All data sample counts 5088
INFO: 2017-09-03 21:17:54: main.py:181 **  Train data batch size 8
INFO: 2017-09-03 21:17:54: main.py:182 **  Train data sample counts 4064
INFO: 2017-09-03 21:17:54: main.py:183 **  Valid data sample counts 1018
INFO: 2017-09-03 21:17:57: main.py:142 **  Train=>
Epoch: 0	Batch_num: 0	lr: 0.000	loss: 1.201	acc: 63.760%	train_loss: 1.2008	train_acc: 63.76%
INFO: 2017-09-03 21:18:20: main.py:142 **  Train=>
Epoch: 0	Batch_num: 30	lr: 0.000	loss: 1.139	acc: 61.150%	train_loss: 1.2083	train_acc: 49.94%
INFO: 2017-09-03 21:18:43: main.py:142 **  Train=>
Epoch: 0	Batch_num: 60	lr: 0.000	loss: 0.784	acc: 74.470%	train_loss: 0.6389	train_acc: 72.64%
INFO: 2017-09-03 21:19:06: main.py:142 **  Train=>
Epoch: 0	Batch_num: 90	lr: 0.000	loss: 0.644	acc: 79.690%	train_loss: 0.4292	train_acc: 88.18%
INFO: 2017-09-03 21:19:29: main.py:142 **  Train=>
Epoch: 0	Batch_num: 120	lr: 0.000	loss: 0.570	acc: 81.280%	train_loss: 0.5882	train_acc: 73.62%
INFO: 2017-09-03 21:19:53: main.py:142 **  Train=>
Epoch: 0	Batch_num: 150	lr: 0.000	loss: 0.519	acc: 85.110%	train_loss: 0.7308	train_acc: 78.22%
INFO: 2017-09-03 21:20:18: main.py:142 **  Train=>
Epoch: 0	Batch_num: 180	lr: 0.000	loss: 0.468	acc: 84.480%	train_loss: 0.2141	train_acc: 94.97%
INFO: 2017-09-03 21:20:44: main.py:142 **  Train=>
Epoch: 0	Batch_num: 210	lr: 0.000	loss: 0.480	acc: 85.710%	train_loss: 0.4362	train_acc: 104.4%
INFO: 2017-09-03 21:21:10: main.py:142 **  Train=>
Epoch: 0	Batch_num: 240	lr: 0.000	loss: 0.410	acc: 85.940%	train_loss: 0.2568	train_acc: 93.01%
INFO: 2017-09-03 21:21:36: main.py:142 **  Train=>
Epoch: 0	Batch_num: 270	lr: 0.000	loss: 0.442	acc: 85.490%	train_loss: 1.068	train_acc: 63.86%
INFO: 2017-09-03 21:22:02: main.py:142 **  Train=>
Epoch: 0	Batch_num: 300	lr: 0.000	loss: 0.387	acc: 88.140%	train_loss: 0.4071	train_acc: 83.21%
INFO: 2017-09-03 21:22:28: main.py:142 **  Train=>
Epoch: 0	Batch_num: 330	lr: 0.000	loss: 0.401	acc: 86.170%	train_loss: 0.1596	train_acc: 96.84%
INFO: 2017-09-03 21:22:54: main.py:142 **  Train=>
Epoch: 0	Batch_num: 360	lr: 0.000	loss: 0.417	acc: 85.070%	train_loss: 0.5572	train_acc: 59.03%
INFO: 2017-09-03 21:23:20: main.py:142 **  Train=>
Epoch: 0	Batch_num: 390	lr: 0.000	loss: 0.337	acc: 89.510%	train_loss: 0.2172	train_acc: 93.37%
INFO: 2017-09-03 21:23:46: main.py:142 **  Train=>
Epoch: 0	Batch_num: 420	lr: 0.000	loss: 0.357	acc: 89.190%	train_loss: 0.2101	train_acc: 92.28%
INFO: 2017-09-03 21:24:12: main.py:142 **  Train=>
Epoch: 0	Batch_num: 450	lr: 0.000	loss: 0.310	acc: 90.040%	train_loss: 0.1875	train_acc: 94.03%
INFO: 2017-09-03 21:24:38: main.py:142 **  Train=>
Epoch: 0	Batch_num: 480	lr: 0.000	loss: 0.375	acc: 88.630%	train_loss: 0.4156	train_acc: 83.42%
INFO: 2017-09-03 21:25:02: main.py:142 **  Train=>
Epoch: 0	Batch_num: 507	lr: 0.000	loss: 0.375	acc: 88.630%	train_loss: 0.3939	train_acc: 103.68%
INFO: 2017-09-03 21:25:39: main.py:81 **  Validate=>
Epoch: 0	Valid_loss: 0.358	Valid_acc: 92.240%
INFO: 2017-09-03 21:25:41: main.py:142 **  Train=>
Epoch: 1	Batch_num: 0	lr: 0.000	loss: 0.383	acc: 88.630%	train_loss: 0.3834	train_acc: 88.63%
INFO: 2017-09-03 21:26:06: main.py:142 **  Train=>
Epoch: 1	Batch_num: 30	lr: 0.000	loss: 1.183	acc: 66.590%	train_loss: 1.3535	train_acc: 50.01%
INFO: 2017-09-03 21:26:32: main.py:142 **  Train=>
Epoch: 1	Batch_num: 60	lr: 0.000	loss: 1.306	acc: 8.210%	train_loss: 1.2639	train_acc: 0.1%
INFO: 2017-09-03 21:26:57: main.py:142 **  Train=>
Epoch: 1	Batch_num: 90	lr: 0.000	loss: 8.656	acc: 6.670%	train_loss: 10.9429	train_acc: 0.0%
INFO: 2017-09-03 21:27:21: main.py:142 **  Train=>
Epoch: 1	Batch_num: 120	lr: 0.000	loss: 12.173	acc: 6.710%	train_loss: 15.7301	train_acc: 0.0%
INFO: 2017-09-03 21:27:45: main.py:142 **  Train=>
Epoch: 1	Batch_num: 150	lr: 0.000	loss: 11.924	acc: 7.520%	train_loss: 11.9552	train_acc: 25.0%
INFO: 2017-09-03 21:28:09: main.py:142 **  Train=>
Epoch: 1	Batch_num: 180	lr: 0.000	loss: 11.671	acc: 5.850%	train_loss: 10.4256	train_acc: 25.0%
INFO: 2017-09-03 21:28:33: main.py:142 **  Train=>
Epoch: 1	Batch_num: 210	lr: 0.000	loss: 11.053	acc: 12.520%	train_loss: 8.522	train_acc: 25.01%
INFO: 2017-09-03 21:28:57: main.py:142 **  Train=>
Epoch: 1	Batch_num: 240	lr: 0.000	loss: 11.853	acc: 7.510%	train_loss: 11.7447	train_acc: 0.0%
INFO: 2017-09-03 21:29:22: main.py:142 **  Train=>
Epoch: 1	Batch_num: 270	lr: 0.000	loss: 11.623	acc: 7.510%	train_loss: 13.3975	train_acc: 0.0%
INFO: 2017-09-03 21:29:46: main.py:142 **  Train=>
Epoch: 1	Batch_num: 300	lr: 0.000	loss: 12.473	acc: 8.340%	train_loss: 12.8213	train_acc: 25.01%
INFO: 2017-09-03 21:30:42: utils.py:58 **  All dataset size is 5088
INFO: 2017-09-03 21:30:42: utils.py:59 **  Train dataset size is 4071
INFO: 2017-09-03 21:30:42: utils.py:61 **  Valid dataset size is 1017
INFO: 2017-09-03 21:30:42: main.py:166 **  Loading dataset...
INFO: 2017-09-03 21:30:42: main.py:180 **  All data sample counts 5088
INFO: 2017-09-03 21:30:42: main.py:181 **  Train data batch size 8
INFO: 2017-09-03 21:30:42: main.py:182 **  Train data sample counts 4064
INFO: 2017-09-03 21:30:42: main.py:183 **  Valid data sample counts 1018
INFO: 2017-09-03 21:30:45: main.py:142 **  Train=>
Epoch: 0	Batch_num: 0	lr: 0.000	loss: 1.327	acc: 18.090%	train_loss: 1.3274	train_acc: 18.09%
INFO: 2017-09-03 21:31:09: main.py:142 **  Train=>
Epoch: 0	Batch_num: 30	lr: 0.000	loss: 1.151	acc: 54.870%	train_loss: 0.9885	train_acc: 66.51%
INFO: 2017-09-03 21:31:34: main.py:142 **  Train=>
Epoch: 0	Batch_num: 60	lr: 0.000	loss: 0.808	acc: 79.510%	train_loss: 0.6292	train_acc: 84.68%
INFO: 2017-09-03 21:32:01: main.py:142 **  Train=>
Epoch: 0	Batch_num: 90	lr: 0.000	loss: 0.639	acc: 79.340%	train_loss: 0.6034	train_acc: 82.19%
INFO: 2017-09-03 21:32:28: main.py:142 **  Train=>
Epoch: 0	Batch_num: 120	lr: 0.000	loss: 0.557	acc: 80.260%	train_loss: 0.6722	train_acc: 79.79%
INFO: 2017-09-03 21:32:55: main.py:142 **  Train=>
Epoch: 0	Batch_num: 150	lr: 0.000	loss: 0.522	acc: 82.940%	train_loss: 0.4685	train_acc: 84.24%
INFO: 2017-09-03 21:33:21: main.py:142 **  Train=>
Epoch: 0	Batch_num: 180	lr: 0.000	loss: 0.423	acc: 86.670%	train_loss: 0.3896	train_acc: 87.05%
INFO: 2017-09-03 21:33:48: main.py:142 **  Train=>
Epoch: 0	Batch_num: 210	lr: 0.000	loss: 0.471	acc: 84.120%	train_loss: 0.8369	train_acc: 63.87%
INFO: 2017-09-03 21:34:14: main.py:142 **  Train=>
Epoch: 0	Batch_num: 240	lr: 0.000	loss: 0.443	acc: 86.180%	train_loss: 0.2857	train_acc: 92.98%
INFO: 2017-09-03 21:34:40: main.py:142 **  Train=>
Epoch: 0	Batch_num: 270	lr: 0.000	loss: 0.442	acc: 86.110%	train_loss: 0.3685	train_acc: 88.09%
INFO: 2017-09-03 21:35:06: main.py:142 **  Train=>
Epoch: 0	Batch_num: 300	lr: 0.000	loss: 0.485	acc: 83.530%	train_loss: 0.4645	train_acc: 94.25%
INFO: 2017-09-03 21:35:32: main.py:142 **  Train=>
Epoch: 0	Batch_num: 330	lr: 0.000	loss: 0.370	acc: 89.760%	train_loss: 0.2755	train_acc: 108.99%
INFO: 2017-09-03 21:35:58: main.py:142 **  Train=>
Epoch: 0	Batch_num: 360	lr: 0.000	loss: 0.514	acc: 83.570%	train_loss: 0.5531	train_acc: 85.54%
INFO: 2017-09-03 21:36:24: main.py:142 **  Train=>
Epoch: 0	Batch_num: 390	lr: 0.000	loss: 0.363	acc: 88.590%	train_loss: 0.2788	train_acc: 109.45%
INFO: 2017-09-03 21:36:51: main.py:142 **  Train=>
Epoch: 0	Batch_num: 420	lr: 0.000	loss: 0.361	acc: 87.450%	train_loss: 0.7743	train_acc: 75.63%
INFO: 2017-09-03 21:37:18: main.py:142 **  Train=>
Epoch: 0	Batch_num: 450	lr: 0.000	loss: 0.346	acc: 90.210%	train_loss: 0.2528	train_acc: 83.77%
INFO: 2017-09-03 21:37:44: main.py:142 **  Train=>
Epoch: 0	Batch_num: 480	lr: 0.000	loss: 0.352	acc: 90.580%	train_loss: 0.2767	train_acc: 92.88%
INFO: 2017-09-03 21:38:08: main.py:142 **  Train=>
Epoch: 0	Batch_num: 507	lr: 0.000	loss: 0.352	acc: 90.580%	train_loss: 0.1515	train_acc: 95.05%
INFO: 2017-09-03 21:38:59: main.py:81 **  Validate=>
Epoch: 0	Valid_loss: 1.108	Valid_acc: 71.700%
INFO: 2017-09-03 21:39:00: main.py:142 **  Train=>
Epoch: 1	Batch_num: 0	lr: 0.000	loss: 0.848	acc: 86.700%	train_loss: 0.8477	train_acc: 86.7%
INFO: 2017-09-03 21:39:27: main.py:142 **  Train=>
Epoch: 1	Batch_num: 30	lr: 0.000	loss: 0.342	acc: 89.260%	train_loss: 0.5523	train_acc: 72.11%
INFO: 2017-09-03 21:39:53: main.py:142 **  Train=>
Epoch: 1	Batch_num: 60	lr: 0.000	loss: 0.321	acc: 89.700%	train_loss: 0.3648	train_acc: 87.58%
INFO: 2017-09-03 21:40:20: main.py:142 **  Train=>
Epoch: 1	Batch_num: 90	lr: 0.000	loss: 0.326	acc: 89.210%	train_loss: 0.2578	train_acc: 92.0%
INFO: 2017-09-03 21:40:47: main.py:142 **  Train=>
Epoch: 1	Batch_num: 120	lr: 0.000	loss: 0.315	acc: 91.610%	train_loss: 0.4685	train_acc: 95.91%
INFO: 2017-09-03 21:41:14: main.py:142 **  Train=>
Epoch: 1	Batch_num: 150	lr: 0.000	loss: 0.282	acc: 92.670%	train_loss: 0.1408	train_acc: 97.27%
INFO: 2017-09-03 21:41:40: main.py:142 **  Train=>
Epoch: 1	Batch_num: 180	lr: 0.000	loss: 0.244	acc: 93.800%	train_loss: 0.2872	train_acc: 109.86%
INFO: 2017-09-03 21:42:07: main.py:142 **  Train=>
Epoch: 1	Batch_num: 210	lr: 0.000	loss: 0.286	acc: 96.920%	train_loss: 0.2675	train_acc: 108.59%
INFO: 2017-09-03 21:42:33: main.py:142 **  Train=>
Epoch: 1	Batch_num: 240	lr: 0.000	loss: 0.317	acc: 92.570%	train_loss: 0.3512	train_acc: 106.24%
INFO: 2017-09-03 21:43:00: main.py:142 **  Train=>
Epoch: 1	Batch_num: 270	lr: 0.000	loss: 0.276	acc: 93.130%	train_loss: 0.1826	train_acc: 95.59%
INFO: 2017-09-03 21:43:28: main.py:142 **  Train=>
Epoch: 1	Batch_num: 300	lr: 0.000	loss: 0.250	acc: 93.030%	train_loss: 0.3902	train_acc: 89.67%
INFO: 2017-09-03 21:43:55: main.py:142 **  Train=>
Epoch: 1	Batch_num: 330	lr: 0.000	loss: 0.270	acc: 92.400%	train_loss: 0.3543	train_acc: 81.5%
INFO: 2017-09-03 21:44:22: main.py:142 **  Train=>
Epoch: 1	Batch_num: 360	lr: 0.000	loss: 0.219	acc: 97.260%	train_loss: 0.1298	train_acc: 96.25%
INFO: 2017-09-03 21:44:48: main.py:142 **  Train=>
Epoch: 1	Batch_num: 390	lr: 0.000	loss: 0.244	acc: 95.300%	train_loss: 0.1691	train_acc: 95.55%
INFO: 2017-09-03 21:45:16: main.py:142 **  Train=>
Epoch: 1	Batch_num: 420	lr: 0.000	loss: 0.207	acc: 96.190%	train_loss: 0.095	train_acc: 97.36%
INFO: 2017-09-03 21:45:43: main.py:142 **  Train=>
Epoch: 1	Batch_num: 450	lr: 0.000	loss: 0.245	acc: 94.790%	train_loss: 0.1566	train_acc: 95.6%
INFO: 2017-09-03 21:46:10: main.py:142 **  Train=>
Epoch: 1	Batch_num: 480	lr: 0.000	loss: 0.226	acc: 94.570%	train_loss: 0.125	train_acc: 96.07%
INFO: 2017-09-03 21:46:36: main.py:142 **  Train=>
Epoch: 1	Batch_num: 507	lr: 0.000	loss: 0.226	acc: 94.570%	train_loss: 0.2237	train_acc: 84.62%
INFO: 2017-09-03 21:47:27: main.py:81 **  Validate=>
Epoch: 1	Valid_loss: 0.944	Valid_acc: 77.970%
INFO: 2017-09-03 21:47:28: main.py:142 **  Train=>
Epoch: 2	Batch_num: 0	lr: 0.000	loss: 0.296	acc: 94.180%	train_loss: 0.2962	train_acc: 94.18%
INFO: 2017-09-03 21:47:55: main.py:142 **  Train=>
Epoch: 2	Batch_num: 30	lr: 0.000	loss: 0.188	acc: 98.370%	train_loss: 0.1269	train_acc: 97.49%
INFO: 2017-09-03 21:48:22: main.py:142 **  Train=>
Epoch: 2	Batch_num: 60	lr: 0.000	loss: 0.158	acc: 96.280%	train_loss: 0.1038	train_acc: 97.96%
INFO: 2017-09-03 21:48:48: main.py:142 **  Train=>
Epoch: 2	Batch_num: 90	lr: 0.000	loss: 0.227	acc: 98.190%	train_loss: 0.2849	train_acc: 109.15%
INFO: 2017-09-03 21:49:14: main.py:142 **  Train=>
Epoch: 2	Batch_num: 120	lr: 0.000	loss: 0.176	acc: 98.010%	train_loss: 0.0786	train_acc: 97.37%
INFO: 2017-09-03 21:49:40: main.py:142 **  Train=>
Epoch: 2	Batch_num: 150	lr: 0.000	loss: 0.136	acc: 97.950%	train_loss: 0.2004	train_acc: 92.33%
INFO: 2017-09-03 21:50:06: main.py:142 **  Train=>
Epoch: 2	Batch_num: 180	lr: 0.000	loss: 0.197	acc: 97.440%	train_loss: 0.2083	train_acc: 110.52%
INFO: 2017-09-03 21:50:32: main.py:142 **  Train=>
Epoch: 2	Batch_num: 210	lr: 0.000	loss: 0.199	acc: 97.850%	train_loss: 0.2802	train_acc: 104.43%
INFO: 2017-09-03 21:50:58: main.py:142 **  Train=>
Epoch: 2	Batch_num: 240	lr: 0.000	loss: 0.171	acc: 95.310%	train_loss: 0.2286	train_acc: 84.67%
INFO: 2017-09-03 21:51:26: main.py:142 **  Train=>
Epoch: 2	Batch_num: 270	lr: 0.000	loss: 0.185	acc: 97.020%	train_loss: 0.1319	train_acc: 93.4%
INFO: 2017-09-03 21:51:52: main.py:142 **  Train=>
Epoch: 2	Batch_num: 300	lr: 0.000	loss: 0.154	acc: 98.740%	train_loss: 0.1228	train_acc: 92.78%
INFO: 2017-09-03 21:52:44: utils.py:58 **  All dataset size is 5088
INFO: 2017-09-03 21:52:44: utils.py:59 **  Train dataset size is 4071
INFO: 2017-09-03 21:52:44: utils.py:61 **  Valid dataset size is 1017
INFO: 2017-09-03 21:52:44: main.py:166 **  Loading dataset...
INFO: 2017-09-03 21:52:44: main.py:180 **  All data sample counts 5088
INFO: 2017-09-03 21:52:44: main.py:181 **  Train data batch size 8
INFO: 2017-09-03 21:52:44: main.py:182 **  Train data sample counts 4064
INFO: 2017-09-03 21:52:44: main.py:183 **  Valid data sample counts 1018
INFO: 2017-09-03 21:52:47: main.py:142 **  Train=>
Epoch: 0	Batch_num: 0	lr: 0.000	loss: 1.325	acc: 46.110%	train_loss: 1.3254	train_acc: 46.11%
INFO: 2017-09-06 21:56:51: utils.py:58 **  All dataset size is 5088
INFO: 2017-09-06 21:56:51: utils.py:59 **  Train dataset size is 4071
INFO: 2017-09-06 21:56:51: utils.py:61 **  Valid dataset size is 1017
INFO: 2017-09-06 21:56:51: main.py:166 **  Loading dataset...
INFO: 2017-09-06 21:56:51: main.py:180 **  All data sample counts 5088
INFO: 2017-09-06 21:56:51: main.py:181 **  Train data batch size 8
INFO: 2017-09-06 21:56:51: main.py:182 **  Train data sample counts 4064
INFO: 2017-09-06 21:56:51: main.py:183 **  Valid data sample counts 1018
INFO: 2017-09-06 21:56:55: main.py:142 **  Train=>
Epoch: 0	Batch_num: 0	lr: 0.000	loss: 1.271	acc: 3.070%	train_loss: 1.2711	train_acc: 3.07%
INFO: 2017-09-06 21:57:18: main.py:142 **  Train=>
Epoch: 0	Batch_num: 30	lr: 0.000	loss: 1.247	acc: 36.010%	train_loss: 1.1364	train_acc: 57.07%
INFO: 2017-09-06 21:57:41: main.py:142 **  Train=>
Epoch: 0	Batch_num: 60	lr: 0.000	loss: 0.903	acc: 71.940%	train_loss: 0.6183	train_acc: 84.95%
INFO: 2017-09-06 21:58:05: main.py:142 **  Train=>
Epoch: 0	Batch_num: 90	lr: 0.000	loss: 0.665	acc: 78.460%	train_loss: 0.9519	train_acc: 53.64%
INFO: 2017-09-06 21:58:28: main.py:142 **  Train=>
Epoch: 0	Batch_num: 120	lr: 0.000	loss: 0.559	acc: 79.870%	train_loss: 0.5475	train_acc: 75.96%
INFO: 2017-09-06 21:58:52: main.py:142 **  Train=>
Epoch: 0	Batch_num: 150	lr: 0.000	loss: 0.447	acc: 85.020%	train_loss: 0.3784	train_acc: 89.23%
INFO: 2017-09-06 21:59:17: main.py:142 **  Train=>
Epoch: 0	Batch_num: 180	lr: 0.000	loss: 0.491	acc: 85.460%	train_loss: 0.3579	train_acc: 101.79%
INFO: 2017-09-06 21:59:42: main.py:142 **  Train=>
Epoch: 0	Batch_num: 210	lr: 0.000	loss: 0.571	acc: 82.490%	train_loss: 0.7198	train_acc: 77.4%
INFO: 2017-09-06 22:00:07: main.py:142 **  Train=>
Epoch: 0	Batch_num: 240	lr: 0.000	loss: 0.468	acc: 85.560%	train_loss: 0.4321	train_acc: 96.79%
INFO: 2017-09-06 22:00:32: main.py:142 **  Train=>
Epoch: 0	Batch_num: 270	lr: 0.000	loss: 0.411	acc: 87.530%	train_loss: 0.4131	train_acc: 87.33%
INFO: 2017-09-06 22:00:58: main.py:142 **  Train=>
Epoch: 0	Batch_num: 300	lr: 0.000	loss: 0.414	acc: 86.490%	train_loss: 0.5247	train_acc: 73.51%
INFO: 2017-09-06 22:01:25: main.py:142 **  Train=>
Epoch: 0	Batch_num: 330	lr: 0.000	loss: 0.371	acc: 88.350%	train_loss: 0.1817	train_acc: 95.29%
INFO: 2017-09-06 22:01:52: main.py:142 **  Train=>
Epoch: 0	Batch_num: 360	lr: 0.000	loss: 0.372	acc: 85.760%	train_loss: 0.6066	train_acc: 67.39%
INFO: 2017-09-06 22:02:19: main.py:142 **  Train=>
Epoch: 0	Batch_num: 390	lr: 0.000	loss: 0.356	acc: 88.920%	train_loss: 0.2423	train_acc: 94.22%
INFO: 2017-09-06 22:02:47: main.py:142 **  Train=>
Epoch: 0	Batch_num: 420	lr: 0.000	loss: 0.359	acc: 86.900%	train_loss: 0.2295	train_acc: 92.11%
INFO: 2017-09-06 22:03:14: main.py:142 **  Train=>
Epoch: 0	Batch_num: 450	lr: 0.000	loss: 0.263	acc: 90.890%	train_loss: 0.4236	train_acc: 90.37%
INFO: 2017-09-06 22:03:41: main.py:142 **  Train=>
Epoch: 0	Batch_num: 480	lr: 0.000	loss: 0.363	acc: 88.030%	train_loss: 0.4151	train_acc: 85.87%
INFO: 2017-09-06 22:04:06: main.py:142 **  Train=>
Epoch: 0	Batch_num: 507	lr: 0.000	loss: 0.363	acc: 88.030%	train_loss: 0.413	train_acc: 121.21%
INFO: 2017-09-06 22:04:44: main.py:81 **  Validate=>
Epoch: 0	Valid_loss: 0.331	Valid_acc: 92.210%
INFO: 2017-09-06 22:04:48: main.py:142 **  Train=>
Epoch: 1	Batch_num: 0	lr: 0.000	loss: 0.677	acc: 87.730%	train_loss: 0.6774	train_acc: 87.73%
INFO: 2017-09-06 22:05:15: main.py:142 **  Train=>
Epoch: 1	Batch_num: 30	lr: 0.000	loss: 0.285	acc: 92.080%	train_loss: 0.1325	train_acc: 97.23%
INFO: 2017-09-06 22:05:43: main.py:142 **  Train=>
Epoch: 1	Batch_num: 60	lr: 0.000	loss: 0.214	acc: 95.750%	train_loss: 0.3151	train_acc: 81.52%
INFO: 2017-09-06 22:06:11: main.py:142 **  Train=>
Epoch: 1	Batch_num: 90	lr: 0.000	loss: 0.218	acc: 94.500%	train_loss: 0.1504	train_acc: 96.12%
INFO: 2017-09-06 22:06:38: main.py:142 **  Train=>
Epoch: 1	Batch_num: 120	lr: 0.000	loss: 0.238	acc: 100.700%	train_loss: 0.1041	train_acc: 96.41%
INFO: 2017-09-06 22:07:06: main.py:142 **  Train=>
Epoch: 1	Batch_num: 150	lr: 0.000	loss: 0.205	acc: 97.310%	train_loss: 0.1243	train_acc: 95.97%
INFO: 2017-09-06 22:07:33: main.py:142 **  Train=>
Epoch: 1	Batch_num: 180	lr: 0.000	loss: 0.230	acc: 94.720%	train_loss: 0.1917	train_acc: 94.14%
INFO: 2017-09-06 22:08:00: main.py:142 **  Train=>
Epoch: 1	Batch_num: 210	lr: 0.000	loss: 0.270	acc: 92.860%	train_loss: 0.2655	train_acc: 109.02%
INFO: 2017-09-06 22:08:27: main.py:142 **  Train=>
Epoch: 1	Batch_num: 240	lr: 0.000	loss: 0.303	acc: 94.380%	train_loss: 0.6396	train_acc: 101.49%
INFO: 2017-09-06 22:08:55: main.py:142 **  Train=>
Epoch: 1	Batch_num: 270	lr: 0.000	loss: 0.322	acc: 91.390%	train_loss: 0.2606	train_acc: 94.48%
INFO: 2017-09-06 22:09:22: main.py:142 **  Train=>
Epoch: 1	Batch_num: 300	lr: 0.000	loss: 0.319	acc: 92.840%	train_loss: 0.1444	train_acc: 94.89%
INFO: 2017-09-06 22:09:49: main.py:142 **  Train=>
Epoch: 1	Batch_num: 330	lr: 0.000	loss: 0.218	acc: 98.570%	train_loss: 0.1413	train_acc: 94.99%
INFO: 2017-09-06 22:10:17: main.py:142 **  Train=>
Epoch: 1	Batch_num: 360	lr: 0.000	loss: 0.246	acc: 93.110%	train_loss: 0.2363	train_acc: 84.22%
INFO: 2017-09-06 22:10:44: main.py:142 **  Train=>
Epoch: 1	Batch_num: 390	lr: 0.000	loss: 0.225	acc: 94.520%	train_loss: 0.1881	train_acc: 94.28%
INFO: 2017-09-06 22:11:12: main.py:142 **  Train=>
Epoch: 1	Batch_num: 420	lr: 0.000	loss: 0.181	acc: 94.600%	train_loss: 0.1794	train_acc: 95.09%
INFO: 2017-09-06 22:11:40: main.py:142 **  Train=>
Epoch: 1	Batch_num: 450	lr: 0.000	loss: 0.177	acc: 96.480%	train_loss: 0.3063	train_acc: 101.87%
INFO: 2017-09-06 22:12:07: main.py:142 **  Train=>
Epoch: 1	Batch_num: 480	lr: 0.000	loss: 0.211	acc: 96.270%	train_loss: 0.1736	train_acc: 93.0%
INFO: 2017-09-06 22:12:32: main.py:142 **  Train=>
Epoch: 1	Batch_num: 507	lr: 0.000	loss: 0.211	acc: 96.270%	train_loss: 0.3158	train_acc: 107.48%
INFO: 2017-09-06 22:13:11: main.py:81 **  Validate=>
Epoch: 1	Valid_loss: 0.174	Valid_acc: 96.590%
INFO: 2017-09-06 22:13:15: main.py:142 **  Train=>
Epoch: 2	Batch_num: 0	lr: 0.000	loss: 0.092	acc: 97.170%	train_loss: 0.0923	train_acc: 97.17%
INFO: 2017-09-06 22:13:43: main.py:142 **  Train=>
Epoch: 2	Batch_num: 30	lr: 0.000	loss: 0.203	acc: 97.480%	train_loss: 0.1634	train_acc: 92.24%
INFO: 2017-09-06 22:14:11: main.py:142 **  Train=>
Epoch: 2	Batch_num: 60	lr: 0.000	loss: 0.223	acc: 93.150%	train_loss: 0.2255	train_acc: 94.54%
INFO: 2017-09-06 22:14:38: main.py:142 **  Train=>
Epoch: 2	Batch_num: 90	lr: 0.000	loss: 0.196	acc: 95.930%	train_loss: 0.1287	train_acc: 97.05%
INFO: 2017-09-06 22:15:05: main.py:142 **  Train=>
Epoch: 2	Batch_num: 120	lr: 0.000	loss: 0.213	acc: 94.960%	train_loss: 0.1594	train_acc: 94.25%
INFO: 2017-09-06 22:15:33: main.py:142 **  Train=>
Epoch: 2	Batch_num: 150	lr: 0.000	loss: 0.180	acc: 95.690%	train_loss: 0.0926	train_acc: 97.31%
INFO: 2017-09-06 22:16:00: main.py:142 **  Train=>
Epoch: 2	Batch_num: 180	lr: 0.000	loss: 0.153	acc: 96.720%	train_loss: 0.1859	train_acc: 110.64%
INFO: 2017-09-06 22:16:27: main.py:142 **  Train=>
Epoch: 2	Batch_num: 210	lr: 0.000	loss: 0.202	acc: 93.160%	train_loss: 0.1271	train_acc: 95.89%
INFO: 2017-09-06 22:16:54: main.py:142 **  Train=>
Epoch: 2	Batch_num: 240	lr: 0.000	loss: 0.162	acc: 95.210%	train_loss: 0.0571	train_acc: 98.39%
INFO: 2017-09-06 22:17:22: main.py:142 **  Train=>
Epoch: 2	Batch_num: 270	lr: 0.000	loss: 0.167	acc: 98.030%	train_loss: 0.0693	train_acc: 98.16%
INFO: 2017-09-06 22:17:49: main.py:142 **  Train=>
Epoch: 2	Batch_num: 300	lr: 0.000	loss: 0.145	acc: 97.700%	train_loss: 0.1079	train_acc: 97.77%
INFO: 2017-09-06 22:18:16: main.py:142 **  Train=>
Epoch: 2	Batch_num: 330	lr: 0.000	loss: 0.136	acc: 98.090%	train_loss: 0.1796	train_acc: 94.53%
INFO: 2017-09-06 22:18:43: main.py:142 **  Train=>
Epoch: 2	Batch_num: 360	lr: 0.000	loss: 0.137	acc: 97.860%	train_loss: 0.1824	train_acc: 88.63%
INFO: 2017-09-06 22:19:09: main.py:142 **  Train=>
Epoch: 2	Batch_num: 390	lr: 0.000	loss: 0.142	acc: 96.090%	train_loss: 0.1165	train_acc: 96.65%
INFO: 2017-09-06 22:19:35: main.py:142 **  Train=>
Epoch: 2	Batch_num: 420	lr: 0.000	loss: 0.179	acc: 95.160%	train_loss: 0.1573	train_acc: 94.14%
INFO: 2017-09-06 22:20:02: main.py:142 **  Train=>
Epoch: 2	Batch_num: 450	lr: 0.000	loss: 0.174	acc: 96.720%	train_loss: 0.0982	train_acc: 95.51%
INFO: 2017-09-06 22:20:27: main.py:142 **  Train=>
Epoch: 2	Batch_num: 480	lr: 0.000	loss: 0.169	acc: 93.210%	train_loss: 0.1109	train_acc: 95.53%
INFO: 2017-09-06 22:20:51: main.py:142 **  Train=>
Epoch: 2	Batch_num: 507	lr: 0.000	loss: 0.169	acc: 93.210%	train_loss: 0.1416	train_acc: 96.12%
INFO: 2017-09-06 22:21:27: main.py:81 **  Validate=>
Epoch: 2	Valid_loss: 0.200	Valid_acc: 94.600%
INFO: 2017-09-06 22:21:29: main.py:142 **  Train=>
Epoch: 3	Batch_num: 0	lr: 0.000	loss: 0.089	acc: 97.140%	train_loss: 0.0895	train_acc: 97.14%
INFO: 2017-09-06 22:21:55: main.py:142 **  Train=>
Epoch: 3	Batch_num: 30	lr: 0.000	loss: 0.172	acc: 97.840%	train_loss: 0.1191	train_acc: 96.13%
INFO: 2017-09-06 22:22:21: main.py:142 **  Train=>
Epoch: 3	Batch_num: 60	lr: 0.000	loss: 0.169	acc: 94.610%	train_loss: 0.1633	train_acc: 96.09%
INFO: 2017-09-06 22:22:47: main.py:142 **  Train=>
Epoch: 3	Batch_num: 90	lr: 0.000	loss: 0.162	acc: 95.710%	train_loss: 0.2487	train_acc: 93.75%
INFO: 2017-09-06 22:23:13: main.py:142 **  Train=>
Epoch: 3	Batch_num: 120	lr: 0.000	loss: 0.137	acc: 95.850%	train_loss: 0.0634	train_acc: 98.48%
INFO: 2017-09-06 22:23:39: main.py:142 **  Train=>
Epoch: 3	Batch_num: 150	lr: 0.000	loss: 0.134	acc: 97.990%	train_loss: 0.0747	train_acc: 97.89%
INFO: 2017-09-06 22:24:05: main.py:142 **  Train=>
Epoch: 3	Batch_num: 180	lr: 0.000	loss: 0.186	acc: 99.210%	train_loss: 0.8948	train_acc: 90.27%
INFO: 2017-09-06 22:24:31: main.py:142 **  Train=>
Epoch: 3	Batch_num: 210	lr: 0.000	loss: 0.220	acc: 94.020%	train_loss: 0.2018	train_acc: 84.52%
INFO: 2017-09-06 22:24:57: main.py:142 **  Train=>
Epoch: 3	Batch_num: 240	lr: 0.000	loss: 0.189	acc: 93.840%	train_loss: 0.1475	train_acc: 93.88%
INFO: 2017-09-06 22:25:23: main.py:142 **  Train=>
Epoch: 3	Batch_num: 270	lr: 0.000	loss: 0.150	acc: 97.440%	train_loss: 0.0582	train_acc: 98.67%
INFO: 2017-09-06 22:25:49: main.py:142 **  Train=>
Epoch: 3	Batch_num: 300	lr: 0.000	loss: 0.206	acc: 95.570%	train_loss: 0.175	train_acc: 106.53%
INFO: 2017-09-06 22:26:15: main.py:142 **  Train=>
Epoch: 3	Batch_num: 330	lr: 0.000	loss: 0.170	acc: 95.810%	train_loss: 0.077	train_acc: 97.79%
INFO: 2017-09-06 22:26:41: main.py:142 **  Train=>
Epoch: 3	Batch_num: 360	lr: 0.000	loss: 0.220	acc: 93.070%	train_loss: 0.1084	train_acc: 95.44%
INFO: 2017-09-06 22:27:07: main.py:142 **  Train=>
Epoch: 3	Batch_num: 390	lr: 0.000	loss: 0.148	acc: 96.960%	train_loss: 0.1328	train_acc: 92.24%
INFO: 2017-09-06 22:27:33: main.py:142 **  Train=>
Epoch: 3	Batch_num: 420	lr: 0.000	loss: 0.137	acc: 95.670%	train_loss: 0.3252	train_acc: 74.08%
INFO: 2017-09-06 22:27:59: main.py:142 **  Train=>
Epoch: 3	Batch_num: 450	lr: 0.000	loss: 0.145	acc: 96.420%	train_loss: 0.1408	train_acc: 95.53%
INFO: 2017-09-06 22:28:25: main.py:142 **  Train=>
Epoch: 3	Batch_num: 480	lr: 0.000	loss: 0.112	acc: 98.010%	train_loss: 0.0986	train_acc: 97.46%
INFO: 2017-09-06 22:28:48: main.py:142 **  Train=>
Epoch: 3	Batch_num: 507	lr: 0.000	loss: 0.112	acc: 98.010%	train_loss: 0.2868	train_acc: 82.07%
INFO: 2017-09-06 22:29:24: main.py:81 **  Validate=>
Epoch: 3	Valid_loss: 0.551	Valid_acc: 82.000%
INFO: 2017-09-06 22:29:27: main.py:142 **  Train=>
Epoch: 4	Batch_num: 0	lr: 0.000	loss: 0.204	acc: 93.860%	train_loss: 0.2039	train_acc: 93.86%
INFO: 2017-09-06 22:29:52: main.py:142 **  Train=>
Epoch: 4	Batch_num: 30	lr: 0.000	loss: 0.322	acc: 90.480%	train_loss: 0.3728	train_acc: 88.22%
INFO: 2017-09-06 22:30:18: main.py:142 **  Train=>
Epoch: 4	Batch_num: 60	lr: 0.000	loss: 0.237	acc: 94.000%	train_loss: 0.278	train_acc: 109.53%
INFO: 2017-09-06 22:30:44: main.py:142 **  Train=>
Epoch: 4	Batch_num: 90	lr: 0.000	loss: 0.163	acc: 96.350%	train_loss: 0.1189	train_acc: 95.64%
INFO: 2017-09-06 22:31:10: main.py:142 **  Train=>
Epoch: 4	Batch_num: 120	lr: 0.000	loss: 0.150	acc: 98.480%	train_loss: 0.218	train_acc: 96.22%
INFO: 2017-09-06 22:31:36: main.py:142 **  Train=>
Epoch: 4	Batch_num: 150	lr: 0.000	loss: 0.156	acc: 95.350%	train_loss: 0.0389	train_acc: 108.75%
INFO: 2017-09-06 22:32:02: main.py:142 **  Train=>
Epoch: 4	Batch_num: 180	lr: 0.000	loss: 0.112	acc: 97.530%	train_loss: -0.0643	train_acc: 110.97%
INFO: 2017-09-06 22:32:28: main.py:142 **  Train=>
Epoch: 4	Batch_num: 210	lr: 0.000	loss: 0.078	acc: 99.210%	train_loss: 0.2042	train_acc: 86.35%
INFO: 2017-09-06 22:32:54: main.py:142 **  Train=>
Epoch: 4	Batch_num: 240	lr: 0.000	loss: 0.063	acc: 99.650%	train_loss: 0.1142	train_acc: 95.09%
INFO: 2017-09-06 22:33:19: main.py:142 **  Train=>
Epoch: 4	Batch_num: 270	lr: 0.000	loss: 0.074	acc: 98.530%	train_loss: 0.068	train_acc: 98.45%
INFO: 2017-09-06 22:33:45: main.py:142 **  Train=>
Epoch: 4	Batch_num: 300	lr: 0.000	loss: 0.058	acc: 99.450%	train_loss: 0.0718	train_acc: 96.76%
INFO: 2017-09-06 22:34:11: main.py:142 **  Train=>
Epoch: 4	Batch_num: 330	lr: 0.000	loss: 0.101	acc: 95.710%	train_loss: 0.0774	train_acc: 97.73%
INFO: 2017-09-06 22:34:37: main.py:142 **  Train=>
Epoch: 4	Batch_num: 360	lr: 0.000	loss: 0.091	acc: 96.920%	train_loss: -0.0657	train_acc: 110.47%
INFO: 2017-09-06 22:35:03: main.py:142 **  Train=>
Epoch: 4	Batch_num: 390	lr: 0.000	loss: 0.089	acc: 98.040%	train_loss: 0.0592	train_acc: 97.88%
INFO: 2017-09-06 22:35:29: main.py:142 **  Train=>
Epoch: 4	Batch_num: 420	lr: 0.000	loss: 0.152	acc: 97.350%	train_loss: 0.1005	train_acc: 96.14%
INFO: 2017-09-06 22:35:55: main.py:142 **  Train=>
Epoch: 4	Batch_num: 450	lr: 0.000	loss: 0.132	acc: 97.090%	train_loss: 0.1401	train_acc: 96.28%
INFO: 2017-09-06 22:36:21: main.py:142 **  Train=>
Epoch: 4	Batch_num: 480	lr: 0.000	loss: 0.241	acc: 92.690%	train_loss: 0.1375	train_acc: 95.7%
INFO: 2017-09-06 22:36:44: main.py:142 **  Train=>
Epoch: 4	Batch_num: 507	lr: 0.000	loss: 0.241	acc: 92.690%	train_loss: 0.1186	train_acc: 96.17%
INFO: 2017-09-06 22:37:21: main.py:81 **  Validate=>
Epoch: 4	Valid_loss: 0.108	Valid_acc: 97.820%
INFO: 2017-09-06 22:37:25: main.py:142 **  Train=>
Epoch: 5	Batch_num: 0	lr: 0.000	loss: 0.069	acc: 97.830%	train_loss: 0.0688	train_acc: 97.83%
INFO: 2017-09-06 22:37:50: main.py:142 **  Train=>
Epoch: 5	Batch_num: 30	lr: 0.000	loss: 0.116	acc: 99.970%	train_loss: 0.1068	train_acc: 96.85%
INFO: 2017-09-06 22:38:16: main.py:142 **  Train=>
Epoch: 5	Batch_num: 60	lr: 0.000	loss: 0.111	acc: 96.810%	train_loss: 0.0896	train_acc: 96.6%
INFO: 2017-09-06 22:38:43: main.py:142 **  Train=>
Epoch: 5	Batch_num: 90	lr: 0.000	loss: 0.099	acc: 98.130%	train_loss: 0.2245	train_acc: 84.88%
INFO: 2017-09-06 22:39:10: main.py:142 **  Train=>
Epoch: 5	Batch_num: 120	lr: 0.000	loss: 0.095	acc: 97.840%	train_loss: 0.1221	train_acc: 97.48%
INFO: 2017-09-06 22:39:36: main.py:142 **  Train=>
Epoch: 5	Batch_num: 150	lr: 0.000	loss: 0.092	acc: 97.110%	train_loss: 0.0948	train_acc: 97.78%
INFO: 2017-09-06 22:40:02: main.py:142 **  Train=>
Epoch: 5	Batch_num: 180	lr: 0.000	loss: 0.173	acc: 96.080%	train_loss: 0.1476	train_acc: 94.23%
INFO: 2017-09-06 22:40:29: main.py:142 **  Train=>
Epoch: 5	Batch_num: 210	lr: 0.000	loss: 0.140	acc: 96.690%	train_loss: 0.3318	train_acc: 94.8%
INFO: 2017-09-06 22:40:56: main.py:142 **  Train=>
Epoch: 5	Batch_num: 240	lr: 0.000	loss: 0.101	acc: 96.300%	train_loss: 0.0723	train_acc: 96.74%
INFO: 2017-09-06 22:41:24: main.py:142 **  Train=>
Epoch: 5	Batch_num: 270	lr: 0.000	loss: 0.176	acc: 95.560%	train_loss: 0.0912	train_acc: 97.52%
INFO: 2017-09-06 22:41:51: main.py:142 **  Train=>
Epoch: 5	Batch_num: 300	lr: 0.000	loss: 0.253	acc: 94.710%	train_loss: 0.1357	train_acc: 95.26%
INFO: 2017-09-06 22:42:18: main.py:142 **  Train=>
Epoch: 5	Batch_num: 330	lr: 0.000	loss: 0.215	acc: 95.470%	train_loss: 0.0959	train_acc: 98.27%
INFO: 2017-09-06 22:42:44: main.py:142 **  Train=>
Epoch: 5	Batch_num: 360	lr: 0.000	loss: 0.172	acc: 97.220%	train_loss: 0.1424	train_acc: 95.15%
INFO: 2017-09-06 22:43:10: main.py:142 **  Train=>
Epoch: 5	Batch_num: 390	lr: 0.000	loss: 0.146	acc: 96.390%	train_loss: 0.1024	train_acc: 96.9%
INFO: 2017-09-06 22:43:36: main.py:142 **  Train=>
Epoch: 5	Batch_num: 420	lr: 0.000	loss: 0.090	acc: 98.500%	train_loss: 0.2125	train_acc: 85.3%
INFO: 2017-09-06 22:44:03: main.py:142 **  Train=>
Epoch: 5	Batch_num: 450	lr: 0.000	loss: 0.153	acc: 95.370%	train_loss: 0.0853	train_acc: 97.45%
INFO: 2017-09-06 22:44:29: main.py:142 **  Train=>
Epoch: 5	Batch_num: 480	lr: 0.000	loss: 0.145	acc: 98.620%	train_loss: 0.0978	train_acc: 97.17%
INFO: 2017-09-06 22:44:53: main.py:142 **  Train=>
Epoch: 5	Batch_num: 507	lr: 0.000	loss: 0.145	acc: 98.620%	train_loss: 0.1206	train_acc: 96.66%
INFO: 2017-09-06 22:45:31: main.py:81 **  Validate=>
Epoch: 5	Valid_loss: 0.073	Valid_acc: 98.720%
INFO: 2017-09-06 22:45:36: main.py:142 **  Train=>
Epoch: 6	Batch_num: 0	lr: 0.000	loss: 0.081	acc: 97.730%	train_loss: 0.0806	train_acc: 97.73%
INFO: 2017-09-06 22:46:01: main.py:142 **  Train=>
Epoch: 6	Batch_num: 30	lr: 0.000	loss: 0.278	acc: 98.720%	train_loss: 0.6342	train_acc: 85.12%
INFO: 2017-09-06 22:46:27: main.py:142 **  Train=>
Epoch: 6	Batch_num: 60	lr: 0.000	loss: 0.352	acc: 90.170%	train_loss: 0.3935	train_acc: 74.01%
INFO: 2017-09-06 22:46:54: main.py:142 **  Train=>
Epoch: 6	Batch_num: 90	lr: 0.000	loss: 0.323	acc: 89.450%	train_loss: 0.2317	train_acc: 95.7%
INFO: 2017-09-06 22:47:20: main.py:142 **  Train=>
Epoch: 6	Batch_num: 120	lr: 0.000	loss: 0.363	acc: 88.670%	train_loss: 0.2059	train_acc: 94.1%
INFO: 2017-09-06 22:47:47: main.py:142 **  Train=>
Epoch: 6	Batch_num: 150	lr: 0.000	loss: 0.318	acc: 90.950%	train_loss: 0.1733	train_acc: 94.69%
INFO: 2017-09-06 22:48:13: main.py:142 **  Train=>
Epoch: 6	Batch_num: 180	lr: 0.000	loss: 0.230	acc: 92.740%	train_loss: 0.3094	train_acc: 91.07%
INFO: 2017-09-06 22:48:39: main.py:142 **  Train=>
Epoch: 6	Batch_num: 210	lr: 0.000	loss: 0.178	acc: 93.850%	train_loss: -0.0024	train_acc: 109.66%
INFO: 2017-09-06 22:49:06: main.py:142 **  Train=>
Epoch: 6	Batch_num: 240	lr: 0.000	loss: 0.303	acc: 92.360%	train_loss: 0.3075	train_acc: 83.0%
INFO: 2017-09-06 22:49:32: main.py:142 **  Train=>
Epoch: 6	Batch_num: 270	lr: 0.000	loss: 0.297	acc: 93.040%	train_loss: 0.148	train_acc: 96.16%
INFO: 2017-09-06 22:49:59: main.py:142 **  Train=>
Epoch: 6	Batch_num: 300	lr: 0.000	loss: 0.286	acc: 92.550%	train_loss: 0.1634	train_acc: 94.06%
INFO: 2017-09-06 22:50:25: main.py:142 **  Train=>
Epoch: 6	Batch_num: 330	lr: 0.000	loss: 0.164	acc: 95.470%	train_loss: 0.151	train_acc: 94.34%
INFO: 2017-09-06 22:50:51: main.py:142 **  Train=>
Epoch: 6	Batch_num: 360	lr: 0.000	loss: 0.153	acc: 97.250%	train_loss: 0.0809	train_acc: 98.22%
INFO: 2017-09-06 22:51:17: main.py:142 **  Train=>
Epoch: 6	Batch_num: 390	lr: 0.000	loss: 0.216	acc: 94.910%	train_loss: 0.3937	train_acc: 96.14%
INFO: 2017-09-06 22:51:44: main.py:142 **  Train=>
Epoch: 6	Batch_num: 420	lr: 0.000	loss: 0.370	acc: 89.280%	train_loss: 0.2687	train_acc: 102.46%
INFO: 2017-09-06 22:52:10: main.py:142 **  Train=>
Epoch: 6	Batch_num: 450	lr: 0.000	loss: 0.215	acc: 94.580%	train_loss: 0.2065	train_acc: 91.69%
INFO: 2017-09-06 22:52:36: main.py:142 **  Train=>
Epoch: 6	Batch_num: 480	lr: 0.000	loss: 0.304	acc: 89.710%	train_loss: 0.2533	train_acc: 100.33%
INFO: 2017-09-06 22:52:59: main.py:142 **  Train=>
Epoch: 6	Batch_num: 507	lr: 0.000	loss: 0.304	acc: 89.710%	train_loss: 0.0075	train_acc: 108.44%
INFO: 2017-09-06 22:53:37: main.py:81 **  Validate=>
Epoch: 6	Valid_loss: 0.178	Valid_acc: 95.250%
INFO: 2017-09-06 22:53:40: main.py:142 **  Train=>
Epoch: 7	Batch_num: 0	lr: 0.000	loss: 0.111	acc: 95.830%	train_loss: 0.1108	train_acc: 95.83%
INFO: 2017-09-06 22:54:06: main.py:142 **  Train=>
Epoch: 7	Batch_num: 30	lr: 0.000	loss: 0.145	acc: 96.800%	train_loss: 0.0766	train_acc: 108.56%
INFO: 2017-09-06 22:54:32: main.py:142 **  Train=>
Epoch: 7	Batch_num: 60	lr: 0.000	loss: 0.161	acc: 95.220%	train_loss: 0.1298	train_acc: 98.41%
INFO: 2017-09-06 22:54:58: main.py:142 **  Train=>
Epoch: 7	Batch_num: 90	lr: 0.000	loss: 0.171	acc: 93.600%	train_loss: 0.1887	train_acc: 95.6%
INFO: 2017-09-06 22:55:24: main.py:142 **  Train=>
Epoch: 7	Batch_num: 120	lr: 0.000	loss: 0.201	acc: 94.360%	train_loss: 0.4019	train_acc: 92.13%
INFO: 2017-09-06 22:55:50: main.py:142 **  Train=>
Epoch: 7	Batch_num: 150	lr: 0.000	loss: 0.253	acc: 90.880%	train_loss: 0.4441	train_acc: 71.77%
INFO: 2017-09-06 22:56:15: main.py:142 **  Train=>
Epoch: 7	Batch_num: 180	lr: 0.000	loss: 0.212	acc: 92.800%	train_loss: 0.1623	train_acc: 93.85%
INFO: 2017-09-06 22:56:41: main.py:142 **  Train=>
Epoch: 7	Batch_num: 210	lr: 0.000	loss: 0.116	acc: 97.620%	train_loss: 0.0627	train_acc: 102.92%
INFO: 2017-09-06 22:57:07: main.py:142 **  Train=>
Epoch: 7	Batch_num: 240	lr: 0.000	loss: 0.111	acc: 97.060%	train_loss: 0.1463	train_acc: 91.39%
INFO: 2017-09-06 22:57:33: main.py:142 **  Train=>
Epoch: 7	Batch_num: 270	lr: 0.000	loss: 0.308	acc: 90.250%	train_loss: 0.4594	train_acc: 89.92%
INFO: 2017-09-06 22:57:59: main.py:142 **  Train=>
Epoch: 7	Batch_num: 300	lr: 0.000	loss: 0.258	acc: 94.810%	train_loss: 0.1946	train_acc: 89.79%
INFO: 2017-09-06 22:58:24: main.py:142 **  Train=>
Epoch: 7	Batch_num: 330	lr: 0.000	loss: 0.191	acc: 96.180%	train_loss: 0.0863	train_acc: 97.22%
INFO: 2017-09-06 22:58:52: main.py:142 **  Train=>
Epoch: 7	Batch_num: 360	lr: 0.000	loss: 0.191	acc: 91.940%	train_loss: 0.1285	train_acc: 95.09%
INFO: 2017-09-06 22:59:19: main.py:142 **  Train=>
Epoch: 7	Batch_num: 390	lr: 0.000	loss: 0.154	acc: 95.270%	train_loss: 0.1614	train_acc: 94.86%
INFO: 2017-09-06 22:59:47: main.py:142 **  Train=>
Epoch: 7	Batch_num: 420	lr: 0.000	loss: 0.119	acc: 97.400%	train_loss: 0.0861	train_acc: 99.96%
INFO: 2017-09-06 23:00:14: main.py:142 **  Train=>
Epoch: 7	Batch_num: 450	lr: 0.000	loss: 0.175	acc: 94.320%	train_loss: 0.0925	train_acc: 97.04%
INFO: 2017-09-06 23:00:42: main.py:142 **  Train=>
Epoch: 7	Batch_num: 480	lr: 0.000	loss: 0.102	acc: 97.880%	train_loss: 0.0785	train_acc: 97.33%
INFO: 2017-09-06 23:01:07: main.py:142 **  Train=>
Epoch: 7	Batch_num: 507	lr: 0.000	loss: 0.102	acc: 97.880%	train_loss: 0.1037	train_acc: 97.22%
INFO: 2017-09-06 23:01:46: main.py:81 **  Validate=>
Epoch: 7	Valid_loss: 0.083	Valid_acc: 98.920%
INFO: 2017-09-06 23:01:51: main.py:142 **  Train=>
Epoch: 8	Batch_num: 0	lr: 0.000	loss: 0.089	acc: 97.680%	train_loss: 0.0888	train_acc: 97.68%
INFO: 2017-09-06 23:02:18: main.py:142 **  Train=>
Epoch: 8	Batch_num: 30	lr: 0.000	loss: 0.045	acc: 102.280%	train_loss: -0.0253	train_acc: 107.92%
INFO: 2017-09-06 23:02:46: main.py:142 **  Train=>
Epoch: 8	Batch_num: 60	lr: 0.000	loss: 0.086	acc: 98.670%	train_loss: 0.0913	train_acc: 108.78%
INFO: 2017-09-06 23:03:14: main.py:142 **  Train=>
Epoch: 8	Batch_num: 90	lr: 0.000	loss: 0.486	acc: 90.100%	train_loss: 0.3657	train_acc: 89.31%
INFO: 2017-09-06 23:03:42: main.py:142 **  Train=>
Epoch: 8	Batch_num: 120	lr: 0.000	loss: 0.304	acc: 90.570%	train_loss: 0.2014	train_acc: 94.12%
INFO: 2017-09-06 23:04:11: main.py:142 **  Train=>
Epoch: 8	Batch_num: 150	lr: 0.000	loss: 0.292	acc: 90.010%	train_loss: 0.1311	train_acc: 97.11%
INFO: 2017-09-06 23:04:38: main.py:142 **  Train=>
Epoch: 8	Batch_num: 180	lr: 0.000	loss: 0.212	acc: 93.730%	train_loss: 0.8761	train_acc: 87.71%
INFO: 2017-09-06 23:05:05: main.py:142 **  Train=>
Epoch: 8	Batch_num: 210	lr: 0.000	loss: 0.226	acc: 92.470%	train_loss: 0.1293	train_acc: 96.18%
INFO: 2017-09-06 23:05:32: main.py:142 **  Train=>
Epoch: 8	Batch_num: 240	lr: 0.000	loss: 0.174	acc: 94.440%	train_loss: 0.2069	train_acc: 85.1%
INFO: 2017-09-06 23:05:59: main.py:142 **  Train=>
Epoch: 8	Batch_num: 270	lr: 0.000	loss: 0.117	acc: 97.990%	train_loss: 0.1344	train_acc: 93.65%
INFO: 2017-09-06 23:06:27: main.py:142 **  Train=>
Epoch: 8	Batch_num: 300	lr: 0.000	loss: 0.141	acc: 95.680%	train_loss: 0.283	train_acc: 80.23%
INFO: 2017-09-06 23:06:54: main.py:142 **  Train=>
Epoch: 8	Batch_num: 330	lr: 0.000	loss: 0.177	acc: 94.340%	train_loss: 0.15	train_acc: 92.94%
INFO: 2017-09-06 23:07:22: main.py:142 **  Train=>
Epoch: 8	Batch_num: 360	lr: 0.000	loss: 0.117	acc: 96.350%	train_loss: 0.0867	train_acc: 96.96%
INFO: 2017-09-06 23:07:49: main.py:142 **  Train=>
Epoch: 8	Batch_num: 390	lr: 0.000	loss: 0.106	acc: 98.360%	train_loss: 0.0987	train_acc: 97.48%
INFO: 2017-09-06 23:08:16: main.py:142 **  Train=>
Epoch: 8	Batch_num: 420	lr: 0.000	loss: 0.109	acc: 98.200%	train_loss: 0.1716	train_acc: 110.5%
INFO: 2017-09-06 23:08:42: main.py:142 **  Train=>
Epoch: 8	Batch_num: 450	lr: 0.000	loss: 0.118	acc: 97.690%	train_loss: 0.1546	train_acc: 94.36%
INFO: 2017-09-06 23:09:08: main.py:142 **  Train=>
Epoch: 8	Batch_num: 480	lr: 0.000	loss: 0.167	acc: 96.600%	train_loss: -0.1641	train_acc: 122.36%
INFO: 2017-09-06 23:09:31: main.py:142 **  Train=>
Epoch: 8	Batch_num: 507	lr: 0.000	loss: 0.167	acc: 96.600%	train_loss: 0.1262	train_acc: 96.59%
INFO: 2017-09-06 23:10:08: main.py:81 **  Validate=>
Epoch: 8	Valid_loss: 0.285	Valid_acc: 91.950%
INFO: 2017-09-06 23:10:11: main.py:142 **  Train=>
Epoch: 9	Batch_num: 0	lr: 0.000	loss: 0.501	acc: 94.060%	train_loss: 0.5006	train_acc: 94.06%
INFO: 2017-09-06 23:10:37: main.py:142 **  Train=>
Epoch: 9	Batch_num: 30	lr: 0.000	loss: 0.167	acc: 96.230%	train_loss: -0.1193	train_acc: 121.71%
INFO: 2017-09-06 23:11:03: main.py:142 **  Train=>
Epoch: 9	Batch_num: 60	lr: 0.000	loss: 0.130	acc: 98.080%	train_loss: 0.1503	train_acc: 97.1%
INFO: 2017-09-06 23:11:29: main.py:142 **  Train=>
Epoch: 9	Batch_num: 90	lr: 0.000	loss: 0.068	acc: 101.470%	train_loss: 0.2379	train_acc: 93.5%
INFO: 2017-09-06 23:11:55: main.py:142 **  Train=>
Epoch: 9	Batch_num: 120	lr: 0.000	loss: 0.073	acc: 101.010%	train_loss: 0.0448	train_acc: 109.67%
INFO: 2017-09-06 23:12:21: main.py:142 **  Train=>
Epoch: 9	Batch_num: 150	lr: 0.000	loss: 0.079	acc: 99.070%	train_loss: 0.0707	train_acc: 98.17%
INFO: 2017-09-06 23:12:49: main.py:142 **  Train=>
Epoch: 9	Batch_num: 180	lr: 0.000	loss: 0.125	acc: 96.780%	train_loss: 0.0786	train_acc: 97.78%
INFO: 2017-09-06 23:13:17: main.py:142 **  Train=>
Epoch: 9	Batch_num: 210	lr: 0.000	loss: 0.125	acc: 96.440%	train_loss: 0.0878	train_acc: 97.27%
INFO: 2017-09-06 23:13:44: main.py:142 **  Train=>
Epoch: 9	Batch_num: 240	lr: 0.000	loss: 0.106	acc: 96.690%	train_loss: 0.1148	train_acc: 94.8%
INFO: 2017-09-06 23:14:12: main.py:142 **  Train=>
Epoch: 9	Batch_num: 270	lr: 0.000	loss: 0.189	acc: 97.080%	train_loss: 0.1589	train_acc: 95.69%
INFO: 2017-09-06 23:14:40: main.py:142 **  Train=>
Epoch: 9	Batch_num: 300	lr: 0.000	loss: 0.184	acc: 94.480%	train_loss: -0.0164	train_acc: 109.62%
INFO: 2017-09-06 23:15:08: main.py:142 **  Train=>
Epoch: 9	Batch_num: 330	lr: 0.000	loss: 0.328	acc: 93.650%	train_loss: 0.0129	train_acc: 109.07%
INFO: 2017-09-06 23:15:35: main.py:142 **  Train=>
Epoch: 9	Batch_num: 360	lr: 0.000	loss: 0.251	acc: 91.860%	train_loss: 0.1432	train_acc: 94.59%
INFO: 2017-09-06 23:16:02: main.py:142 **  Train=>
Epoch: 9	Batch_num: 390	lr: 0.000	loss: 0.119	acc: 98.310%	train_loss: 0.0937	train_acc: 97.55%
INFO: 2017-09-06 23:16:29: main.py:142 **  Train=>
Epoch: 9	Batch_num: 420	lr: 0.000	loss: 0.179	acc: 92.610%	train_loss: 0.4083	train_acc: 85.24%
INFO: 2017-09-06 23:16:56: main.py:142 **  Train=>
Epoch: 9	Batch_num: 450	lr: 0.000	loss: 0.146	acc: 95.160%	train_loss: 0.1006	train_acc: 97.38%
INFO: 2017-09-06 23:17:23: main.py:142 **  Train=>
Epoch: 9	Batch_num: 480	lr: 0.000	loss: 0.123	acc: 97.480%	train_loss: 0.2334	train_acc: 89.02%
INFO: 2017-09-06 23:17:48: main.py:142 **  Train=>
Epoch: 9	Batch_num: 507	lr: 0.000	loss: 0.123	acc: 97.480%	train_loss: 0.0948	train_acc: 108.32%
INFO: 2017-09-06 23:18:26: main.py:81 **  Validate=>
Epoch: 9	Valid_loss: 2.003	Valid_acc: 58.890%
INFO: 2017-09-06 23:18:29: main.py:142 **  Train=>
Epoch: 10	Batch_num: 0	lr: 0.000	loss: 0.152	acc: 95.300%	train_loss: 0.1524	train_acc: 95.3%
INFO: 2017-09-06 23:18:56: main.py:142 **  Train=>
Epoch: 10	Batch_num: 30	lr: 0.000	loss: 0.253	acc: 94.110%	train_loss: 0.1243	train_acc: 96.1%
INFO: 2017-09-06 23:19:23: main.py:142 **  Train=>
Epoch: 10	Batch_num: 60	lr: 0.000	loss: 0.200	acc: 94.430%	train_loss: 0.2538	train_acc: 92.94%
INFO: 2017-09-06 23:19:50: main.py:142 **  Train=>
Epoch: 10	Batch_num: 90	lr: 0.000	loss: 0.230	acc: 93.290%	train_loss: 0.4086	train_acc: 82.09%
INFO: 2017-09-06 23:20:17: main.py:142 **  Train=>
Epoch: 10	Batch_num: 120	lr: 0.000	loss: 0.146	acc: 97.910%	train_loss: 0.3083	train_acc: 92.93%
INFO: 2017-09-06 23:20:43: main.py:142 **  Train=>
Epoch: 10	Batch_num: 150	lr: 0.000	loss: 0.184	acc: 94.270%	train_loss: 0.0823	train_acc: 101.72%
INFO: 2017-09-06 23:21:10: main.py:142 **  Train=>
Epoch: 10	Batch_num: 180	lr: 0.000	loss: 0.132	acc: 95.700%	train_loss: 0.1008	train_acc: 95.73%
INFO: 2017-09-06 23:21:38: main.py:142 **  Train=>
Epoch: 10	Batch_num: 210	lr: 0.000	loss: 0.119	acc: 97.240%	train_loss: -0.0544	train_acc: 110.49%
INFO: 2017-09-06 23:22:04: main.py:142 **  Train=>
Epoch: 10	Batch_num: 240	lr: 0.000	loss: 0.110	acc: 97.380%	train_loss: 0.1684	train_acc: 89.94%
INFO: 2017-09-06 23:22:31: main.py:142 **  Train=>
Epoch: 10	Batch_num: 270	lr: 0.000	loss: 0.090	acc: 97.970%	train_loss: 0.1815	train_acc: 86.36%
INFO: 2017-09-06 23:22:58: main.py:142 **  Train=>
Epoch: 10	Batch_num: 300	lr: 0.000	loss: 0.102	acc: 96.910%	train_loss: 0.1474	train_acc: 92.23%
INFO: 2017-09-06 23:23:25: main.py:142 **  Train=>
Epoch: 10	Batch_num: 330	lr: 0.000	loss: 0.107	acc: 97.200%	train_loss: 0.1236	train_acc: 97.4%
INFO: 2017-09-06 23:23:52: main.py:142 **  Train=>
Epoch: 10	Batch_num: 360	lr: 0.000	loss: 0.123	acc: 97.860%	train_loss: 0.0531	train_acc: 99.01%
INFO: 2017-09-06 23:24:19: main.py:142 **  Train=>
Epoch: 10	Batch_num: 390	lr: 0.000	loss: 0.109	acc: 97.540%	train_loss: 0.0951	train_acc: 97.66%
INFO: 2017-09-06 23:24:46: main.py:142 **  Train=>
Epoch: 10	Batch_num: 420	lr: 0.000	loss: 0.138	acc: 96.590%	train_loss: 0.1982	train_acc: 88.35%
INFO: 2017-09-06 23:25:12: main.py:142 **  Train=>
Epoch: 10	Batch_num: 450	lr: 0.000	loss: 0.131	acc: 95.320%	train_loss: -0.0475	train_acc: 110.35%
INFO: 2017-09-06 23:25:38: main.py:142 **  Train=>
Epoch: 10	Batch_num: 480	lr: 0.000	loss: 0.063	acc: 100.150%	train_loss: -0.0426	train_acc: 109.14%
INFO: 2017-09-06 23:26:02: main.py:142 **  Train=>
Epoch: 10	Batch_num: 507	lr: 0.000	loss: 0.063	acc: 100.150%	train_loss: 0.1063	train_acc: 97.63%
INFO: 2017-09-06 23:26:40: main.py:81 **  Validate=>
Epoch: 10	Valid_loss: 0.080	Valid_acc: 97.860%
INFO: 2017-09-06 23:26:42: main.py:142 **  Train=>
Epoch: 11	Batch_num: 0	lr: 0.000	loss: 0.127	acc: 95.870%	train_loss: 0.1265	train_acc: 95.87%
INFO: 2017-09-06 23:27:10: main.py:142 **  Train=>
Epoch: 11	Batch_num: 30	lr: 0.000	loss: 0.062	acc: 99.130%	train_loss: -0.2151	train_acc: 124.2%
INFO: 2017-09-06 23:27:36: main.py:142 **  Train=>
Epoch: 11	Batch_num: 60	lr: 0.000	loss: 0.168	acc: 95.810%	train_loss: 0.2144	train_acc: 85.04%
INFO: 2017-09-06 23:28:02: main.py:142 **  Train=>
Epoch: 11	Batch_num: 90	lr: 0.000	loss: 0.088	acc: 97.540%	train_loss: 0.1182	train_acc: 96.56%
INFO: 2017-09-06 23:28:28: main.py:142 **  Train=>
Epoch: 11	Batch_num: 120	lr: 0.000	loss: 0.061	acc: 100.450%	train_loss: 0.0897	train_acc: 97.02%
INFO: 2017-09-06 23:28:54: main.py:142 **  Train=>
Epoch: 11	Batch_num: 150	lr: 0.000	loss: 0.074	acc: 99.320%	train_loss: -0.0749	train_acc: 110.59%
INFO: 2017-09-06 23:29:19: main.py:142 **  Train=>
Epoch: 11	Batch_num: 180	lr: 0.000	loss: 0.089	acc: 95.820%	train_loss: 0.0562	train_acc: 98.36%
INFO: 2017-09-06 23:29:45: main.py:142 **  Train=>
Epoch: 11	Batch_num: 210	lr: 0.000	loss: 0.046	acc: 100.720%	train_loss: -0.0384	train_acc: 107.64%
INFO: 2017-09-06 23:30:11: main.py:142 **  Train=>
Epoch: 11	Batch_num: 240	lr: 0.000	loss: 0.064	acc: 100.040%	train_loss: 0.0854	train_acc: 98.46%
INFO: 2017-09-06 23:30:37: main.py:142 **  Train=>
Epoch: 11	Batch_num: 270	lr: 0.000	loss: 0.047	acc: 99.980%	train_loss: -0.0673	train_acc: 110.43%
INFO: 2017-09-06 23:31:03: main.py:142 **  Train=>
Epoch: 11	Batch_num: 300	lr: 0.000	loss: 0.041	acc: 101.290%	train_loss: -0.0584	train_acc: 110.18%
INFO: 2017-09-06 23:31:29: main.py:142 **  Train=>
Epoch: 11	Batch_num: 330	lr: 0.000	loss: 0.084	acc: 97.380%	train_loss: 0.1935	train_acc: 85.25%
INFO: 2017-09-06 23:31:54: main.py:142 **  Train=>
Epoch: 11	Batch_num: 360	lr: 0.000	loss: 0.081	acc: 97.250%	train_loss: 0.1063	train_acc: 95.85%
INFO: 2017-09-06 23:32:20: main.py:142 **  Train=>
Epoch: 11	Batch_num: 390	lr: 0.000	loss: 0.083	acc: 97.040%	train_loss: -0.0701	train_acc: 111.17%
INFO: 2017-09-06 23:32:47: main.py:142 **  Train=>
Epoch: 11	Batch_num: 420	lr: 0.000	loss: 0.039	acc: 102.230%	train_loss: -0.0504	train_acc: 110.92%
INFO: 2017-09-06 23:33:14: main.py:142 **  Train=>
Epoch: 11	Batch_num: 450	lr: 0.000	loss: 0.064	acc: 100.510%	train_loss: 0.1186	train_acc: 97.03%
INFO: 2017-09-06 23:33:42: main.py:142 **  Train=>
Epoch: 11	Batch_num: 480	lr: 0.000	loss: 0.060	acc: 100.350%	train_loss: 0.1529	train_acc: 89.78%
INFO: 2017-09-06 23:34:07: main.py:142 **  Train=>
Epoch: 11	Batch_num: 507	lr: 0.000	loss: 0.060	acc: 100.350%	train_loss: 0.0911	train_acc: 96.41%
INFO: 2017-09-06 23:34:45: main.py:81 **  Validate=>
Epoch: 11	Valid_loss: 0.050	Valid_acc: 99.770%
INFO: 2017-09-06 23:34:49: main.py:142 **  Train=>
Epoch: 12	Batch_num: 0	lr: 0.000	loss: 0.157	acc: 91.470%	train_loss: 0.1567	train_acc: 91.47%
INFO: 2017-09-06 23:35:17: main.py:142 **  Train=>
Epoch: 12	Batch_num: 30	lr: 0.000	loss: 0.038	acc: 100.930%	train_loss: -0.0762	train_acc: 110.36%
INFO: 2017-09-06 23:35:44: main.py:142 **  Train=>
Epoch: 12	Batch_num: 60	lr: 0.000	loss: 0.063	acc: 99.100%	train_loss: 0.1118	train_acc: 97.57%
INFO: 2017-09-06 23:36:12: main.py:142 **  Train=>
Epoch: 12	Batch_num: 90	lr: 0.000	loss: 0.070	acc: 98.450%	train_loss: 0.0526	train_acc: 98.56%
INFO: 2017-09-06 23:36:40: main.py:142 **  Train=>
Epoch: 12	Batch_num: 120	lr: 0.000	loss: 0.054	acc: 99.820%	train_loss: 0.1076	train_acc: 96.67%
INFO: 2017-09-06 23:37:08: main.py:142 **  Train=>
Epoch: 12	Batch_num: 150	lr: 0.000	loss: 0.033	acc: 101.080%	train_loss: 0.0746	train_acc: 97.24%
INFO: 2017-09-06 23:37:35: main.py:142 **  Train=>
Epoch: 12	Batch_num: 180	lr: 0.000	loss: 0.076	acc: 98.440%	train_loss: 0.0883	train_acc: 97.28%
INFO: 2017-09-06 23:38:03: main.py:142 **  Train=>
Epoch: 12	Batch_num: 210	lr: 0.000	loss: 0.050	acc: 100.360%	train_loss: -0.057	train_acc: 110.24%
INFO: 2017-09-06 23:38:31: main.py:142 **  Train=>
Epoch: 12	Batch_num: 240	lr: 0.000	loss: 0.062	acc: 98.360%	train_loss: -0.0871	train_acc: 111.61%
INFO: 2017-09-06 23:38:58: main.py:142 **  Train=>
Epoch: 12	Batch_num: 270	lr: 0.000	loss: 0.034	acc: 101.150%	train_loss: 0.0423	train_acc: 98.74%
INFO: 2017-09-06 23:39:26: main.py:142 **  Train=>
Epoch: 12	Batch_num: 300	lr: 0.000	loss: 0.043	acc: 100.180%	train_loss: -0.0737	train_acc: 111.34%
INFO: 2017-09-06 23:39:54: main.py:142 **  Train=>
Epoch: 12	Batch_num: 330	lr: 0.000	loss: 0.059	acc: 98.860%	train_loss: 0.0873	train_acc: 97.57%
INFO: 2017-09-06 23:40:22: main.py:142 **  Train=>
Epoch: 12	Batch_num: 360	lr: 0.000	loss: 0.048	acc: 99.840%	train_loss: 0.0545	train_acc: 97.98%
INFO: 2017-09-06 23:40:50: main.py:142 **  Train=>
Epoch: 12	Batch_num: 390	lr: 0.000	loss: 0.088	acc: 96.680%	train_loss: 0.0688	train_acc: 96.91%
INFO: 2017-09-06 23:41:17: main.py:142 **  Train=>
Epoch: 12	Batch_num: 420	lr: 0.000	loss: 0.062	acc: 99.310%	train_loss: -0.0872	train_acc: 111.44%
INFO: 2017-09-06 23:41:45: main.py:142 **  Train=>
Epoch: 12	Batch_num: 450	lr: 0.000	loss: 0.311	acc: 94.610%	train_loss: 0.2725	train_acc: 94.52%
INFO: 2017-09-06 23:42:13: main.py:142 **  Train=>
Epoch: 12	Batch_num: 480	lr: 0.000	loss: 0.107	acc: 100.100%	train_loss: 0.2212	train_acc: 85.73%
INFO: 2017-09-06 23:42:37: main.py:142 **  Train=>
Epoch: 12	Batch_num: 507	lr: 0.000	loss: 0.107	acc: 100.100%	train_loss: 0.1045	train_acc: 97.14%
INFO: 2017-09-06 23:43:16: main.py:81 **  Validate=>
Epoch: 12	Valid_loss: 0.078	Valid_acc: 99.180%
INFO: 2017-09-06 23:43:19: main.py:142 **  Train=>
Epoch: 13	Batch_num: 0	lr: 0.000	loss: 0.110	acc: 95.530%	train_loss: 0.1103	train_acc: 95.53%
INFO: 2017-09-06 23:43:46: main.py:142 **  Train=>
Epoch: 13	Batch_num: 30	lr: 0.000	loss: 0.097	acc: 98.490%	train_loss: 0.0053	train_acc: 109.16%
INFO: 2017-09-06 23:44:14: main.py:142 **  Train=>
Epoch: 13	Batch_num: 60	lr: 0.000	loss: 0.105	acc: 97.920%	train_loss: -0.0487	train_acc: 110.87%
INFO: 2017-09-06 23:44:41: main.py:142 **  Train=>
Epoch: 13	Batch_num: 90	lr: 0.000	loss: 0.081	acc: 98.990%	train_loss: 0.0876	train_acc: 98.15%
INFO: 2017-09-06 23:45:09: main.py:142 **  Train=>
Epoch: 13	Batch_num: 120	lr: 0.000	loss: 0.094	acc: 96.990%	train_loss: 0.1148	train_acc: 97.1%
INFO: 2017-09-06 23:45:37: main.py:142 **  Train=>
Epoch: 13	Batch_num: 150	lr: 0.000	loss: 0.081	acc: 97.400%	train_loss: 0.0689	train_acc: 98.08%
INFO: 2017-09-06 23:46:04: main.py:142 **  Train=>
Epoch: 13	Batch_num: 180	lr: 0.000	loss: 0.057	acc: 99.130%	train_loss: 0.0547	train_acc: 98.58%
INFO: 2017-09-06 23:46:32: main.py:142 **  Train=>
Epoch: 13	Batch_num: 210	lr: 0.000	loss: 0.065	acc: 98.940%	train_loss: 0.0639	train_acc: 97.94%
INFO: 2017-09-06 23:46:59: main.py:142 **  Train=>
Epoch: 13	Batch_num: 240	lr: 0.000	loss: 0.055	acc: 99.500%	train_loss: 0.0437	train_acc: 98.58%
INFO: 2017-09-06 23:47:27: main.py:142 **  Train=>
Epoch: 13	Batch_num: 270	lr: 0.000	loss: 0.068	acc: 99.040%	train_loss: 0.0523	train_acc: 98.36%
INFO: 2017-09-06 23:47:55: main.py:142 **  Train=>
Epoch: 13	Batch_num: 300	lr: 0.000	loss: 0.072	acc: 99.180%	train_loss: -0.0638	train_acc: 110.81%
INFO: 2017-09-06 23:48:22: main.py:142 **  Train=>
Epoch: 13	Batch_num: 330	lr: 0.000	loss: 0.075	acc: 98.840%	train_loss: 0.1488	train_acc: 97.05%
INFO: 2017-09-06 23:48:50: main.py:142 **  Train=>
Epoch: 13	Batch_num: 360	lr: 0.000	loss: 0.075	acc: 97.990%	train_loss: -0.0544	train_acc: 111.01%
INFO: 2017-09-06 23:49:17: main.py:142 **  Train=>
Epoch: 13	Batch_num: 390	lr: 0.000	loss: 0.051	acc: 99.710%	train_loss: 0.0468	train_acc: 98.23%
INFO: 2017-09-06 23:49:45: main.py:142 **  Train=>
Epoch: 13	Batch_num: 420	lr: 0.000	loss: 0.031	acc: 101.240%	train_loss: 0.0713	train_acc: 96.87%
INFO: 2017-09-06 23:50:12: main.py:142 **  Train=>
Epoch: 13	Batch_num: 450	lr: 0.000	loss: 0.039	acc: 102.240%	train_loss: 0.0471	train_acc: 98.25%
INFO: 2017-09-06 23:50:40: main.py:142 **  Train=>
Epoch: 13	Batch_num: 480	lr: 0.000	loss: 0.095	acc: 98.550%	train_loss: -0.0286	train_acc: 107.77%
INFO: 2017-09-06 23:51:05: main.py:142 **  Train=>
Epoch: 13	Batch_num: 507	lr: 0.000	loss: 0.095	acc: 98.550%	train_loss: 0.068	train_acc: 97.95%
INFO: 2017-09-06 23:51:44: main.py:81 **  Validate=>
Epoch: 13	Valid_loss: 0.043	Valid_acc: 100.520%
INFO: 2017-09-06 23:51:48: main.py:142 **  Train=>
Epoch: 14	Batch_num: 0	lr: 0.000	loss: 0.148	acc: 92.040%	train_loss: 0.1485	train_acc: 92.04%
INFO: 2017-09-06 23:52:16: main.py:142 **  Train=>
Epoch: 14	Batch_num: 30	lr: 0.000	loss: 0.042	acc: 102.850%	train_loss: 0.054	train_acc: 97.72%
INFO: 2017-09-06 23:52:43: main.py:142 **  Train=>
Epoch: 14	Batch_num: 60	lr: 0.000	loss: 0.060	acc: 98.500%	train_loss: 0.0576	train_acc: 98.02%
INFO: 2017-09-06 23:53:11: main.py:142 **  Train=>
Epoch: 14	Batch_num: 90	lr: 0.000	loss: 0.061	acc: 99.580%	train_loss: 0.0642	train_acc: 98.14%
INFO: 2017-09-06 23:53:38: main.py:142 **  Train=>
Epoch: 14	Batch_num: 120	lr: 0.000	loss: 0.046	acc: 100.650%	train_loss: 0.0679	train_acc: 98.13%
INFO: 2017-09-06 23:54:06: main.py:142 **  Train=>
Epoch: 14	Batch_num: 150	lr: 0.000	loss: 0.080	acc: 96.730%	train_loss: 0.0705	train_acc: 97.68%
INFO: 2017-09-06 23:54:33: main.py:142 **  Train=>
Epoch: 14	Batch_num: 180	lr: 0.000	loss: 0.037	acc: 100.980%	train_loss: 0.0402	train_acc: 98.96%
INFO: 2017-09-06 23:55:01: main.py:142 **  Train=>
Epoch: 14	Batch_num: 210	lr: 0.000	loss: 0.057	acc: 99.180%	train_loss: -0.1852	train_acc: 123.09%
INFO: 2017-09-06 23:55:27: main.py:142 **  Train=>
Epoch: 14	Batch_num: 240	lr: 0.000	loss: 0.048	acc: 99.410%	train_loss: -0.2095	train_acc: 123.34%
INFO: 2017-09-06 23:55:53: main.py:142 **  Train=>
Epoch: 14	Batch_num: 270	lr: 0.000	loss: 0.069	acc: 98.110%	train_loss: 0.045	train_acc: 99.05%
INFO: 2017-09-06 23:56:19: main.py:142 **  Train=>
Epoch: 14	Batch_num: 300	lr: 0.000	loss: 0.030	acc: 101.520%	train_loss: 0.0601	train_acc: 98.65%
INFO: 2017-09-06 23:56:45: main.py:142 **  Train=>
Epoch: 14	Batch_num: 330	lr: 0.000	loss: 0.038	acc: 101.310%	train_loss: 0.2068	train_acc: 95.96%
INFO: 2017-09-06 23:57:11: main.py:142 **  Train=>
Epoch: 14	Batch_num: 360	lr: 0.000	loss: 0.155	acc: 97.920%	train_loss: -0.0378	train_acc: 122.75%
INFO: 2017-09-06 23:57:37: main.py:142 **  Train=>
Epoch: 14	Batch_num: 390	lr: 0.000	loss: 0.099	acc: 98.430%	train_loss: 0.1058	train_acc: 97.08%
INFO: 2017-09-06 23:58:02: main.py:142 **  Train=>
Epoch: 14	Batch_num: 420	lr: 0.000	loss: 0.057	acc: 101.080%	train_loss: 0.1747	train_acc: 110.32%
INFO: 2017-09-06 23:58:28: main.py:142 **  Train=>
Epoch: 14	Batch_num: 450	lr: 0.000	loss: 0.056	acc: 99.750%	train_loss: 0.0783	train_acc: 97.27%
INFO: 2017-09-06 23:58:54: main.py:142 **  Train=>
Epoch: 14	Batch_num: 480	lr: 0.000	loss: 0.056	acc: 100.460%	train_loss: 0.0942	train_acc: 94.01%
INFO: 2017-09-06 23:59:17: main.py:142 **  Train=>
Epoch: 14	Batch_num: 507	lr: 0.000	loss: 0.056	acc: 100.460%	train_loss: 0.0886	train_acc: 96.76%
INFO: 2017-09-06 23:59:54: main.py:81 **  Validate=>
Epoch: 14	Valid_loss: 0.041	Valid_acc: 100.240%
INFO: 2017-09-06 23:59:57: main.py:142 **  Train=>
Epoch: 15	Batch_num: 0	lr: 0.000	loss: 0.075	acc: 98.120%	train_loss: 0.0752	train_acc: 98.12%
INFO: 2017-09-07 00:00:23: main.py:142 **  Train=>
Epoch: 15	Batch_num: 30	lr: 0.000	loss: 0.035	acc: 101.690%	train_loss: 0.0812	train_acc: 97.73%
INFO: 2017-09-07 00:00:49: main.py:142 **  Train=>
Epoch: 15	Batch_num: 60	lr: 0.000	loss: 0.052	acc: 100.270%	train_loss: 0.081	train_acc: 95.55%
INFO: 2017-09-07 00:01:15: main.py:142 **  Train=>
Epoch: 15	Batch_num: 90	lr: 0.000	loss: 0.171	acc: 94.320%	train_loss: 0.0863	train_acc: 98.09%
INFO: 2017-09-07 00:01:41: main.py:142 **  Train=>
Epoch: 15	Batch_num: 120	lr: 0.000	loss: 0.084	acc: 99.630%	train_loss: 0.1268	train_acc: 96.37%
INFO: 2017-09-07 00:02:07: main.py:142 **  Train=>
Epoch: 15	Batch_num: 150	lr: 0.000	loss: 0.078	acc: 98.540%	train_loss: 0.1628	train_acc: 86.73%
INFO: 2017-09-07 00:02:32: main.py:142 **  Train=>
Epoch: 15	Batch_num: 180	lr: 0.000	loss: 0.106	acc: 98.260%	train_loss: 0.109	train_acc: 96.61%
INFO: 2017-09-07 00:02:58: main.py:142 **  Train=>
Epoch: 15	Batch_num: 210	lr: 0.000	loss: 0.034	acc: 103.860%	train_loss: -0.0383	train_acc: 108.92%
INFO: 2017-09-07 00:03:24: main.py:142 **  Train=>
Epoch: 15	Batch_num: 240	lr: 0.000	loss: 0.050	acc: 100.150%	train_loss: 0.0598	train_acc: 97.84%
INFO: 2017-09-07 00:03:50: main.py:142 **  Train=>
Epoch: 15	Batch_num: 270	lr: 0.000	loss: 0.051	acc: 99.630%	train_loss: 0.0671	train_acc: 97.7%
INFO: 2017-09-07 00:04:16: main.py:142 **  Train=>
Epoch: 15	Batch_num: 300	lr: 0.000	loss: 0.042	acc: 101.660%	train_loss: 0.0817	train_acc: 97.34%
INFO: 2017-09-07 00:04:41: main.py:142 **  Train=>
Epoch: 15	Batch_num: 330	lr: 0.000	loss: 0.060	acc: 100.330%	train_loss: 0.0669	train_acc: 98.01%
INFO: 2017-09-07 00:05:07: main.py:142 **  Train=>
Epoch: 15	Batch_num: 360	lr: 0.000	loss: 0.074	acc: 98.890%	train_loss: 0.0917	train_acc: 98.03%
INFO: 2017-09-07 00:05:33: main.py:142 **  Train=>
Epoch: 15	Batch_num: 390	lr: 0.000	loss: 0.035	acc: 101.570%	train_loss: 0.0446	train_acc: 98.88%
INFO: 2017-09-07 00:06:00: main.py:142 **  Train=>
Epoch: 15	Batch_num: 420	lr: 0.000	loss: 0.061	acc: 98.290%	train_loss: 0.2005	train_acc: 86.07%
INFO: 2017-09-07 00:06:26: main.py:142 **  Train=>
Epoch: 15	Batch_num: 450	lr: 0.000	loss: 0.048	acc: 99.270%	train_loss: 0.233	train_acc: 80.69%
INFO: 2017-09-07 00:06:52: main.py:142 **  Train=>
Epoch: 15	Batch_num: 480	lr: 0.000	loss: 0.070	acc: 99.190%	train_loss: 0.4606	train_acc: 94.34%
INFO: 2017-09-07 00:07:15: main.py:142 **  Train=>
Epoch: 15	Batch_num: 507	lr: 0.000	loss: 0.070	acc: 99.190%	train_loss: 0.1574	train_acc: 96.94%
INFO: 2017-09-07 00:07:52: main.py:81 **  Validate=>
Epoch: 15	Valid_loss: 0.026	Valid_acc: 102.330%
INFO: 2017-09-07 00:07:56: main.py:142 **  Train=>
Epoch: 16	Batch_num: 0	lr: 0.000	loss: 0.100	acc: 95.670%	train_loss: 0.0998	train_acc: 95.67%
INFO: 2017-09-07 00:08:22: main.py:142 **  Train=>
Epoch: 16	Batch_num: 30	lr: 0.000	loss: 0.050	acc: 99.430%	train_loss: 0.0379	train_acc: 99.02%
INFO: 2017-09-07 00:08:48: main.py:142 **  Train=>
Epoch: 16	Batch_num: 60	lr: 0.000	loss: 0.038	acc: 100.360%	train_loss: -0.0755	train_acc: 110.97%
INFO: 2017-09-07 00:09:13: main.py:142 **  Train=>
Epoch: 16	Batch_num: 90	lr: 0.000	loss: 0.045	acc: 99.910%	train_loss: 0.1075	train_acc: 96.73%
INFO: 2017-09-07 00:09:39: main.py:142 **  Train=>
Epoch: 16	Batch_num: 120	lr: 0.000	loss: 0.029	acc: 101.270%	train_loss: -0.0825	train_acc: 111.44%
INFO: 2017-09-07 00:10:05: main.py:142 **  Train=>
Epoch: 16	Batch_num: 150	lr: 0.000	loss: 0.051	acc: 99.760%	train_loss: 0.077	train_acc: 98.15%
INFO: 2017-09-07 00:10:31: main.py:142 **  Train=>
Epoch: 16	Batch_num: 180	lr: 0.000	loss: 0.067	acc: 97.880%	train_loss: 0.0721	train_acc: 98.2%
INFO: 2017-09-07 00:10:57: main.py:142 **  Train=>
Epoch: 16	Batch_num: 210	lr: 0.000	loss: 0.058	acc: 99.340%	train_loss: -0.0452	train_acc: 110.28%
INFO: 2017-09-07 00:11:23: main.py:142 **  Train=>
Epoch: 16	Batch_num: 240	lr: 0.000	loss: 0.105	acc: 97.940%	train_loss: 0.1046	train_acc: 99.98%
INFO: 2017-09-07 00:11:49: main.py:142 **  Train=>
Epoch: 16	Batch_num: 270	lr: 0.000	loss: 0.228	acc: 95.860%	train_loss: 0.2603	train_acc: 96.76%
INFO: 2017-09-07 00:12:15: main.py:142 **  Train=>
Epoch: 16	Batch_num: 300	lr: 0.000	loss: 0.238	acc: 92.830%	train_loss: 0.122	train_acc: 96.35%
INFO: 2017-09-07 00:12:41: main.py:142 **  Train=>
Epoch: 16	Batch_num: 330	lr: 0.000	loss: 0.132	acc: 98.080%	train_loss: 0.1923	train_acc: 95.51%
INFO: 2017-09-07 00:13:07: main.py:142 **  Train=>
Epoch: 16	Batch_num: 360	lr: 0.000	loss: 0.115	acc: 97.930%	train_loss: 0.1141	train_acc: 96.95%
INFO: 2017-09-07 00:13:32: main.py:142 **  Train=>
Epoch: 16	Batch_num: 390	lr: 0.000	loss: 0.101	acc: 97.450%	train_loss: 0.075	train_acc: 97.55%
INFO: 2017-09-07 00:13:58: main.py:142 **  Train=>
Epoch: 16	Batch_num: 420	lr: 0.000	loss: 0.100	acc: 100.920%	train_loss: 0.6409	train_acc: 86.2%
INFO: 2017-09-07 00:14:24: main.py:142 **  Train=>
Epoch: 16	Batch_num: 450	lr: 0.000	loss: 0.305	acc: 96.100%	train_loss: 0.1241	train_acc: 97.0%
INFO: 2017-09-07 00:14:50: main.py:142 **  Train=>
Epoch: 16	Batch_num: 480	lr: 0.000	loss: 0.268	acc: 94.720%	train_loss: 0.1393	train_acc: 103.26%
INFO: 2017-09-07 00:15:13: main.py:142 **  Train=>
Epoch: 16	Batch_num: 507	lr: 0.000	loss: 0.268	acc: 94.720%	train_loss: 0.1694	train_acc: 94.2%
INFO: 2017-09-07 00:15:50: main.py:81 **  Validate=>
Epoch: 16	Valid_loss: 0.132	Valid_acc: 96.850%
INFO: 2017-09-07 00:15:53: main.py:142 **  Train=>
Epoch: 17	Batch_num: 0	lr: 0.000	loss: 0.095	acc: 97.360%	train_loss: 0.0952	train_acc: 97.36%
INFO: 2017-09-07 00:16:19: main.py:142 **  Train=>
Epoch: 17	Batch_num: 30	lr: 0.000	loss: 0.110	acc: 98.750%	train_loss: -0.0065	train_acc: 108.74%
INFO: 2017-09-07 00:16:45: main.py:142 **  Train=>
Epoch: 17	Batch_num: 60	lr: 0.000	loss: 0.109	acc: 99.580%	train_loss: 0.0801	train_acc: 98.07%
INFO: 2017-09-07 00:17:11: main.py:142 **  Train=>
Epoch: 17	Batch_num: 90	lr: 0.000	loss: 0.119	acc: 97.000%	train_loss: 0.089	train_acc: 97.12%
INFO: 2017-09-07 00:17:36: main.py:142 **  Train=>
Epoch: 17	Batch_num: 120	lr: 0.000	loss: 0.111	acc: 96.660%	train_loss: 0.0977	train_acc: 97.43%
INFO: 2017-09-07 00:18:02: main.py:142 **  Train=>
Epoch: 17	Batch_num: 150	lr: 0.000	loss: 0.093	acc: 98.680%	train_loss: 0.1033	train_acc: 95.43%
INFO: 2017-09-07 00:18:28: main.py:142 **  Train=>
Epoch: 17	Batch_num: 180	lr: 0.000	loss: 0.098	acc: 96.870%	train_loss: 0.2343	train_acc: 89.28%
INFO: 2017-09-07 00:18:54: main.py:142 **  Train=>
Epoch: 17	Batch_num: 210	lr: 0.000	loss: 0.043	acc: 101.020%	train_loss: 0.2878	train_acc: 77.81%
INFO: 2017-09-07 00:19:20: main.py:142 **  Train=>
Epoch: 17	Batch_num: 240	lr: 0.000	loss: 0.061	acc: 101.430%	train_loss: -0.0618	train_acc: 110.69%
INFO: 2017-09-07 00:19:46: main.py:142 **  Train=>
Epoch: 17	Batch_num: 270	lr: 0.000	loss: 0.056	acc: 101.530%	train_loss: 0.0847	train_acc: 98.25%
INFO: 2017-09-07 00:20:12: main.py:142 **  Train=>
Epoch: 17	Batch_num: 300	lr: 0.000	loss: 0.260	acc: 95.280%	train_loss: 0.2091	train_acc: 94.53%
INFO: 2017-09-07 00:20:38: main.py:142 **  Train=>
Epoch: 17	Batch_num: 330	lr: 0.000	loss: 0.142	acc: 97.510%	train_loss: 0.1344	train_acc: 96.51%
INFO: 2017-09-07 00:21:04: main.py:142 **  Train=>
Epoch: 17	Batch_num: 360	lr: 0.000	loss: 0.149	acc: 94.790%	train_loss: 0.3118	train_acc: 82.99%
INFO: 2017-09-07 00:21:29: main.py:142 **  Train=>
Epoch: 17	Batch_num: 390	lr: 0.000	loss: 0.147	acc: 93.840%	train_loss: 0.2036	train_acc: 90.1%
INFO: 2017-09-07 00:21:55: main.py:142 **  Train=>
Epoch: 17	Batch_num: 420	lr: 0.000	loss: 0.110	acc: 100.560%	train_loss: 0.1819	train_acc: 90.12%
INFO: 2017-09-07 00:22:21: main.py:142 **  Train=>
Epoch: 17	Batch_num: 450	lr: 0.000	loss: 0.097	acc: 100.090%	train_loss: 0.1008	train_acc: 95.05%
INFO: 2017-09-07 00:22:47: main.py:142 **  Train=>
Epoch: 17	Batch_num: 480	lr: 0.000	loss: 0.094	acc: 98.320%	train_loss: 0.2051	train_acc: 85.71%
INFO: 2017-09-07 00:23:10: main.py:142 **  Train=>
Epoch: 17	Batch_num: 507	lr: 0.000	loss: 0.094	acc: 98.320%	train_loss: -0.0013	train_acc: 106.72%
INFO: 2017-09-07 00:23:47: main.py:81 **  Validate=>
Epoch: 17	Valid_loss: 0.053	Valid_acc: 99.940%
INFO: 2017-09-07 00:23:49: main.py:142 **  Train=>
Epoch: 18	Batch_num: 0	lr: 0.000	loss: 0.227	acc: 97.030%	train_loss: 0.2272	train_acc: 97.03%
INFO: 2017-09-07 00:24:15: main.py:142 **  Train=>
Epoch: 18	Batch_num: 30	lr: 0.000	loss: 0.053	acc: 101.560%	train_loss: 0.0026	train_acc: 104.41%
INFO: 2017-09-07 00:24:41: main.py:142 **  Train=>
Epoch: 18	Batch_num: 60	lr: 0.000	loss: 0.879	acc: 81.750%	train_loss: 0.8485	train_acc: 72.84%
INFO: 2017-09-07 00:25:07: main.py:142 **  Train=>
Epoch: 18	Batch_num: 90	lr: 0.000	loss: 0.546	acc: 80.850%	train_loss: 0.3701	train_acc: 88.14%
INFO: 2017-09-07 00:25:33: main.py:142 **  Train=>
Epoch: 18	Batch_num: 120	lr: 0.000	loss: 0.426	acc: 89.010%	train_loss: 0.4686	train_acc: 88.93%
INFO: 2017-09-07 00:25:59: main.py:142 **  Train=>
Epoch: 18	Batch_num: 150	lr: 0.000	loss: 0.452	acc: 84.940%	train_loss: 0.4076	train_acc: 89.53%
INFO: 2017-09-11 22:04:55: utils.py:58 **  All dataset size is 5088
INFO: 2017-09-11 22:04:55: utils.py:59 **  Train dataset size is 4071
INFO: 2017-09-11 22:04:55: utils.py:61 **  Valid dataset size is 1017
INFO: 2017-09-11 22:04:55: main.py:166 **  Loading dataset...
INFO: 2017-09-11 22:04:55: main.py:180 **  All data sample counts 5088
INFO: 2017-09-11 22:04:55: main.py:181 **  Train data batch size 8
INFO: 2017-09-11 22:04:55: main.py:182 **  Train data sample counts 4064
INFO: 2017-09-11 22:04:55: main.py:183 **  Valid data sample counts 1018
INFO: 2017-09-11 22:06:56: utils.py:58 **  All dataset size is 5088
INFO: 2017-09-11 22:06:56: utils.py:59 **  Train dataset size is 4071
INFO: 2017-09-11 22:06:56: utils.py:61 **  Valid dataset size is 1017
INFO: 2017-09-11 22:06:56: main.py:166 **  Loading dataset...
INFO: 2017-09-11 22:06:56: main.py:180 **  All data sample counts 5088
INFO: 2017-09-11 22:06:56: main.py:181 **  Train data batch size 8
INFO: 2017-09-11 22:06:56: main.py:182 **  Train data sample counts 4064
INFO: 2017-09-11 22:06:56: main.py:183 **  Valid data sample counts 1018
INFO: 2017-09-11 22:08:11: utils.py:58 **  All dataset size is 5088
INFO: 2017-09-11 22:08:11: utils.py:59 **  Train dataset size is 4071
INFO: 2017-09-11 22:08:11: utils.py:61 **  Valid dataset size is 1017
INFO: 2017-09-11 22:08:11: main.py:166 **  Loading dataset...
INFO: 2017-09-11 22:08:11: main.py:180 **  All data sample counts 5088
INFO: 2017-09-11 22:08:11: main.py:181 **  Train data batch size 8
INFO: 2017-09-11 22:08:11: main.py:182 **  Train data sample counts 4064
INFO: 2017-09-11 22:08:11: main.py:183 **  Valid data sample counts 1018
INFO: 2017-09-11 22:08:40: utils.py:58 **  All dataset size is 5088
INFO: 2017-09-11 22:08:40: utils.py:59 **  Train dataset size is 4071
INFO: 2017-09-11 22:08:40: utils.py:61 **  Valid dataset size is 1017
INFO: 2017-09-11 22:08:40: main.py:166 **  Loading dataset...
INFO: 2017-09-11 22:08:40: main.py:180 **  All data sample counts 5088
INFO: 2017-09-11 22:08:40: main.py:181 **  Train data batch size 8
INFO: 2017-09-11 22:08:40: main.py:182 **  Train data sample counts 4064
INFO: 2017-09-11 22:08:40: main.py:183 **  Valid data sample counts 1018
INFO: 2017-09-11 22:09:52: utils.py:58 **  All dataset size is 5088
INFO: 2017-09-11 22:09:52: utils.py:59 **  Train dataset size is 4071
INFO: 2017-09-11 22:09:52: utils.py:61 **  Valid dataset size is 1017
INFO: 2017-09-11 22:09:52: main.py:166 **  Loading dataset...
INFO: 2017-09-11 22:09:52: main.py:180 **  All data sample counts 5088
INFO: 2017-09-11 22:09:52: main.py:181 **  Train data batch size 8
INFO: 2017-09-11 22:09:52: main.py:182 **  Train data sample counts 4064
INFO: 2017-09-11 22:09:52: main.py:183 **  Valid data sample counts 1018
INFO: 2017-09-11 22:10:37: utils.py:58 **  All dataset size is 5088
INFO: 2017-09-11 22:10:37: utils.py:59 **  Train dataset size is 4071
INFO: 2017-09-11 22:10:37: utils.py:61 **  Valid dataset size is 1017
INFO: 2017-09-11 22:10:37: main.py:166 **  Loading dataset...
INFO: 2017-09-11 22:10:37: main.py:180 **  All data sample counts 5088
INFO: 2017-09-11 22:10:37: main.py:181 **  Train data batch size 8
INFO: 2017-09-11 22:10:37: main.py:182 **  Train data sample counts 4064
INFO: 2017-09-11 22:10:37: main.py:183 **  Valid data sample counts 1018
INFO: 2017-09-11 22:10:41: main.py:142 **  Train=>
Epoch: 0	Batch_num: 0	lr: 0.000	loss: 1.268	acc: 11.990%	train_loss: 1.2675	train_acc: 11.99%
INFO: 2017-09-11 22:11:05: utils.py:58 **  All dataset size is 5088
INFO: 2017-09-11 22:11:05: utils.py:59 **  Train dataset size is 4071
INFO: 2017-09-11 22:11:05: utils.py:61 **  Valid dataset size is 1017
INFO: 2017-09-11 22:11:05: main.py:166 **  Loading dataset...
INFO: 2017-09-11 22:11:05: main.py:180 **  All data sample counts 5088
INFO: 2017-09-11 22:11:05: main.py:181 **  Train data batch size 8
INFO: 2017-09-11 22:11:05: main.py:182 **  Train data sample counts 4064
INFO: 2017-09-11 22:11:05: main.py:183 **  Valid data sample counts 1018
INFO: 2017-09-11 22:11:08: main.py:142 **  Train=>
Epoch: 0	Batch_num: 0	lr: 0.000	loss: 1.280	acc: 54.120%	train_loss: 1.2795	train_acc: 54.12%
INFO: 2017-09-11 22:12:35: utils.py:58 **  All dataset size is 5088
INFO: 2017-09-11 22:12:35: utils.py:59 **  Train dataset size is 4071
INFO: 2017-09-11 22:12:35: utils.py:61 **  Valid dataset size is 1017
INFO: 2017-09-11 22:12:35: main.py:166 **  Loading dataset...
INFO: 2017-09-11 22:12:35: main.py:180 **  All data sample counts 5088
INFO: 2017-09-11 22:12:35: main.py:181 **  Train data batch size 8
INFO: 2017-09-11 22:12:35: main.py:182 **  Train data sample counts 4064
INFO: 2017-09-11 22:12:35: main.py:183 **  Valid data sample counts 1018
INFO: 2017-09-11 22:13:11: utils.py:58 **  All dataset size is 5088
INFO: 2017-09-11 22:13:11: utils.py:59 **  Train dataset size is 4071
INFO: 2017-09-11 22:13:11: utils.py:61 **  Valid dataset size is 1017
INFO: 2017-09-11 22:13:11: main.py:166 **  Loading dataset...
INFO: 2017-09-11 22:13:11: main.py:180 **  All data sample counts 5088
INFO: 2017-09-11 22:13:11: main.py:181 **  Train data batch size 8
INFO: 2017-09-11 22:13:11: main.py:182 **  Train data sample counts 4064
INFO: 2017-09-11 22:13:11: main.py:183 **  Valid data sample counts 1018
INFO: 2017-09-11 22:14:26: utils.py:58 **  All dataset size is 5088
INFO: 2017-09-11 22:14:26: utils.py:59 **  Train dataset size is 4071
INFO: 2017-09-11 22:14:26: utils.py:61 **  Valid dataset size is 1017
INFO: 2017-09-11 22:14:26: main.py:166 **  Loading dataset...
INFO: 2017-09-11 22:14:26: main.py:180 **  All data sample counts 5088
INFO: 2017-09-11 22:14:26: main.py:181 **  Train data batch size 8
INFO: 2017-09-11 22:14:26: main.py:182 **  Train data sample counts 4064
INFO: 2017-09-11 22:14:26: main.py:183 **  Valid data sample counts 1018
INFO: 2017-09-11 22:15:05: utils.py:58 **  All dataset size is 5088
INFO: 2017-09-11 22:15:05: utils.py:59 **  Train dataset size is 4071
INFO: 2017-09-11 22:15:05: utils.py:61 **  Valid dataset size is 1017
INFO: 2017-09-11 22:15:05: main.py:166 **  Loading dataset...
INFO: 2017-09-11 22:15:05: main.py:180 **  All data sample counts 5088
INFO: 2017-09-11 22:15:05: main.py:181 **  Train data batch size 8
INFO: 2017-09-11 22:15:05: main.py:182 **  Train data sample counts 4064
INFO: 2017-09-11 22:15:05: main.py:183 **  Valid data sample counts 1018
INFO: 2017-09-11 22:15:22: utils.py:58 **  All dataset size is 5088
INFO: 2017-09-11 22:15:22: utils.py:59 **  Train dataset size is 4071
INFO: 2017-09-11 22:15:22: utils.py:61 **  Valid dataset size is 1017
INFO: 2017-09-11 22:15:22: main.py:166 **  Loading dataset...
INFO: 2017-09-11 22:15:22: main.py:180 **  All data sample counts 5088
INFO: 2017-09-11 22:15:22: main.py:181 **  Train data batch size 8
INFO: 2017-09-11 22:15:22: main.py:182 **  Train data sample counts 4064
INFO: 2017-09-11 22:15:22: main.py:183 **  Valid data sample counts 1018
INFO: 2017-09-11 22:15:25: main.py:142 **  Train=>
Epoch: 0	Batch_num: 0	lr: 0.000	loss: 1.097	acc: 70.950%	train_loss: 1.0968	train_acc: 70.95%
INFO: 2017-09-12 21:13:24: utils.py:58 **  All dataset size is 5088
INFO: 2017-09-12 21:13:24: utils.py:59 **  Train dataset size is 4071
INFO: 2017-09-12 21:13:24: utils.py:61 **  Valid dataset size is 1017
INFO: 2017-09-12 21:13:24: main.py:215 **  Loading dataset...
INFO: 2017-09-12 21:13:24: main.py:229 **  All data sample counts 5088
INFO: 2017-09-12 21:13:24: main.py:230 **  Train data batch size 8
INFO: 2017-09-12 21:13:24: main.py:231 **  Train data sample counts 4064
INFO: 2017-09-12 21:13:24: main.py:232 **  Valid data sample counts 1018
INFO: 2017-09-12 21:14:13: utils.py:58 **  All dataset size is 5088
INFO: 2017-09-12 21:14:13: utils.py:59 **  Train dataset size is 4071
INFO: 2017-09-12 21:14:13: utils.py:61 **  Valid dataset size is 1017
INFO: 2017-09-12 21:14:13: main.py:215 **  Loading dataset...
INFO: 2017-09-12 21:14:13: main.py:229 **  All data sample counts 5088
INFO: 2017-09-12 21:14:13: main.py:230 **  Train data batch size 8
INFO: 2017-09-12 21:14:13: main.py:231 **  Train data sample counts 4064
INFO: 2017-09-12 21:14:13: main.py:232 **  Valid data sample counts 1018
INFO: 2017-09-12 21:15:30: utils.py:58 **  All dataset size is 5088
INFO: 2017-09-12 21:15:30: utils.py:59 **  Train dataset size is 4071
INFO: 2017-09-12 21:15:30: utils.py:61 **  Valid dataset size is 1017
INFO: 2017-09-12 21:15:30: main.py:215 **  Loading dataset...
INFO: 2017-09-12 21:15:30: main.py:229 **  All data sample counts 5088
INFO: 2017-09-12 21:15:30: main.py:230 **  Train data batch size 8
INFO: 2017-09-12 21:15:30: main.py:231 **  Train data sample counts 4064
INFO: 2017-09-12 21:15:30: main.py:232 **  Valid data sample counts 1018
INFO: 2017-09-12 21:15:50: utils.py:58 **  All dataset size is 5088
INFO: 2017-09-12 21:15:50: utils.py:59 **  Train dataset size is 4071
INFO: 2017-09-12 21:15:50: utils.py:61 **  Valid dataset size is 1017
INFO: 2017-09-12 21:15:50: main.py:215 **  Loading dataset...
INFO: 2017-09-12 21:15:50: main.py:229 **  All data sample counts 5088
INFO: 2017-09-12 21:15:50: main.py:230 **  Train data batch size 8
INFO: 2017-09-12 21:15:50: main.py:231 **  Train data sample counts 4064
INFO: 2017-09-12 21:15:50: main.py:232 **  Valid data sample counts 1018
INFO: 2017-09-12 21:16:52: utils.py:58 **  All dataset size is 5088
INFO: 2017-09-12 21:16:52: utils.py:59 **  Train dataset size is 4071
INFO: 2017-09-12 21:16:52: utils.py:61 **  Valid dataset size is 1017
INFO: 2017-09-12 21:16:52: main.py:215 **  Loading dataset...
INFO: 2017-09-12 21:16:52: main.py:229 **  All data sample counts 5088
INFO: 2017-09-12 21:16:52: main.py:230 **  Train data batch size 8
INFO: 2017-09-12 21:16:52: main.py:231 **  Train data sample counts 4064
INFO: 2017-09-12 21:16:52: main.py:232 **  Valid data sample counts 1018
INFO: 2017-09-12 21:19:48: utils.py:58 **  All dataset size is 5088
INFO: 2017-09-12 21:19:48: utils.py:59 **  Train dataset size is 4071
INFO: 2017-09-12 21:19:48: utils.py:61 **  Valid dataset size is 1017
INFO: 2017-09-12 21:19:48: main.py:216 **  Loading dataset...
INFO: 2017-09-12 21:19:48: main.py:230 **  All data sample counts 5088
INFO: 2017-09-12 21:19:48: main.py:231 **  Train data batch size 8
INFO: 2017-09-12 21:19:48: main.py:232 **  Train data sample counts 4064
INFO: 2017-09-12 21:19:48: main.py:233 **  Valid data sample counts 1018
INFO: 2017-09-12 21:20:33: utils.py:58 **  All dataset size is 5088
INFO: 2017-09-12 21:20:33: utils.py:59 **  Train dataset size is 4071
INFO: 2017-09-12 21:20:33: utils.py:61 **  Valid dataset size is 1017
INFO: 2017-09-12 21:20:33: main.py:216 **  Loading dataset...
INFO: 2017-09-12 21:20:33: main.py:230 **  All data sample counts 5088
INFO: 2017-09-12 21:20:33: main.py:231 **  Train data batch size 8
INFO: 2017-09-12 21:20:33: main.py:232 **  Train data sample counts 8136
INFO: 2017-09-12 21:20:33: main.py:233 **  Valid data sample counts 1018
INFO: 2017-09-12 21:20:36: main.py:144 **  Train=>
Epoch: 0	Batch_num: 0	lr: 0.000	loss: 1.418	acc: 37.420%	train_loss: 1.4177	train_acc: 37.42%
INFO: 2017-09-12 21:22:06: utils.py:58 **  All dataset size is 5088
INFO: 2017-09-12 21:22:06: utils.py:59 **  Train dataset size is 4071
INFO: 2017-09-12 21:22:06: utils.py:61 **  Valid dataset size is 1017
INFO: 2017-09-12 21:22:06: main.py:216 **  Loading dataset...
INFO: 2017-09-12 21:22:06: main.py:230 **  All data sample counts 5088
INFO: 2017-09-12 21:22:06: main.py:231 **  Train data batch size 8
INFO: 2017-09-12 21:22:06: main.py:232 **  Train data sample counts 8136
INFO: 2017-09-12 21:22:06: main.py:233 **  Valid data sample counts 1018
INFO: 2017-09-12 21:22:09: main.py:144 **  Train=>
Epoch: 0	Batch_num: 0	lr: 0.000	loss: 1.368	acc: 33.820%	train_loss: 1.3679	train_acc: 33.82%
INFO: 2017-09-12 21:22:32: main.py:144 **  Train=>
Epoch: 0	Batch_num: 30	lr: 0.000	loss: 0.938	acc: 80.470%	train_loss: 0.6018	train_acc: 90.45%
INFO: 2017-09-12 21:22:56: main.py:144 **  Train=>
Epoch: 0	Batch_num: 60	lr: 0.000	loss: 0.308	acc: 93.760%	train_loss: 0.154	train_acc: 96.13%
INFO: 2017-09-12 21:23:19: main.py:144 **  Train=>
Epoch: 0	Batch_num: 90	lr: 0.000	loss: 0.183	acc: 94.090%	train_loss: 0.1882	train_acc: 93.78%
INFO: 2017-09-12 21:23:44: main.py:144 **  Train=>
Epoch: 0	Batch_num: 120	lr: 0.000	loss: 0.132	acc: 95.560%	train_loss: 0.154	train_acc: 94.13%
INFO: 2017-09-12 21:24:09: main.py:144 **  Train=>
Epoch: 0	Batch_num: 150	lr: 0.000	loss: 0.111	acc: 96.050%	train_loss: 0.0948	train_acc: 96.96%
INFO: 2017-09-12 21:24:35: main.py:144 **  Train=>
Epoch: 0	Batch_num: 180	lr: 0.000	loss: 0.083	acc: 96.990%	train_loss: 0.0601	train_acc: 97.75%
INFO: 2017-09-12 21:25:01: main.py:144 **  Train=>
Epoch: 0	Batch_num: 210	lr: 0.000	loss: 0.071	acc: 97.480%	train_loss: 0.0713	train_acc: 97.39%
INFO: 2017-09-12 21:25:27: main.py:144 **  Train=>
Epoch: 0	Batch_num: 240	lr: 0.000	loss: 0.065	acc: 97.640%	train_loss: 0.0642	train_acc: 97.64%
INFO: 2017-09-12 21:25:53: main.py:144 **  Train=>
Epoch: 0	Batch_num: 270	lr: 0.000	loss: 0.059	acc: 97.780%	train_loss: 0.058	train_acc: 97.76%
INFO: 2017-09-12 21:26:19: main.py:144 **  Train=>
Epoch: 0	Batch_num: 300	lr: 0.000	loss: 0.053	acc: 97.990%	train_loss: 0.0419	train_acc: 98.37%
INFO: 2017-09-12 21:26:45: main.py:144 **  Train=>
Epoch: 0	Batch_num: 330	lr: 0.000	loss: 0.052	acc: 97.990%	train_loss: 0.0526	train_acc: 98.16%
INFO: 2017-09-12 21:27:11: main.py:144 **  Train=>
Epoch: 0	Batch_num: 360	lr: 0.000	loss: 0.049	acc: 98.100%	train_loss: 0.0384	train_acc: 98.57%
INFO: 2017-09-12 21:27:37: main.py:144 **  Train=>
Epoch: 0	Batch_num: 390	lr: 0.000	loss: 0.047	acc: 98.170%	train_loss: 0.0454	train_acc: 98.22%
INFO: 2017-09-12 21:28:04: main.py:144 **  Train=>
Epoch: 0	Batch_num: 420	lr: 0.000	loss: 0.046	acc: 98.170%	train_loss: 0.0703	train_acc: 97.15%
INFO: 2017-09-12 21:28:31: main.py:144 **  Train=>
Epoch: 0	Batch_num: 450	lr: 0.000	loss: 0.052	acc: 97.940%	train_loss: 0.0536	train_acc: 97.84%
INFO: 2017-09-12 21:28:57: main.py:144 **  Train=>
Epoch: 0	Batch_num: 480	lr: 0.000	loss: 0.043	acc: 98.290%	train_loss: 0.0403	train_acc: 98.31%
INFO: 2017-09-12 21:29:23: main.py:144 **  Train=>
Epoch: 0	Batch_num: 510	lr: 0.000	loss: 0.043	acc: 98.230%	train_loss: 0.0375	train_acc: 98.48%
INFO: 2017-09-12 21:29:49: main.py:144 **  Train=>
Epoch: 0	Batch_num: 540	lr: 0.000	loss: 0.042	acc: 98.320%	train_loss: 0.042	train_acc: 98.51%
INFO: 2017-09-12 21:30:15: main.py:144 **  Train=>
Epoch: 0	Batch_num: 570	lr: 0.000	loss: 0.040	acc: 98.360%	train_loss: 0.0379	train_acc: 98.33%
INFO: 2017-09-12 21:30:41: main.py:144 **  Train=>
Epoch: 0	Batch_num: 600	lr: 0.000	loss: 0.039	acc: 98.420%	train_loss: 0.0427	train_acc: 98.06%
INFO: 2017-09-12 21:31:07: main.py:144 **  Train=>
Epoch: 0	Batch_num: 630	lr: 0.000	loss: 0.036	acc: 98.550%	train_loss: 0.0325	train_acc: 98.65%
INFO: 2017-09-12 21:31:33: main.py:144 **  Train=>
Epoch: 0	Batch_num: 660	lr: 0.000	loss: 0.041	acc: 98.380%	train_loss: 0.0462	train_acc: 98.29%
INFO: 2017-09-12 21:32:00: main.py:144 **  Train=>
Epoch: 0	Batch_num: 690	lr: 0.000	loss: 0.038	acc: 98.440%	train_loss: 0.0377	train_acc: 98.4%
INFO: 2017-09-12 21:32:26: main.py:144 **  Train=>
Epoch: 0	Batch_num: 720	lr: 0.000	loss: 0.036	acc: 98.500%	train_loss: 0.0349	train_acc: 98.51%
INFO: 2017-09-12 21:32:52: main.py:144 **  Train=>
Epoch: 0	Batch_num: 750	lr: 0.000	loss: 0.037	acc: 98.490%	train_loss: 0.0355	train_acc: 98.5%
INFO: 2017-09-12 21:33:18: main.py:144 **  Train=>
Epoch: 0	Batch_num: 780	lr: 0.000	loss: 0.037	acc: 98.450%	train_loss: 0.0387	train_acc: 98.25%
INFO: 2017-09-12 21:33:44: main.py:144 **  Train=>
Epoch: 0	Batch_num: 810	lr: 0.000	loss: 0.035	acc: 98.540%	train_loss: 0.0473	train_acc: 97.99%
INFO: 2017-09-12 21:34:10: main.py:144 **  Train=>
Epoch: 0	Batch_num: 840	lr: 0.000	loss: 0.034	acc: 98.550%	train_loss: 0.0449	train_acc: 98.07%
INFO: 2017-09-12 21:34:36: main.py:144 **  Train=>
Epoch: 0	Batch_num: 870	lr: 0.000	loss: 0.033	acc: 98.610%	train_loss: 0.0301	train_acc: 98.66%
INFO: 2017-09-12 21:35:02: main.py:144 **  Train=>
Epoch: 0	Batch_num: 900	lr: 0.000	loss: 0.036	acc: 98.490%	train_loss: 0.0273	train_acc: 98.85%
INFO: 2017-09-12 21:35:28: main.py:144 **  Train=>
Epoch: 0	Batch_num: 930	lr: 0.000	loss: 0.038	acc: 98.390%	train_loss: 0.0342	train_acc: 98.56%
INFO: 2017-09-12 21:35:55: main.py:144 **  Train=>
Epoch: 0	Batch_num: 960	lr: 0.000	loss: 0.035	acc: 98.550%	train_loss: 0.0422	train_acc: 98.22%
INFO: 2017-09-12 21:36:21: main.py:144 **  Train=>
Epoch: 0	Batch_num: 990	lr: 0.000	loss: 0.035	acc: 98.550%	train_loss: 0.0397	train_acc: 98.44%
INFO: 2017-09-12 21:36:43: main.py:144 **  Train=>
Epoch: 0	Batch_num: 1016	lr: 0.000	loss: 0.035	acc: 98.550%	train_loss: 0.0383	train_acc: 98.4%
INFO: 2017-09-12 21:37:47: main.py:83 **  Validate=>
Epoch: 0	Valid_loss: 0.042	Valid_acc: 98.250%
INFO: 2017-09-12 21:37:51: main.py:144 **  Train=>
Epoch: 1	Batch_num: 0	lr: 0.000	loss: 0.031	acc: 98.710%	train_loss: 0.0313	train_acc: 98.71%
INFO: 2017-09-12 21:38:17: main.py:144 **  Train=>
Epoch: 1	Batch_num: 30	lr: 0.000	loss: 0.034	acc: 98.570%	train_loss: 0.0331	train_acc: 98.57%
INFO: 2017-09-12 21:38:43: main.py:144 **  Train=>
Epoch: 1	Batch_num: 60	lr: 0.000	loss: 0.031	acc: 98.630%	train_loss: 0.0287	train_acc: 98.76%
INFO: 2017-09-12 21:39:09: main.py:144 **  Train=>
Epoch: 1	Batch_num: 90	lr: 0.000	loss: 0.033	acc: 98.610%	train_loss: 0.0331	train_acc: 98.56%
INFO: 2017-09-12 21:39:35: main.py:144 **  Train=>
Epoch: 1	Batch_num: 120	lr: 0.000	loss: 0.033	acc: 98.610%	train_loss: 0.0285	train_acc: 98.73%
INFO: 2017-09-12 21:40:01: main.py:144 **  Train=>
Epoch: 1	Batch_num: 150	lr: 0.000	loss: 0.033	acc: 98.600%	train_loss: 0.0372	train_acc: 98.57%
INFO: 2017-09-12 21:40:27: main.py:144 **  Train=>
Epoch: 1	Batch_num: 180	lr: 0.000	loss: 0.032	acc: 98.640%	train_loss: 0.0294	train_acc: 98.64%
INFO: 2017-09-12 21:40:53: main.py:144 **  Train=>
Epoch: 1	Batch_num: 210	lr: 0.000	loss: 0.033	acc: 98.600%	train_loss: 0.0326	train_acc: 98.59%
INFO: 2017-09-12 21:41:20: main.py:144 **  Train=>
Epoch: 1	Batch_num: 240	lr: 0.000	loss: 0.031	acc: 98.670%	train_loss: 0.0336	train_acc: 98.64%
INFO: 2017-09-12 21:41:46: main.py:144 **  Train=>
Epoch: 1	Batch_num: 270	lr: 0.000	loss: 0.030	acc: 98.660%	train_loss: 0.0314	train_acc: 98.6%
INFO: 2017-09-12 21:42:12: main.py:144 **  Train=>
Epoch: 1	Batch_num: 300	lr: 0.000	loss: 0.029	acc: 98.710%	train_loss: 0.036	train_acc: 98.33%
INFO: 2017-09-12 21:42:38: main.py:144 **  Train=>
Epoch: 1	Batch_num: 330	lr: 0.000	loss: 0.033	acc: 98.570%	train_loss: 0.0367	train_acc: 98.6%
INFO: 2017-09-12 21:43:04: main.py:144 **  Train=>
Epoch: 1	Batch_num: 360	lr: 0.000	loss: 0.031	acc: 98.630%	train_loss: 0.0278	train_acc: 98.85%
INFO: 2017-09-12 21:43:31: main.py:144 **  Train=>
Epoch: 1	Batch_num: 390	lr: 0.000	loss: 0.031	acc: 98.640%	train_loss: 0.0296	train_acc: 98.7%
INFO: 2017-09-12 21:43:57: main.py:144 **  Train=>
Epoch: 1	Batch_num: 420	lr: 0.000	loss: 0.031	acc: 98.630%	train_loss: 0.0409	train_acc: 98.21%
INFO: 2017-09-12 21:44:22: main.py:144 **  Train=>
Epoch: 1	Batch_num: 450	lr: 0.000	loss: 0.033	acc: 98.580%	train_loss: 0.0373	train_acc: 98.45%
INFO: 2017-09-12 21:44:48: main.py:144 **  Train=>
Epoch: 1	Batch_num: 480	lr: 0.000	loss: 0.029	acc: 98.690%	train_loss: 0.0283	train_acc: 98.67%
INFO: 2017-09-12 21:45:14: main.py:144 **  Train=>
Epoch: 1	Batch_num: 510	lr: 0.000	loss: 0.030	acc: 98.640%	train_loss: 0.0268	train_acc: 98.76%
INFO: 2017-09-12 21:45:40: main.py:144 **  Train=>
Epoch: 1	Batch_num: 540	lr: 0.000	loss: 0.030	acc: 98.660%	train_loss: 0.0314	train_acc: 98.77%
INFO: 2017-09-12 21:46:06: main.py:144 **  Train=>
Epoch: 1	Batch_num: 570	lr: 0.000	loss: 0.029	acc: 98.700%	train_loss: 0.0276	train_acc: 98.67%
INFO: 2017-09-12 21:46:32: main.py:144 **  Train=>
Epoch: 1	Batch_num: 600	lr: 0.000	loss: 0.029	acc: 98.720%	train_loss: 0.0321	train_acc: 98.42%
INFO: 2017-09-12 21:46:58: main.py:144 **  Train=>
Epoch: 1	Batch_num: 630	lr: 0.000	loss: 0.029	acc: 98.750%	train_loss: 0.0242	train_acc: 98.83%
INFO: 2017-09-12 21:47:24: main.py:144 **  Train=>
Epoch: 1	Batch_num: 660	lr: 0.000	loss: 0.030	acc: 98.660%	train_loss: 0.038	train_acc: 98.54%
INFO: 2017-09-12 21:47:50: main.py:144 **  Train=>
Epoch: 1	Batch_num: 690	lr: 0.000	loss: 0.031	acc: 98.640%	train_loss: 0.0272	train_acc: 98.73%
INFO: 2017-09-12 21:48:16: main.py:144 **  Train=>
Epoch: 1	Batch_num: 720	lr: 0.000	loss: 0.028	acc: 98.710%	train_loss: 0.0293	train_acc: 98.63%
INFO: 2017-09-12 21:48:42: main.py:144 **  Train=>
Epoch: 1	Batch_num: 750	lr: 0.000	loss: 0.028	acc: 98.750%	train_loss: 0.0289	train_acc: 98.68%
INFO: 2017-09-12 21:49:08: main.py:144 **  Train=>
Epoch: 1	Batch_num: 780	lr: 0.000	loss: 0.029	acc: 98.700%	train_loss: 0.0288	train_acc: 98.61%
INFO: 2017-09-12 21:49:35: main.py:144 **  Train=>
Epoch: 1	Batch_num: 810	lr: 0.000	loss: 0.028	acc: 98.740%	train_loss: 0.0337	train_acc: 98.4%
INFO: 2017-09-12 21:50:01: main.py:144 **  Train=>
Epoch: 1	Batch_num: 840	lr: 0.000	loss: 0.027	acc: 98.740%	train_loss: 0.0299	train_acc: 98.66%
INFO: 2017-09-12 21:50:27: main.py:144 **  Train=>
Epoch: 1	Batch_num: 870	lr: 0.000	loss: 0.027	acc: 98.760%	train_loss: 0.0241	train_acc: 98.88%
INFO: 2017-09-12 21:50:53: main.py:144 **  Train=>
Epoch: 1	Batch_num: 900	lr: 0.000	loss: 0.027	acc: 98.750%	train_loss: 0.0244	train_acc: 98.88%
INFO: 2017-09-12 21:51:18: main.py:144 **  Train=>
Epoch: 1	Batch_num: 930	lr: 0.000	loss: 0.028	acc: 98.730%	train_loss: 0.0256	train_acc: 98.81%
INFO: 2017-09-12 21:51:44: main.py:144 **  Train=>
Epoch: 1	Batch_num: 960	lr: 0.000	loss: 0.028	acc: 98.740%	train_loss: 0.0295	train_acc: 98.61%
INFO: 2017-09-12 21:52:10: main.py:144 **  Train=>
Epoch: 1	Batch_num: 990	lr: 0.000	loss: 0.028	acc: 98.750%	train_loss: 0.0277	train_acc: 98.81%
INFO: 2017-09-12 21:52:33: main.py:144 **  Train=>
Epoch: 1	Batch_num: 1016	lr: 0.000	loss: 0.028	acc: 98.750%	train_loss: 0.0284	train_acc: 98.68%
INFO: 2017-09-12 21:53:36: main.py:83 **  Validate=>
Epoch: 1	Valid_loss: 0.035	Valid_acc: 98.490%
INFO: 2017-09-12 21:53:40: main.py:144 **  Train=>
Epoch: 2	Batch_num: 0	lr: 0.000	loss: 0.025	acc: 98.830%	train_loss: 0.0251	train_acc: 98.83%
INFO: 2017-09-12 21:54:06: main.py:144 **  Train=>
Epoch: 2	Batch_num: 30	lr: 0.000	loss: 0.027	acc: 98.760%	train_loss: 0.0257	train_acc: 98.81%
INFO: 2017-09-12 21:54:32: main.py:144 **  Train=>
Epoch: 2	Batch_num: 60	lr: 0.000	loss: 0.026	acc: 98.780%	train_loss: 0.0246	train_acc: 98.83%
INFO: 2017-09-12 21:54:58: main.py:144 **  Train=>
Epoch: 2	Batch_num: 90	lr: 0.000	loss: 0.027	acc: 98.790%	train_loss: 0.0257	train_acc: 98.81%
INFO: 2017-09-12 21:55:24: main.py:144 **  Train=>
Epoch: 2	Batch_num: 120	lr: 0.000	loss: 0.027	acc: 98.780%	train_loss: 0.026	train_acc: 98.82%
INFO: 2017-09-12 21:55:50: main.py:144 **  Train=>
Epoch: 2	Batch_num: 150	lr: 0.000	loss: 0.028	acc: 98.740%	train_loss: 0.0293	train_acc: 98.8%
INFO: 2017-09-12 21:56:16: main.py:144 **  Train=>
Epoch: 2	Batch_num: 180	lr: 0.000	loss: 0.027	acc: 98.780%	train_loss: 0.0254	train_acc: 98.75%
INFO: 2017-09-12 21:56:42: main.py:144 **  Train=>
Epoch: 2	Batch_num: 210	lr: 0.000	loss: 0.028	acc: 98.750%	train_loss: 0.0271	train_acc: 98.72%
INFO: 2017-09-12 21:57:08: main.py:144 **  Train=>
Epoch: 2	Batch_num: 240	lr: 0.000	loss: 0.027	acc: 98.780%	train_loss: 0.0289	train_acc: 98.77%
INFO: 2017-09-12 21:57:34: main.py:144 **  Train=>
Epoch: 2	Batch_num: 270	lr: 0.000	loss: 0.027	acc: 98.760%	train_loss: 0.0281	train_acc: 98.74%
INFO: 2017-09-12 21:58:00: main.py:144 **  Train=>
Epoch: 2	Batch_num: 300	lr: 0.000	loss: 0.025	acc: 98.800%	train_loss: 0.03	train_acc: 98.53%
INFO: 2017-09-12 21:58:26: main.py:144 **  Train=>
Epoch: 2	Batch_num: 330	lr: 0.000	loss: 0.027	acc: 98.730%	train_loss: 0.0277	train_acc: 98.84%
INFO: 2017-09-12 21:58:52: main.py:144 **  Train=>
Epoch: 2	Batch_num: 360	lr: 0.000	loss: 0.026	acc: 98.790%	train_loss: 0.0246	train_acc: 98.87%
INFO: 2017-09-12 21:59:18: main.py:144 **  Train=>
Epoch: 2	Batch_num: 390	lr: 0.000	loss: 0.027	acc: 98.750%	train_loss: 0.0246	train_acc: 98.86%
INFO: 2017-09-12 21:59:44: main.py:144 **  Train=>
Epoch: 2	Batch_num: 420	lr: 0.000	loss: 0.027	acc: 98.730%	train_loss: 0.0303	train_acc: 98.6%
INFO: 2017-09-12 22:00:10: main.py:144 **  Train=>
Epoch: 2	Batch_num: 450	lr: 0.000	loss: 0.028	acc: 98.730%	train_loss: 0.0319	train_acc: 98.61%
INFO: 2017-09-12 22:00:36: main.py:144 **  Train=>
Epoch: 2	Batch_num: 480	lr: 0.000	loss: 0.026	acc: 98.770%	train_loss: 0.0238	train_acc: 98.78%
INFO: 2017-09-12 22:01:02: main.py:144 **  Train=>
Epoch: 2	Batch_num: 510	lr: 0.000	loss: 0.026	acc: 98.770%	train_loss: 0.0231	train_acc: 98.87%
INFO: 2017-09-12 22:01:28: main.py:144 **  Train=>
Epoch: 2	Batch_num: 540	lr: 0.000	loss: 0.026	acc: 98.790%	train_loss: 0.0287	train_acc: 98.77%
INFO: 2017-09-12 22:01:54: main.py:144 **  Train=>
Epoch: 2	Batch_num: 570	lr: 0.000	loss: 0.025	acc: 98.810%	train_loss: 0.0246	train_acc: 98.7%
INFO: 2017-09-12 22:02:20: main.py:144 **  Train=>
Epoch: 2	Batch_num: 600	lr: 0.000	loss: 0.025	acc: 98.800%	train_loss: 0.0262	train_acc: 98.67%
INFO: 2017-09-12 22:02:46: main.py:144 **  Train=>
Epoch: 2	Batch_num: 630	lr: 0.000	loss: 0.026	acc: 98.830%	train_loss: 0.023	train_acc: 98.85%
INFO: 2017-09-12 22:03:12: main.py:144 **  Train=>
Epoch: 2	Batch_num: 660	lr: 0.000	loss: 0.026	acc: 98.780%	train_loss: 0.0272	train_acc: 98.88%
INFO: 2017-09-12 22:03:39: main.py:144 **  Train=>
Epoch: 2	Batch_num: 690	lr: 0.000	loss: 0.025	acc: 98.810%	train_loss: 0.0245	train_acc: 98.81%
INFO: 2017-09-12 22:04:05: main.py:144 **  Train=>
Epoch: 2	Batch_num: 720	lr: 0.000	loss: 0.025	acc: 98.820%	train_loss: 0.03	train_acc: 98.6%
INFO: 2017-09-12 22:04:31: main.py:144 **  Train=>
Epoch: 2	Batch_num: 750	lr: 0.000	loss: 0.025	acc: 98.810%	train_loss: 0.0264	train_acc: 98.72%
INFO: 2017-09-12 22:04:57: main.py:144 **  Train=>
Epoch: 2	Batch_num: 780	lr: 0.000	loss: 0.025	acc: 98.820%	train_loss: 0.025	train_acc: 98.75%
INFO: 2017-09-12 22:05:23: main.py:144 **  Train=>
Epoch: 2	Batch_num: 810	lr: 0.000	loss: 0.026	acc: 98.790%	train_loss: 0.0285	train_acc: 98.55%
INFO: 2017-09-12 22:05:49: main.py:144 **  Train=>
Epoch: 2	Batch_num: 840	lr: 0.000	loss: 0.025	acc: 98.810%	train_loss: 0.027	train_acc: 98.76%
INFO: 2017-09-12 22:06:15: main.py:144 **  Train=>
Epoch: 2	Batch_num: 870	lr: 0.000	loss: 0.025	acc: 98.820%	train_loss: 0.0214	train_acc: 98.97%
INFO: 2017-09-12 22:06:41: main.py:144 **  Train=>
Epoch: 2	Batch_num: 900	lr: 0.000	loss: 0.024	acc: 98.830%	train_loss: 0.0221	train_acc: 98.95%
INFO: 2017-09-12 22:07:07: main.py:144 **  Train=>
Epoch: 2	Batch_num: 930	lr: 0.000	loss: 0.025	acc: 98.810%	train_loss: 0.0234	train_acc: 98.86%
INFO: 2017-09-12 22:07:34: main.py:144 **  Train=>
Epoch: 2	Batch_num: 960	lr: 0.000	loss: 0.025	acc: 98.830%	train_loss: 0.0258	train_acc: 98.75%
INFO: 2017-09-12 22:08:00: main.py:144 **  Train=>
Epoch: 2	Batch_num: 990	lr: 0.000	loss: 0.025	acc: 98.820%	train_loss: 0.0258	train_acc: 98.86%
INFO: 2017-09-12 22:08:23: main.py:144 **  Train=>
Epoch: 2	Batch_num: 1016	lr: 0.000	loss: 0.025	acc: 98.820%	train_loss: 0.0234	train_acc: 98.89%
INFO: 2017-09-12 22:09:26: main.py:83 **  Validate=>
Epoch: 2	Valid_loss: 0.034	Valid_acc: 98.520%
INFO: 2017-09-12 22:09:30: main.py:144 **  Train=>
Epoch: 3	Batch_num: 0	lr: 0.000	loss: 0.023	acc: 98.900%	train_loss: 0.0228	train_acc: 98.9%
INFO: 2017-09-12 22:09:55: main.py:144 **  Train=>
Epoch: 3	Batch_num: 30	lr: 0.000	loss: 0.024	acc: 98.840%	train_loss: 0.0236	train_acc: 98.89%
INFO: 2017-09-12 22:10:21: main.py:144 **  Train=>
Epoch: 3	Batch_num: 60	lr: 0.000	loss: 0.024	acc: 98.840%	train_loss: 0.0228	train_acc: 98.89%
INFO: 2017-09-12 22:10:48: main.py:144 **  Train=>
Epoch: 3	Batch_num: 90	lr: 0.000	loss: 0.024	acc: 98.860%	train_loss: 0.0227	train_acc: 98.91%
INFO: 2017-09-12 22:11:14: main.py:144 **  Train=>
Epoch: 3	Batch_num: 120	lr: 0.000	loss: 0.025	acc: 98.830%	train_loss: 0.0233	train_acc: 98.89%
INFO: 2017-09-12 22:11:40: main.py:144 **  Train=>
Epoch: 3	Batch_num: 150	lr: 0.000	loss: 0.026	acc: 98.800%	train_loss: 0.0268	train_acc: 98.9%
INFO: 2017-09-12 22:12:06: main.py:144 **  Train=>
Epoch: 3	Batch_num: 180	lr: 0.000	loss: 0.024	acc: 98.860%	train_loss: 0.0229	train_acc: 98.84%
INFO: 2017-09-12 22:12:32: main.py:144 **  Train=>
Epoch: 3	Batch_num: 210	lr: 0.000	loss: 0.025	acc: 98.820%	train_loss: 0.0247	train_acc: 98.81%
INFO: 2017-09-12 22:12:58: main.py:144 **  Train=>
Epoch: 3	Batch_num: 240	lr: 0.000	loss: 0.025	acc: 98.840%	train_loss: 0.0251	train_acc: 98.9%
INFO: 2017-09-12 22:13:24: main.py:144 **  Train=>
Epoch: 3	Batch_num: 270	lr: 0.000	loss: 0.024	acc: 98.830%	train_loss: 0.026	train_acc: 98.79%
INFO: 2017-09-12 22:13:50: main.py:144 **  Train=>
Epoch: 3	Batch_num: 300	lr: 0.000	loss: 0.023	acc: 98.870%	train_loss: 0.0225	train_acc: 98.87%
INFO: 2017-09-12 22:14:16: main.py:144 **  Train=>
Epoch: 3	Batch_num: 330	lr: 0.000	loss: 0.024	acc: 98.840%	train_loss: 0.0261	train_acc: 98.88%
INFO: 2017-09-12 22:14:42: main.py:144 **  Train=>
Epoch: 3	Batch_num: 360	lr: 0.000	loss: 0.024	acc: 98.860%	train_loss: 0.0228	train_acc: 98.9%
INFO: 2017-09-12 22:15:08: main.py:144 **  Train=>
Epoch: 3	Batch_num: 390	lr: 0.000	loss: 0.025	acc: 98.830%	train_loss: 0.0227	train_acc: 98.91%
INFO: 2017-09-12 22:15:35: main.py:144 **  Train=>
Epoch: 3	Batch_num: 420	lr: 0.000	loss: 0.025	acc: 98.790%	train_loss: 0.0274	train_acc: 98.68%
INFO: 2017-09-12 22:16:01: main.py:144 **  Train=>
Epoch: 3	Batch_num: 450	lr: 0.000	loss: 0.025	acc: 98.810%	train_loss: 0.0305	train_acc: 98.65%
INFO: 2017-09-12 22:16:27: main.py:144 **  Train=>
Epoch: 3	Batch_num: 480	lr: 0.000	loss: 0.024	acc: 98.840%	train_loss: 0.0225	train_acc: 98.84%
INFO: 2017-09-12 22:16:53: main.py:144 **  Train=>
Epoch: 3	Batch_num: 510	lr: 0.000	loss: 0.024	acc: 98.840%	train_loss: 0.0225	train_acc: 98.9%
INFO: 2017-09-12 22:17:19: main.py:144 **  Train=>
Epoch: 3	Batch_num: 540	lr: 0.000	loss: 0.024	acc: 98.860%	train_loss: 0.0253	train_acc: 98.88%
INFO: 2017-09-12 22:17:45: main.py:144 **  Train=>
Epoch: 3	Batch_num: 570	lr: 0.000	loss: 0.023	acc: 98.880%	train_loss: 0.0225	train_acc: 98.79%
INFO: 2017-09-12 22:18:11: main.py:144 **  Train=>
Epoch: 3	Batch_num: 600	lr: 0.000	loss: 0.023	acc: 98.870%	train_loss: 0.0255	train_acc: 98.69%
INFO: 2017-09-12 22:18:37: main.py:144 **  Train=>
Epoch: 3	Batch_num: 630	lr: 0.000	loss: 0.024	acc: 98.890%	train_loss: 0.0212	train_acc: 98.92%
INFO: 2017-09-12 22:19:03: main.py:144 **  Train=>
Epoch: 3	Batch_num: 660	lr: 0.000	loss: 0.024	acc: 98.860%	train_loss: 0.0255	train_acc: 98.91%
INFO: 2017-09-12 22:19:30: main.py:144 **  Train=>
Epoch: 3	Batch_num: 690	lr: 0.000	loss: 0.024	acc: 98.870%	train_loss: 0.0225	train_acc: 98.89%
INFO: 2017-09-12 22:19:56: main.py:144 **  Train=>
Epoch: 3	Batch_num: 720	lr: 0.000	loss: 0.023	acc: 98.880%	train_loss: 0.0269	train_acc: 98.74%
INFO: 2017-09-12 22:20:22: main.py:144 **  Train=>
Epoch: 3	Batch_num: 750	lr: 0.000	loss: 0.023	acc: 98.900%	train_loss: 0.0241	train_acc: 98.83%
INFO: 2017-09-12 22:20:48: main.py:144 **  Train=>
Epoch: 3	Batch_num: 780	lr: 0.000	loss: 0.023	acc: 98.910%	train_loss: 0.0218	train_acc: 98.91%
INFO: 2017-09-12 22:21:14: main.py:144 **  Train=>
Epoch: 3	Batch_num: 810	lr: 0.000	loss: 0.022	acc: 98.940%	train_loss: 0.0232	train_acc: 98.84%
INFO: 2017-09-12 22:21:40: main.py:144 **  Train=>
Epoch: 3	Batch_num: 840	lr: 0.000	loss: 0.022	acc: 98.950%	train_loss: 0.0241	train_acc: 98.91%
INFO: 2017-09-12 22:22:06: main.py:144 **  Train=>
Epoch: 3	Batch_num: 870	lr: 0.000	loss: 0.023	acc: 98.960%	train_loss: 0.0198	train_acc: 99.04%
INFO: 2017-09-12 22:22:32: main.py:144 **  Train=>
Epoch: 3	Batch_num: 900	lr: 0.000	loss: 0.022	acc: 98.970%	train_loss: 0.0208	train_acc: 99.04%
INFO: 2017-09-12 22:22:58: main.py:144 **  Train=>
Epoch: 3	Batch_num: 930	lr: 0.000	loss: 0.023	acc: 98.960%	train_loss: 0.0222	train_acc: 98.96%
INFO: 2017-09-12 22:23:24: main.py:144 **  Train=>
Epoch: 3	Batch_num: 960	lr: 0.000	loss: 0.023	acc: 98.960%	train_loss: 0.0235	train_acc: 98.89%
INFO: 2017-09-12 22:23:50: main.py:144 **  Train=>
Epoch: 3	Batch_num: 990	lr: 0.000	loss: 0.023	acc: 98.970%	train_loss: 0.0243	train_acc: 98.93%
INFO: 2017-09-12 22:24:13: main.py:144 **  Train=>
Epoch: 3	Batch_num: 1016	lr: 0.000	loss: 0.023	acc: 98.970%	train_loss: 0.023	train_acc: 98.93%
INFO: 2017-09-12 22:25:16: main.py:83 **  Validate=>
Epoch: 3	Valid_loss: 0.023	Valid_acc: 98.940%
INFO: 2017-09-12 22:25:20: main.py:144 **  Train=>
Epoch: 4	Batch_num: 0	lr: 0.000	loss: 0.021	acc: 99.090%	train_loss: 0.0209	train_acc: 99.09%
INFO: 2017-09-12 22:25:46: main.py:144 **  Train=>
Epoch: 4	Batch_num: 30	lr: 0.000	loss: 0.022	acc: 98.990%	train_loss: 0.0204	train_acc: 99.04%
INFO: 2017-09-12 22:26:12: main.py:144 **  Train=>
Epoch: 4	Batch_num: 60	lr: 0.000	loss: 0.022	acc: 98.990%	train_loss: 0.021	train_acc: 99.01%
INFO: 2017-09-12 22:26:38: main.py:144 **  Train=>
Epoch: 4	Batch_num: 90	lr: 0.000	loss: 0.022	acc: 99.000%	train_loss: 0.0218	train_acc: 98.98%
INFO: 2017-09-12 22:27:05: main.py:144 **  Train=>
Epoch: 4	Batch_num: 120	lr: 0.000	loss: 0.023	acc: 98.990%	train_loss: 0.0214	train_acc: 99.04%
INFO: 2017-09-12 22:27:31: main.py:144 **  Train=>
Epoch: 4	Batch_num: 150	lr: 0.000	loss: 0.023	acc: 98.960%	train_loss: 0.0243	train_acc: 99.0%
INFO: 2017-09-12 22:27:57: main.py:144 **  Train=>
Epoch: 4	Batch_num: 180	lr: 0.000	loss: 0.022	acc: 99.000%	train_loss: 0.0211	train_acc: 98.98%
INFO: 2017-09-12 22:28:23: main.py:144 **  Train=>
Epoch: 4	Batch_num: 210	lr: 0.000	loss: 0.022	acc: 98.990%	train_loss: 0.0218	train_acc: 99.04%
INFO: 2017-09-12 22:28:49: main.py:144 **  Train=>
Epoch: 4	Batch_num: 240	lr: 0.000	loss: 0.022	acc: 98.990%	train_loss: 0.0237	train_acc: 99.02%
INFO: 2017-09-12 22:29:15: main.py:144 **  Train=>
Epoch: 4	Batch_num: 270	lr: 0.000	loss: 0.022	acc: 98.980%	train_loss: 0.0242	train_acc: 98.89%
INFO: 2017-09-12 22:29:41: main.py:144 **  Train=>
Epoch: 4	Batch_num: 300	lr: 0.000	loss: 0.021	acc: 99.040%	train_loss: 0.0203	train_acc: 99.03%
INFO: 2017-09-12 22:30:08: main.py:144 **  Train=>
Epoch: 4	Batch_num: 330	lr: 0.000	loss: 0.022	acc: 99.010%	train_loss: 0.0241	train_acc: 99.04%
INFO: 2017-09-12 22:30:34: main.py:144 **  Train=>
Epoch: 4	Batch_num: 360	lr: 0.000	loss: 0.022	acc: 99.000%	train_loss: 0.0205	train_acc: 99.12%
INFO: 2017-09-12 22:31:00: main.py:144 **  Train=>
Epoch: 4	Batch_num: 390	lr: 0.000	loss: 0.023	acc: 98.970%	train_loss: 0.0222	train_acc: 99.02%
INFO: 2017-09-12 22:31:26: main.py:144 **  Train=>
Epoch: 4	Batch_num: 420	lr: 0.000	loss: 0.032	acc: 98.660%	train_loss: 0.0548	train_acc: 97.49%
INFO: 2017-09-12 22:31:52: main.py:144 **  Train=>
Epoch: 4	Batch_num: 450	lr: 0.000	loss: 0.083	acc: 97.020%	train_loss: 0.1201	train_acc: 95.01%
INFO: 2017-09-12 22:32:18: main.py:144 **  Train=>
Epoch: 4	Batch_num: 480	lr: 0.000	loss: 0.089	acc: 96.520%	train_loss: 0.065	train_acc: 96.97%
INFO: 2017-09-12 22:32:44: main.py:144 **  Train=>
Epoch: 4	Batch_num: 510	lr: 0.000	loss: 0.096	acc: 96.270%	train_loss: 0.0818	train_acc: 96.78%
INFO: 2017-09-12 22:33:10: main.py:144 **  Train=>
Epoch: 4	Batch_num: 540	lr: 0.000	loss: 0.048	acc: 98.040%	train_loss: 0.0482	train_acc: 98.23%
INFO: 2017-09-12 22:33:36: main.py:144 **  Train=>
Epoch: 4	Batch_num: 570	lr: 0.000	loss: 0.046	acc: 98.040%	train_loss: 0.043	train_acc: 97.86%
INFO: 2017-09-12 22:34:02: main.py:144 **  Train=>
Epoch: 4	Batch_num: 600	lr: 0.000	loss: 0.041	acc: 98.260%	train_loss: 0.0349	train_acc: 98.22%
INFO: 2017-09-12 22:34:29: main.py:144 **  Train=>
Epoch: 4	Batch_num: 630	lr: 0.000	loss: 0.035	acc: 98.520%	train_loss: 0.0299	train_acc: 98.65%
INFO: 2017-09-12 22:34:55: main.py:144 **  Train=>
Epoch: 4	Batch_num: 660	lr: 0.000	loss: 0.038	acc: 98.380%	train_loss: 0.0448	train_acc: 98.23%
INFO: 2017-09-12 22:35:21: main.py:144 **  Train=>
Epoch: 4	Batch_num: 690	lr: 0.000	loss: 0.035	acc: 98.470%	train_loss: 0.0338	train_acc: 98.44%
INFO: 2017-09-12 22:35:47: main.py:144 **  Train=>
Epoch: 4	Batch_num: 720	lr: 0.000	loss: 0.032	acc: 98.540%	train_loss: 0.0343	train_acc: 98.42%
INFO: 2017-09-12 22:36:13: main.py:144 **  Train=>
Epoch: 4	Batch_num: 750	lr: 0.000	loss: 0.032	acc: 98.600%	train_loss: 0.0304	train_acc: 98.62%
INFO: 2017-09-12 22:36:39: main.py:144 **  Train=>
Epoch: 4	Batch_num: 780	lr: 0.000	loss: 0.032	acc: 98.580%	train_loss: 0.0389	train_acc: 98.17%
INFO: 2017-09-12 22:37:05: main.py:144 **  Train=>
Epoch: 4	Batch_num: 810	lr: 0.000	loss: 0.036	acc: 98.470%	train_loss: 0.0416	train_acc: 98.06%
INFO: 2017-09-12 22:37:32: main.py:144 **  Train=>
Epoch: 4	Batch_num: 840	lr: 0.000	loss: 0.044	acc: 98.120%	train_loss: 0.0451	train_acc: 97.95%
INFO: 2017-09-12 22:37:58: main.py:144 **  Train=>
Epoch: 4	Batch_num: 870	lr: 0.000	loss: 0.033	acc: 98.550%	train_loss: 0.0264	train_acc: 98.72%
INFO: 2017-09-12 22:38:24: main.py:144 **  Train=>
Epoch: 4	Batch_num: 900	lr: 0.000	loss: 0.038	acc: 98.410%	train_loss: 0.0283	train_acc: 98.75%
INFO: 2017-09-12 22:38:50: main.py:144 **  Train=>
Epoch: 4	Batch_num: 930	lr: 0.000	loss: 0.032	acc: 98.580%	train_loss: 0.0267	train_acc: 98.81%
INFO: 2017-09-12 22:39:16: main.py:144 **  Train=>
Epoch: 4	Batch_num: 960	lr: 0.000	loss: 0.030	acc: 98.670%	train_loss: 0.0337	train_acc: 98.53%
INFO: 2017-09-12 22:39:42: main.py:144 **  Train=>
Epoch: 4	Batch_num: 990	lr: 0.000	loss: 0.029	acc: 98.710%	train_loss: 0.031	train_acc: 98.68%
INFO: 2017-09-12 22:40:05: main.py:144 **  Train=>
Epoch: 4	Batch_num: 1016	lr: 0.000	loss: 0.029	acc: 98.710%	train_loss: 0.0317	train_acc: 98.61%
INFO: 2017-09-12 22:41:08: main.py:83 **  Validate=>
Epoch: 4	Valid_loss: 0.028	Valid_acc: 98.760%
INFO: 2017-09-12 22:41:10: main.py:144 **  Train=>
Epoch: 5	Batch_num: 0	lr: 0.000	loss: 0.025	acc: 98.920%	train_loss: 0.0255	train_acc: 98.92%
INFO: 2017-09-12 22:41:36: main.py:144 **  Train=>
Epoch: 5	Batch_num: 30	lr: 0.000	loss: 0.028	acc: 98.760%	train_loss: 0.0315	train_acc: 98.55%
INFO: 2017-09-12 22:42:02: main.py:144 **  Train=>
Epoch: 5	Batch_num: 60	lr: 0.000	loss: 0.027	acc: 98.790%	train_loss: 0.023	train_acc: 98.95%
INFO: 2017-09-12 22:42:28: main.py:144 **  Train=>
Epoch: 5	Batch_num: 90	lr: 0.000	loss: 0.028	acc: 98.780%	train_loss: 0.0258	train_acc: 98.85%
INFO: 2017-09-12 22:42:54: main.py:144 **  Train=>
Epoch: 5	Batch_num: 120	lr: 0.000	loss: 0.028	acc: 98.810%	train_loss: 0.0268	train_acc: 98.86%
INFO: 2017-09-12 22:43:20: main.py:144 **  Train=>
Epoch: 5	Batch_num: 150	lr: 0.000	loss: 0.028	acc: 98.810%	train_loss: 0.03	train_acc: 98.84%
INFO: 2017-09-12 22:43:46: main.py:144 **  Train=>
Epoch: 5	Batch_num: 180	lr: 0.000	loss: 0.026	acc: 98.840%	train_loss: 0.024	train_acc: 98.86%
INFO: 2017-09-12 22:44:13: main.py:144 **  Train=>
Epoch: 5	Batch_num: 210	lr: 0.000	loss: 0.028	acc: 98.810%	train_loss: 0.0248	train_acc: 98.97%
INFO: 2017-09-12 22:44:39: main.py:144 **  Train=>
Epoch: 5	Batch_num: 240	lr: 0.000	loss: 0.028	acc: 98.790%	train_loss: 0.0283	train_acc: 98.85%
INFO: 2017-09-12 22:45:05: main.py:144 **  Train=>
Epoch: 5	Batch_num: 270	lr: 0.000	loss: 0.026	acc: 98.830%	train_loss: 0.0274	train_acc: 98.79%
INFO: 2017-09-12 22:45:31: main.py:144 **  Train=>
Epoch: 5	Batch_num: 300	lr: 0.000	loss: 0.025	acc: 98.880%	train_loss: 0.0267	train_acc: 98.71%
INFO: 2017-09-12 22:45:57: main.py:144 **  Train=>
Epoch: 5	Batch_num: 330	lr: 0.000	loss: 0.027	acc: 98.800%	train_loss: 0.0303	train_acc: 98.83%
INFO: 2017-09-12 22:46:23: main.py:144 **  Train=>
Epoch: 5	Batch_num: 360	lr: 0.000	loss: 0.026	acc: 98.850%	train_loss: 0.0237	train_acc: 99.0%
INFO: 2017-09-12 22:46:49: main.py:144 **  Train=>
Epoch: 5	Batch_num: 390	lr: 0.000	loss: 0.026	acc: 98.880%	train_loss: 0.0229	train_acc: 99.01%
INFO: 2017-09-12 22:47:15: main.py:144 **  Train=>
Epoch: 5	Batch_num: 420	lr: 0.000	loss: 0.026	acc: 98.830%	train_loss: 0.0368	train_acc: 98.29%
INFO: 2017-09-12 22:47:41: main.py:144 **  Train=>
Epoch: 5	Batch_num: 450	lr: 0.000	loss: 0.026	acc: 98.840%	train_loss: 0.0297	train_acc: 98.7%
INFO: 2017-09-12 22:48:07: main.py:144 **  Train=>
Epoch: 5	Batch_num: 480	lr: 0.000	loss: 0.025	acc: 98.870%	train_loss: 0.0234	train_acc: 98.87%
INFO: 2017-09-12 22:48:34: main.py:144 **  Train=>
Epoch: 5	Batch_num: 510	lr: 0.000	loss: 0.025	acc: 98.860%	train_loss: 0.0252	train_acc: 98.83%
INFO: 2017-09-12 22:49:00: main.py:144 **  Train=>
Epoch: 5	Batch_num: 540	lr: 0.000	loss: 0.025	acc: 98.900%	train_loss: 0.0248	train_acc: 99.02%
INFO: 2017-09-12 22:49:26: main.py:144 **  Train=>
Epoch: 5	Batch_num: 570	lr: 0.000	loss: 0.023	acc: 98.940%	train_loss: 0.0223	train_acc: 98.89%
INFO: 2017-09-12 22:49:52: main.py:144 **  Train=>
Epoch: 5	Batch_num: 600	lr: 0.000	loss: 0.025	acc: 98.920%	train_loss: 0.0279	train_acc: 98.67%
INFO: 2017-09-12 22:50:18: main.py:144 **  Train=>
Epoch: 5	Batch_num: 630	lr: 0.000	loss: 0.024	acc: 98.980%	train_loss: 0.0201	train_acc: 99.09%
INFO: 2017-09-12 22:50:44: main.py:144 **  Train=>
Epoch: 5	Batch_num: 660	lr: 0.000	loss: 0.025	acc: 98.910%	train_loss: 0.0272	train_acc: 98.95%
INFO: 2017-09-12 22:51:10: main.py:144 **  Train=>
Epoch: 5	Batch_num: 690	lr: 0.000	loss: 0.024	acc: 98.920%	train_loss: 0.0226	train_acc: 99.01%
INFO: 2017-09-12 22:51:36: main.py:144 **  Train=>
Epoch: 5	Batch_num: 720	lr: 0.000	loss: 0.023	acc: 98.970%	train_loss: 0.0273	train_acc: 98.78%
INFO: 2017-09-12 22:52:03: main.py:144 **  Train=>
Epoch: 5	Batch_num: 750	lr: 0.000	loss: 0.023	acc: 98.990%	train_loss: 0.0229	train_acc: 98.96%
INFO: 2017-09-12 22:52:29: main.py:144 **  Train=>
Epoch: 5	Batch_num: 780	lr: 0.000	loss: 0.025	acc: 98.930%	train_loss: 0.0262	train_acc: 98.78%
INFO: 2017-09-12 22:52:55: main.py:144 **  Train=>
Epoch: 5	Batch_num: 810	lr: 0.000	loss: 0.023	acc: 98.960%	train_loss: 0.0305	train_acc: 98.65%
INFO: 2017-09-12 22:53:21: main.py:144 **  Train=>
Epoch: 5	Batch_num: 840	lr: 0.000	loss: 0.022	acc: 99.000%	train_loss: 0.0243	train_acc: 98.94%
INFO: 2017-09-12 22:53:47: main.py:144 **  Train=>
Epoch: 5	Batch_num: 870	lr: 0.000	loss: 0.023	acc: 99.000%	train_loss: 0.0192	train_acc: 99.13%
INFO: 2017-09-12 22:54:13: main.py:144 **  Train=>
Epoch: 5	Batch_num: 900	lr: 0.000	loss: 0.022	acc: 99.010%	train_loss: 0.0209	train_acc: 99.07%
INFO: 2017-09-12 22:54:39: main.py:144 **  Train=>
Epoch: 5	Batch_num: 930	lr: 0.000	loss: 0.023	acc: 98.990%	train_loss: 0.021	train_acc: 99.07%
INFO: 2017-09-12 22:55:05: main.py:144 **  Train=>
Epoch: 5	Batch_num: 960	lr: 0.000	loss: 0.023	acc: 98.990%	train_loss: 0.0251	train_acc: 98.9%
INFO: 2017-09-12 22:55:31: main.py:144 **  Train=>
Epoch: 5	Batch_num: 990	lr: 0.000	loss: 0.023	acc: 99.020%	train_loss: 0.0251	train_acc: 98.93%
INFO: 2017-09-12 22:55:54: main.py:144 **  Train=>
Epoch: 5	Batch_num: 1016	lr: 0.000	loss: 0.023	acc: 99.020%	train_loss: 0.0247	train_acc: 98.89%
INFO: 2017-09-12 22:56:57: main.py:83 **  Validate=>
Epoch: 5	Valid_loss: 0.024	Valid_acc: 98.960%
INFO: 2017-09-12 22:57:01: main.py:144 **  Train=>
Epoch: 6	Batch_num: 0	lr: 0.000	loss: 0.019	acc: 99.180%	train_loss: 0.0195	train_acc: 99.18%
INFO: 2017-09-12 22:57:27: main.py:144 **  Train=>
Epoch: 6	Batch_num: 30	lr: 0.000	loss: 0.022	acc: 98.990%	train_loss: 0.0237	train_acc: 98.88%
INFO: 2017-09-12 22:57:53: main.py:144 **  Train=>
Epoch: 6	Batch_num: 60	lr: 0.000	loss: 0.022	acc: 99.000%	train_loss: 0.0213	train_acc: 99.02%
INFO: 2017-09-12 22:58:19: main.py:144 **  Train=>
Epoch: 6	Batch_num: 90	lr: 0.000	loss: 0.022	acc: 99.020%	train_loss: 0.0206	train_acc: 99.08%
INFO: 2017-09-12 22:58:45: main.py:144 **  Train=>
Epoch: 6	Batch_num: 120	lr: 0.000	loss: 0.023	acc: 99.020%	train_loss: 0.021	train_acc: 99.1%
INFO: 2017-09-12 22:59:11: main.py:144 **  Train=>
Epoch: 6	Batch_num: 150	lr: 0.000	loss: 0.023	acc: 99.020%	train_loss: 0.0232	train_acc: 99.07%
INFO: 2017-09-12 22:59:38: main.py:144 **  Train=>
Epoch: 6	Batch_num: 180	lr: 0.000	loss: 0.022	acc: 99.040%	train_loss: 0.02	train_acc: 99.06%
INFO: 2017-09-12 23:00:04: main.py:144 **  Train=>
Epoch: 6	Batch_num: 210	lr: 0.000	loss: 0.023	acc: 99.000%	train_loss: 0.0227	train_acc: 99.09%
INFO: 2017-09-12 23:00:30: main.py:144 **  Train=>
Epoch: 6	Batch_num: 240	lr: 0.000	loss: 0.022	acc: 99.030%	train_loss: 0.0242	train_acc: 99.0%
INFO: 2017-09-12 23:00:56: main.py:144 **  Train=>
Epoch: 6	Batch_num: 270	lr: 0.000	loss: 0.022	acc: 99.000%	train_loss: 0.0245	train_acc: 98.89%
INFO: 2017-09-12 23:01:22: main.py:144 **  Train=>
Epoch: 6	Batch_num: 300	lr: 0.000	loss: 0.021	acc: 99.050%	train_loss: 0.0224	train_acc: 98.97%
INFO: 2017-09-12 23:01:48: main.py:144 **  Train=>
Epoch: 6	Batch_num: 330	lr: 0.000	loss: 0.023	acc: 98.980%	train_loss: 0.0247	train_acc: 99.04%
INFO: 2017-09-12 23:02:14: main.py:144 **  Train=>
Epoch: 6	Batch_num: 360	lr: 0.000	loss: 0.022	acc: 99.010%	train_loss: 0.0199	train_acc: 99.17%
INFO: 2017-09-12 23:02:41: main.py:144 **  Train=>
Epoch: 6	Batch_num: 390	lr: 0.000	loss: 0.023	acc: 98.990%	train_loss: 0.02	train_acc: 99.16%
INFO: 2017-09-12 23:03:07: main.py:144 **  Train=>
Epoch: 6	Batch_num: 420	lr: 0.000	loss: 0.023	acc: 98.990%	train_loss: 0.0276	train_acc: 98.7%
INFO: 2017-09-12 23:03:33: main.py:144 **  Train=>
Epoch: 6	Batch_num: 450	lr: 0.000	loss: 0.023	acc: 98.970%	train_loss: 0.0269	train_acc: 98.82%
INFO: 2017-09-12 23:03:59: main.py:144 **  Train=>
Epoch: 6	Batch_num: 480	lr: 0.000	loss: 0.022	acc: 99.010%	train_loss: 0.02	train_acc: 99.02%
INFO: 2017-09-12 23:04:25: main.py:144 **  Train=>
Epoch: 6	Batch_num: 510	lr: 0.000	loss: 0.022	acc: 99.010%	train_loss: 0.0201	train_acc: 99.07%
INFO: 2017-09-12 23:04:51: main.py:144 **  Train=>
Epoch: 6	Batch_num: 540	lr: 0.000	loss: 0.022	acc: 99.040%	train_loss: 0.0221	train_acc: 99.13%
INFO: 2017-09-12 23:05:17: main.py:144 **  Train=>
Epoch: 6	Batch_num: 570	lr: 0.000	loss: 0.021	acc: 99.060%	train_loss: 0.0203	train_acc: 99.0%
INFO: 2017-09-12 23:05:44: main.py:144 **  Train=>
Epoch: 6	Batch_num: 600	lr: 0.000	loss: 0.021	acc: 99.040%	train_loss: 0.0242	train_acc: 98.8%
INFO: 2017-09-12 23:06:10: main.py:144 **  Train=>
Epoch: 6	Batch_num: 630	lr: 0.000	loss: 0.021	acc: 99.100%	train_loss: 0.0185	train_acc: 99.15%
INFO: 2017-09-12 23:06:36: main.py:144 **  Train=>
Epoch: 6	Batch_num: 660	lr: 0.000	loss: 0.022	acc: 99.050%	train_loss: 0.0216	train_acc: 99.16%
INFO: 2017-09-12 23:07:02: main.py:144 **  Train=>
Epoch: 6	Batch_num: 690	lr: 0.000	loss: 0.021	acc: 99.050%	train_loss: 0.0203	train_acc: 99.11%
INFO: 2017-09-12 23:07:28: main.py:144 **  Train=>
Epoch: 6	Batch_num: 720	lr: 0.000	loss: 0.021	acc: 99.050%	train_loss: 0.0253	train_acc: 98.86%
INFO: 2017-09-12 23:07:54: main.py:144 **  Train=>
Epoch: 6	Batch_num: 750	lr: 0.000	loss: 0.021	acc: 99.070%	train_loss: 0.0221	train_acc: 98.98%
INFO: 2017-09-12 23:08:21: main.py:144 **  Train=>
Epoch: 6	Batch_num: 780	lr: 0.000	loss: 0.022	acc: 99.050%	train_loss: 0.0229	train_acc: 98.94%
INFO: 2017-09-12 23:08:47: main.py:144 **  Train=>
Epoch: 6	Batch_num: 810	lr: 0.000	loss: 0.021	acc: 99.070%	train_loss: 0.0208	train_acc: 99.04%
INFO: 2017-09-12 23:09:13: main.py:144 **  Train=>
Epoch: 6	Batch_num: 840	lr: 0.000	loss: 0.020	acc: 99.080%	train_loss: 0.0224	train_acc: 99.04%
INFO: 2017-09-12 23:09:39: main.py:144 **  Train=>
Epoch: 6	Batch_num: 870	lr: 0.000	loss: 0.021	acc: 99.080%	train_loss: 0.0178	train_acc: 99.18%
INFO: 2017-09-12 23:10:05: main.py:144 **  Train=>
Epoch: 6	Batch_num: 900	lr: 0.000	loss: 0.020	acc: 99.100%	train_loss: 0.019	train_acc: 99.16%
INFO: 2017-09-12 23:10:31: main.py:144 **  Train=>
Epoch: 6	Batch_num: 930	lr: 0.000	loss: 0.021	acc: 99.090%	train_loss: 0.0205	train_acc: 99.1%
INFO: 2017-09-12 23:10:58: main.py:144 **  Train=>
Epoch: 6	Batch_num: 960	lr: 0.000	loss: 0.021	acc: 99.070%	train_loss: 0.0234	train_acc: 98.96%
INFO: 2017-09-12 23:11:24: main.py:144 **  Train=>
Epoch: 6	Batch_num: 990	lr: 0.000	loss: 0.021	acc: 99.100%	train_loss: 0.0218	train_acc: 99.06%
INFO: 2017-09-12 23:11:46: main.py:144 **  Train=>
Epoch: 6	Batch_num: 1016	lr: 0.000	loss: 0.021	acc: 99.100%	train_loss: 0.0245	train_acc: 98.93%
INFO: 2017-09-12 23:12:50: main.py:83 **  Validate=>
Epoch: 6	Valid_loss: 0.021	Valid_acc: 99.050%
INFO: 2017-09-12 23:12:53: main.py:144 **  Train=>
Epoch: 7	Batch_num: 0	lr: 0.000	loss: 0.018	acc: 99.220%	train_loss: 0.0184	train_acc: 99.22%
INFO: 2017-09-12 23:13:19: main.py:144 **  Train=>
Epoch: 7	Batch_num: 30	lr: 0.000	loss: 0.020	acc: 99.080%	train_loss: 0.0193	train_acc: 99.1%
INFO: 2017-09-12 23:13:46: main.py:144 **  Train=>
Epoch: 7	Batch_num: 60	lr: 0.000	loss: 0.020	acc: 99.090%	train_loss: 0.0196	train_acc: 99.11%
INFO: 2017-09-12 23:14:12: main.py:144 **  Train=>
Epoch: 7	Batch_num: 90	lr: 0.000	loss: 0.020	acc: 99.110%	train_loss: 0.0198	train_acc: 99.08%
INFO: 2017-09-12 23:14:38: main.py:144 **  Train=>
Epoch: 7	Batch_num: 120	lr: 0.000	loss: 0.021	acc: 99.090%	train_loss: 0.0197	train_acc: 99.16%
INFO: 2017-09-12 23:15:04: main.py:144 **  Train=>
Epoch: 7	Batch_num: 150	lr: 0.000	loss: 0.021	acc: 99.080%	train_loss: 0.0213	train_acc: 99.16%
INFO: 2017-09-12 23:15:30: main.py:144 **  Train=>
Epoch: 7	Batch_num: 180	lr: 0.000	loss: 0.020	acc: 99.100%	train_loss: 0.0188	train_acc: 99.11%
INFO: 2017-09-12 23:15:56: main.py:144 **  Train=>
Epoch: 7	Batch_num: 210	lr: 0.000	loss: 0.021	acc: 99.090%	train_loss: 0.0206	train_acc: 99.18%
INFO: 2017-09-12 23:16:22: main.py:144 **  Train=>
Epoch: 7	Batch_num: 240	lr: 0.000	loss: 0.021	acc: 99.080%	train_loss: 0.0223	train_acc: 99.09%
INFO: 2017-09-12 23:16:48: main.py:144 **  Train=>
Epoch: 7	Batch_num: 270	lr: 0.000	loss: 0.021	acc: 99.080%	train_loss: 0.023	train_acc: 98.95%
INFO: 2017-09-12 23:17:15: main.py:144 **  Train=>
Epoch: 7	Batch_num: 300	lr: 0.000	loss: 0.020	acc: 99.120%	train_loss: 0.0179	train_acc: 99.18%
INFO: 2017-09-12 23:17:41: main.py:144 **  Train=>
Epoch: 7	Batch_num: 330	lr: 0.000	loss: 0.021	acc: 99.080%	train_loss: 0.022	train_acc: 99.13%
INFO: 2017-09-12 23:18:07: main.py:144 **  Train=>
Epoch: 7	Batch_num: 360	lr: 0.000	loss: 0.020	acc: 99.090%	train_loss: 0.0189	train_acc: 99.21%
INFO: 2017-09-12 23:18:33: main.py:144 **  Train=>
Epoch: 7	Batch_num: 390	lr: 0.000	loss: 0.022	acc: 99.050%	train_loss: 0.0198	train_acc: 99.16%
INFO: 2017-09-12 23:18:59: main.py:144 **  Train=>
Epoch: 7	Batch_num: 420	lr: 0.000	loss: 0.022	acc: 99.020%	train_loss: 0.0267	train_acc: 98.76%
INFO: 2017-09-12 23:19:25: main.py:144 **  Train=>
Epoch: 7	Batch_num: 450	lr: 0.000	loss: 0.022	acc: 99.050%	train_loss: 0.0252	train_acc: 98.9%
INFO: 2017-09-12 23:19:51: main.py:144 **  Train=>
Epoch: 7	Batch_num: 480	lr: 0.000	loss: 0.021	acc: 99.080%	train_loss: 0.0189	train_acc: 99.06%
INFO: 2017-09-12 23:20:18: main.py:144 **  Train=>
Epoch: 7	Batch_num: 510	lr: 0.000	loss: 0.020	acc: 99.080%	train_loss: 0.0192	train_acc: 99.11%
INFO: 2017-09-12 23:20:44: main.py:144 **  Train=>
Epoch: 7	Batch_num: 540	lr: 0.000	loss: 0.020	acc: 99.110%	train_loss: 0.0208	train_acc: 99.17%
INFO: 2017-09-12 23:21:10: main.py:144 **  Train=>
Epoch: 7	Batch_num: 570	lr: 0.000	loss: 0.019	acc: 99.130%	train_loss: 0.0191	train_acc: 99.06%
INFO: 2017-09-12 23:21:36: main.py:144 **  Train=>
Epoch: 7	Batch_num: 600	lr: 0.000	loss: 0.020	acc: 99.100%	train_loss: 0.0221	train_acc: 98.9%
INFO: 2017-09-12 23:22:03: main.py:144 **  Train=>
Epoch: 7	Batch_num: 630	lr: 0.000	loss: 0.020	acc: 99.140%	train_loss: 0.0174	train_acc: 99.19%
INFO: 2017-09-12 23:22:29: main.py:144 **  Train=>
Epoch: 7	Batch_num: 660	lr: 0.000	loss: 0.021	acc: 99.100%	train_loss: 0.0209	train_acc: 99.17%
INFO: 2017-09-12 23:22:55: main.py:144 **  Train=>
Epoch: 7	Batch_num: 690	lr: 0.000	loss: 0.020	acc: 99.090%	train_loss: 0.0196	train_acc: 99.15%
INFO: 2017-09-12 23:23:21: main.py:144 **  Train=>
Epoch: 7	Batch_num: 720	lr: 0.000	loss: 0.020	acc: 99.100%	train_loss: 0.0231	train_acc: 98.97%
INFO: 2017-09-12 23:23:47: main.py:144 **  Train=>
Epoch: 7	Batch_num: 750	lr: 0.000	loss: 0.020	acc: 99.130%	train_loss: 0.02	train_acc: 99.09%
INFO: 2017-09-12 23:24:13: main.py:144 **  Train=>
Epoch: 7	Batch_num: 780	lr: 0.000	loss: 0.020	acc: 99.110%	train_loss: 0.0213	train_acc: 98.99%
INFO: 2017-09-12 23:24:39: main.py:144 **  Train=>
Epoch: 7	Batch_num: 810	lr: 0.000	loss: 0.019	acc: 99.120%	train_loss: 0.0192	train_acc: 99.12%
INFO: 2017-09-12 23:25:05: main.py:144 **  Train=>
Epoch: 7	Batch_num: 840	lr: 0.000	loss: 0.019	acc: 99.130%	train_loss: 0.0211	train_acc: 99.09%
INFO: 2017-09-12 23:25:31: main.py:144 **  Train=>
Epoch: 7	Batch_num: 870	lr: 0.000	loss: 0.020	acc: 99.120%	train_loss: 0.0171	train_acc: 99.21%
INFO: 2017-09-12 23:25:57: main.py:144 **  Train=>
Epoch: 7	Batch_num: 900	lr: 0.000	loss: 0.019	acc: 99.130%	train_loss: 0.0185	train_acc: 99.17%
INFO: 2017-09-12 23:26:23: main.py:144 **  Train=>
Epoch: 7	Batch_num: 930	lr: 0.000	loss: 0.020	acc: 99.120%	train_loss: 0.02	train_acc: 99.12%
INFO: 2017-09-12 23:26:50: main.py:144 **  Train=>
Epoch: 7	Batch_num: 960	lr: 0.000	loss: 0.020	acc: 99.110%	train_loss: 0.0221	train_acc: 99.02%
INFO: 2017-09-12 23:27:16: main.py:144 **  Train=>
Epoch: 7	Batch_num: 990	lr: 0.000	loss: 0.020	acc: 99.130%	train_loss: 0.0209	train_acc: 99.09%
INFO: 2017-09-12 23:27:38: main.py:144 **  Train=>
Epoch: 7	Batch_num: 1016	lr: 0.000	loss: 0.020	acc: 99.130%	train_loss: 0.0227	train_acc: 98.98%
INFO: 2017-09-12 23:28:42: main.py:83 **  Validate=>
Epoch: 7	Valid_loss: 0.020	Valid_acc: 99.100%
INFO: 2017-09-12 23:28:46: main.py:144 **  Train=>
Epoch: 8	Batch_num: 0	lr: 0.000	loss: 0.017	acc: 99.270%	train_loss: 0.0171	train_acc: 99.27%
INFO: 2017-09-12 23:29:12: main.py:144 **  Train=>
Epoch: 8	Batch_num: 30	lr: 0.000	loss: 0.019	acc: 99.130%	train_loss: 0.0172	train_acc: 99.2%
INFO: 2017-09-12 23:29:38: main.py:144 **  Train=>
Epoch: 8	Batch_num: 60	lr: 0.000	loss: 0.019	acc: 99.130%	train_loss: 0.019	train_acc: 99.14%
INFO: 2017-09-12 23:30:04: main.py:144 **  Train=>
Epoch: 8	Batch_num: 90	lr: 0.000	loss: 0.019	acc: 99.150%	train_loss: 0.0194	train_acc: 99.12%
INFO: 2017-09-12 23:30:30: main.py:144 **  Train=>
Epoch: 8	Batch_num: 120	lr: 0.000	loss: 0.020	acc: 99.130%	train_loss: 0.0191	train_acc: 99.19%
INFO: 2017-09-12 23:30:56: main.py:144 **  Train=>
Epoch: 8	Batch_num: 150	lr: 0.000	loss: 0.020	acc: 99.130%	train_loss: 0.0209	train_acc: 99.19%
INFO: 2017-09-12 23:31:22: main.py:144 **  Train=>
Epoch: 8	Batch_num: 180	lr: 0.000	loss: 0.019	acc: 99.140%	train_loss: 0.0178	train_acc: 99.17%
INFO: 2017-09-12 23:31:48: main.py:144 **  Train=>
Epoch: 8	Batch_num: 210	lr: 0.000	loss: 0.020	acc: 99.120%	train_loss: 0.021	train_acc: 99.16%
INFO: 2017-09-12 23:32:14: main.py:144 **  Train=>
Epoch: 8	Batch_num: 240	lr: 0.000	loss: 0.020	acc: 99.120%	train_loss: 0.0216	train_acc: 99.12%
INFO: 2017-09-12 23:32:40: main.py:144 **  Train=>
Epoch: 8	Batch_num: 270	lr: 0.000	loss: 0.020	acc: 99.110%	train_loss: 0.0222	train_acc: 98.98%
INFO: 2017-09-12 23:33:07: main.py:144 **  Train=>
Epoch: 8	Batch_num: 300	lr: 0.000	loss: 0.019	acc: 99.160%	train_loss: 0.0172	train_acc: 99.21%
INFO: 2017-09-12 23:33:33: main.py:144 **  Train=>
Epoch: 8	Batch_num: 330	lr: 0.000	loss: 0.019	acc: 99.130%	train_loss: 0.0208	train_acc: 99.17%
INFO: 2017-09-12 23:33:59: main.py:144 **  Train=>
Epoch: 8	Batch_num: 360	lr: 0.000	loss: 0.019	acc: 99.130%	train_loss: 0.0182	train_acc: 99.24%
INFO: 2017-09-12 23:34:25: main.py:144 **  Train=>
Epoch: 8	Batch_num: 390	lr: 0.000	loss: 0.020	acc: 99.120%	train_loss: 0.0193	train_acc: 99.18%
INFO: 2017-09-12 23:34:51: main.py:144 **  Train=>
Epoch: 8	Batch_num: 420	lr: 0.000	loss: 0.020	acc: 99.090%	train_loss: 0.0221	train_acc: 98.95%
INFO: 2017-09-12 23:35:17: main.py:144 **  Train=>
Epoch: 8	Batch_num: 450	lr: 0.000	loss: 0.020	acc: 99.090%	train_loss: 0.0232	train_acc: 98.98%
INFO: 2017-09-12 23:35:44: main.py:144 **  Train=>
Epoch: 8	Batch_num: 480	lr: 0.000	loss: 0.020	acc: 99.110%	train_loss: 0.0185	train_acc: 99.09%
INFO: 2017-09-12 23:36:10: main.py:144 **  Train=>
Epoch: 8	Batch_num: 510	lr: 0.000	loss: 0.020	acc: 99.110%	train_loss: 0.0189	train_acc: 99.13%
INFO: 2017-09-12 23:36:36: main.py:144 **  Train=>
Epoch: 8	Batch_num: 540	lr: 0.000	loss: 0.019	acc: 99.140%	train_loss: 0.0202	train_acc: 99.19%
INFO: 2017-09-12 23:37:02: main.py:144 **  Train=>
Epoch: 8	Batch_num: 570	lr: 0.000	loss: 0.018	acc: 99.160%	train_loss: 0.0185	train_acc: 99.07%
INFO: 2017-09-12 23:37:28: main.py:144 **  Train=>
Epoch: 8	Batch_num: 600	lr: 0.000	loss: 0.019	acc: 99.130%	train_loss: 0.0224	train_acc: 98.87%
INFO: 2017-09-12 23:37:54: main.py:144 **  Train=>
Epoch: 8	Batch_num: 630	lr: 0.000	loss: 0.019	acc: 99.160%	train_loss: 0.017	train_acc: 99.21%
INFO: 2017-09-12 23:38:20: main.py:144 **  Train=>
Epoch: 8	Batch_num: 660	lr: 0.000	loss: 0.019	acc: 99.150%	train_loss: 0.0199	train_acc: 99.21%
INFO: 2017-09-12 23:38:47: main.py:144 **  Train=>
Epoch: 8	Batch_num: 690	lr: 0.000	loss: 0.020	acc: 99.120%	train_loss: 0.0184	train_acc: 99.19%
INFO: 2017-09-12 23:39:13: main.py:144 **  Train=>
Epoch: 8	Batch_num: 720	lr: 0.000	loss: 0.020	acc: 99.110%	train_loss: 0.0226	train_acc: 98.98%
INFO: 2017-09-12 23:39:39: main.py:144 **  Train=>
Epoch: 8	Batch_num: 750	lr: 0.000	loss: 0.019	acc: 99.150%	train_loss: 0.0189	train_acc: 99.13%
INFO: 2017-09-12 23:40:05: main.py:144 **  Train=>
Epoch: 8	Batch_num: 780	lr: 0.000	loss: 0.020	acc: 99.140%	train_loss: 0.0197	train_acc: 99.05%
INFO: 2017-09-12 23:40:31: main.py:144 **  Train=>
Epoch: 8	Batch_num: 810	lr: 0.000	loss: 0.019	acc: 99.140%	train_loss: 0.0188	train_acc: 99.13%
INFO: 2017-09-12 23:40:57: main.py:144 **  Train=>
Epoch: 8	Batch_num: 840	lr: 0.000	loss: 0.018	acc: 99.150%	train_loss: 0.02	train_acc: 99.14%
INFO: 2017-09-12 23:41:24: main.py:144 **  Train=>
Epoch: 8	Batch_num: 870	lr: 0.000	loss: 0.019	acc: 99.150%	train_loss: 0.0168	train_acc: 99.22%
INFO: 2017-09-12 23:41:50: main.py:144 **  Train=>
Epoch: 8	Batch_num: 900	lr: 0.000	loss: 0.019	acc: 99.150%	train_loss: 0.0182	train_acc: 99.18%
INFO: 2017-09-12 23:42:16: main.py:144 **  Train=>
Epoch: 8	Batch_num: 930	lr: 0.000	loss: 0.019	acc: 99.150%	train_loss: 0.0193	train_acc: 99.14%
INFO: 2017-09-12 23:42:42: main.py:144 **  Train=>
Epoch: 8	Batch_num: 960	lr: 0.000	loss: 0.019	acc: 99.150%	train_loss: 0.0204	train_acc: 99.07%
INFO: 2017-09-12 23:43:08: main.py:144 **  Train=>
Epoch: 8	Batch_num: 990	lr: 0.000	loss: 0.019	acc: 99.160%	train_loss: 0.0201	train_acc: 99.12%
INFO: 2017-09-12 23:43:31: main.py:144 **  Train=>
Epoch: 8	Batch_num: 1016	lr: 0.000	loss: 0.019	acc: 99.160%	train_loss: 0.0194	train_acc: 99.11%
INFO: 2017-09-12 23:44:34: main.py:83 **  Validate=>
Epoch: 8	Valid_loss: 0.020	Valid_acc: 99.110%
INFO: 2017-09-12 23:44:38: main.py:144 **  Train=>
Epoch: 9	Batch_num: 0	lr: 0.000	loss: 0.017	acc: 99.280%	train_loss: 0.0168	train_acc: 99.28%
INFO: 2017-09-12 23:45:04: main.py:144 **  Train=>
Epoch: 9	Batch_num: 30	lr: 0.000	loss: 0.018	acc: 99.160%	train_loss: 0.0163	train_acc: 99.24%
INFO: 2017-09-12 23:45:30: main.py:144 **  Train=>
Epoch: 9	Batch_num: 60	lr: 0.000	loss: 0.018	acc: 99.160%	train_loss: 0.0179	train_acc: 99.19%
INFO: 2017-09-12 23:45:56: main.py:144 **  Train=>
Epoch: 9	Batch_num: 90	lr: 0.000	loss: 0.018	acc: 99.180%	train_loss: 0.0189	train_acc: 99.14%
INFO: 2017-09-12 23:46:22: main.py:144 **  Train=>
Epoch: 9	Batch_num: 120	lr: 0.000	loss: 0.019	acc: 99.150%	train_loss: 0.0189	train_acc: 99.18%
INFO: 2017-09-12 23:46:48: main.py:144 **  Train=>
Epoch: 9	Batch_num: 150	lr: 0.000	loss: 0.019	acc: 99.160%	train_loss: 0.0201	train_acc: 99.21%
INFO: 2017-09-12 23:47:15: main.py:144 **  Train=>
Epoch: 9	Batch_num: 180	lr: 0.000	loss: 0.019	acc: 99.160%	train_loss: 0.0173	train_acc: 99.2%
INFO: 2017-09-12 23:47:41: main.py:144 **  Train=>
Epoch: 9	Batch_num: 210	lr: 0.000	loss: 0.019	acc: 99.150%	train_loss: 0.0194	train_acc: 99.21%
INFO: 2017-09-12 23:48:07: main.py:144 **  Train=>
Epoch: 9	Batch_num: 240	lr: 0.000	loss: 0.020	acc: 99.140%	train_loss: 0.0197	train_acc: 99.18%
INFO: 2017-09-12 23:48:33: main.py:144 **  Train=>
Epoch: 9	Batch_num: 270	lr: 0.000	loss: 0.020	acc: 99.120%	train_loss: 0.022	train_acc: 99.0%
INFO: 2017-09-12 23:48:59: main.py:144 **  Train=>
Epoch: 9	Batch_num: 300	lr: 0.000	loss: 0.018	acc: 99.170%	train_loss: 0.0169	train_acc: 99.2%
INFO: 2017-09-12 23:49:26: main.py:144 **  Train=>
Epoch: 9	Batch_num: 330	lr: 0.000	loss: 0.019	acc: 99.150%	train_loss: 0.0195	train_acc: 99.2%
INFO: 2017-09-12 23:49:52: main.py:144 **  Train=>
Epoch: 9	Batch_num: 360	lr: 0.000	loss: 0.019	acc: 99.160%	train_loss: 0.0178	train_acc: 99.25%
INFO: 2017-09-12 23:50:18: main.py:144 **  Train=>
Epoch: 9	Batch_num: 390	lr: 0.000	loss: 0.019	acc: 99.150%	train_loss: 0.0182	train_acc: 99.21%
INFO: 2017-09-12 23:50:44: main.py:144 **  Train=>
Epoch: 9	Batch_num: 420	lr: 0.000	loss: 0.020	acc: 99.110%	train_loss: 0.0213	train_acc: 99.0%
INFO: 2017-09-12 23:51:10: main.py:144 **  Train=>
Epoch: 9	Batch_num: 450	lr: 0.000	loss: 0.020	acc: 99.120%	train_loss: 0.0233	train_acc: 98.97%
INFO: 2017-09-12 23:51:36: main.py:144 **  Train=>
Epoch: 9	Batch_num: 480	lr: 0.000	loss: 0.019	acc: 99.140%	train_loss: 0.018	train_acc: 99.1%
INFO: 2017-09-12 23:52:02: main.py:144 **  Train=>
Epoch: 9	Batch_num: 510	lr: 0.000	loss: 0.019	acc: 99.140%	train_loss: 0.0183	train_acc: 99.16%
INFO: 2017-09-12 23:52:29: main.py:144 **  Train=>
Epoch: 9	Batch_num: 540	lr: 0.000	loss: 0.019	acc: 99.160%	train_loss: 0.0196	train_acc: 99.2%
INFO: 2017-09-12 23:52:55: main.py:144 **  Train=>
Epoch: 9	Batch_num: 570	lr: 0.000	loss: 0.018	acc: 99.180%	train_loss: 0.018	train_acc: 99.1%
INFO: 2017-09-12 23:53:21: main.py:144 **  Train=>
Epoch: 9	Batch_num: 600	lr: 0.000	loss: 0.019	acc: 99.160%	train_loss: 0.0199	train_acc: 98.99%
INFO: 2017-09-12 23:53:47: main.py:144 **  Train=>
Epoch: 9	Batch_num: 630	lr: 0.000	loss: 0.019	acc: 99.190%	train_loss: 0.0168	train_acc: 99.22%
INFO: 2017-09-12 23:54:13: main.py:144 **  Train=>
Epoch: 9	Batch_num: 660	lr: 0.000	loss: 0.019	acc: 99.170%	train_loss: 0.0191	train_acc: 99.24%
INFO: 2017-09-12 23:54:39: main.py:144 **  Train=>
Epoch: 9	Batch_num: 690	lr: 0.000	loss: 0.019	acc: 99.150%	train_loss: 0.0177	train_acc: 99.22%
INFO: 2017-09-12 23:55:05: main.py:144 **  Train=>
Epoch: 9	Batch_num: 720	lr: 0.000	loss: 0.019	acc: 99.120%	train_loss: 0.0221	train_acc: 99.01%
INFO: 2017-09-12 23:55:31: main.py:144 **  Train=>
Epoch: 9	Batch_num: 750	lr: 0.000	loss: 0.019	acc: 99.170%	train_loss: 0.0186	train_acc: 99.15%
INFO: 2017-09-12 23:55:57: main.py:144 **  Train=>
Epoch: 9	Batch_num: 780	lr: 0.000	loss: 0.019	acc: 99.160%	train_loss: 0.0187	train_acc: 99.1%
INFO: 2017-09-12 23:56:23: main.py:144 **  Train=>
Epoch: 9	Batch_num: 810	lr: 0.000	loss: 0.018	acc: 99.170%	train_loss: 0.0178	train_acc: 99.18%
INFO: 2017-09-12 23:56:49: main.py:144 **  Train=>
Epoch: 9	Batch_num: 840	lr: 0.000	loss: 0.018	acc: 99.180%	train_loss: 0.0192	train_acc: 99.17%
INFO: 2017-09-12 23:57:15: main.py:144 **  Train=>
Epoch: 9	Batch_num: 870	lr: 0.000	loss: 0.019	acc: 99.180%	train_loss: 0.0163	train_acc: 99.23%
INFO: 2017-09-12 23:57:41: main.py:144 **  Train=>
Epoch: 9	Batch_num: 900	lr: 0.000	loss: 0.018	acc: 99.170%	train_loss: 0.0178	train_acc: 99.2%
INFO: 2017-09-12 23:58:08: main.py:144 **  Train=>
Epoch: 9	Batch_num: 930	lr: 0.000	loss: 0.018	acc: 99.180%	train_loss: 0.0188	train_acc: 99.15%
INFO: 2017-09-12 23:58:34: main.py:144 **  Train=>
Epoch: 9	Batch_num: 960	lr: 0.000	loss: 0.019	acc: 99.160%	train_loss: 0.0195	train_acc: 99.12%
INFO: 2017-09-12 23:59:00: main.py:144 **  Train=>
Epoch: 9	Batch_num: 990	lr: 0.000	loss: 0.019	acc: 99.180%	train_loss: 0.0193	train_acc: 99.16%
INFO: 2017-09-12 23:59:22: main.py:144 **  Train=>
Epoch: 9	Batch_num: 1016	lr: 0.000	loss: 0.019	acc: 99.180%	train_loss: 0.0181	train_acc: 99.15%
INFO: 2017-09-13 00:00:25: main.py:83 **  Validate=>
Epoch: 9	Valid_loss: 0.020	Valid_acc: 99.120%
INFO: 2017-09-13 00:00:29: main.py:144 **  Train=>
Epoch: 10	Batch_num: 0	lr: 0.000	loss: 0.016	acc: 99.310%	train_loss: 0.0163	train_acc: 99.31%
INFO: 2017-09-13 00:00:54: main.py:144 **  Train=>
Epoch: 10	Batch_num: 30	lr: 0.000	loss: 0.018	acc: 99.170%	train_loss: 0.0156	train_acc: 99.28%
INFO: 2017-09-13 00:01:20: main.py:144 **  Train=>
Epoch: 10	Batch_num: 60	lr: 0.000	loss: 0.018	acc: 99.170%	train_loss: 0.0179	train_acc: 99.2%
INFO: 2017-09-13 00:01:46: main.py:144 **  Train=>
Epoch: 10	Batch_num: 90	lr: 0.000	loss: 0.018	acc: 99.200%	train_loss: 0.0186	train_acc: 99.15%
INFO: 2017-09-13 00:02:13: main.py:144 **  Train=>
Epoch: 10	Batch_num: 120	lr: 0.000	loss: 0.020	acc: 99.130%	train_loss: 0.0217	train_acc: 99.07%
INFO: 2017-09-13 00:02:39: main.py:144 **  Train=>
Epoch: 10	Batch_num: 150	lr: 0.000	loss: 0.020	acc: 99.120%	train_loss: 0.0199	train_acc: 99.22%
INFO: 2017-09-13 00:03:05: main.py:144 **  Train=>
Epoch: 10	Batch_num: 180	lr: 0.000	loss: 0.019	acc: 99.170%	train_loss: 0.0171	train_acc: 99.21%
INFO: 2017-09-13 00:03:31: main.py:144 **  Train=>
Epoch: 10	Batch_num: 210	lr: 0.000	loss: 0.019	acc: 99.170%	train_loss: 0.0193	train_acc: 99.22%
INFO: 2017-09-13 00:03:57: main.py:144 **  Train=>
Epoch: 10	Batch_num: 240	lr: 0.000	loss: 0.019	acc: 99.140%	train_loss: 0.0196	train_acc: 99.19%
INFO: 2017-09-13 00:04:23: main.py:144 **  Train=>
Epoch: 10	Batch_num: 270	lr: 0.000	loss: 0.019	acc: 99.140%	train_loss: 0.0222	train_acc: 98.97%
INFO: 2017-09-13 00:04:49: main.py:144 **  Train=>
Epoch: 10	Batch_num: 300	lr: 0.000	loss: 0.018	acc: 99.180%	train_loss: 0.0166	train_acc: 99.22%
INFO: 2017-09-13 00:05:15: main.py:144 **  Train=>
Epoch: 10	Batch_num: 330	lr: 0.000	loss: 0.019	acc: 99.150%	train_loss: 0.0194	train_acc: 99.2%
INFO: 2017-09-13 00:05:41: main.py:144 **  Train=>
Epoch: 10	Batch_num: 360	lr: 0.000	loss: 0.018	acc: 99.180%	train_loss: 0.0177	train_acc: 99.24%
INFO: 2017-09-13 00:06:07: main.py:144 **  Train=>
Epoch: 10	Batch_num: 390	lr: 0.000	loss: 0.018	acc: 99.170%	train_loss: 0.0175	train_acc: 99.24%
INFO: 2017-09-13 00:06:33: main.py:144 **  Train=>
Epoch: 10	Batch_num: 420	lr: 0.000	loss: 0.019	acc: 99.130%	train_loss: 0.0205	train_acc: 99.02%
INFO: 2017-09-13 00:07:00: main.py:144 **  Train=>
Epoch: 10	Batch_num: 450	lr: 0.000	loss: 0.019	acc: 99.140%	train_loss: 0.0225	train_acc: 99.0%
INFO: 2017-09-13 00:07:26: main.py:144 **  Train=>
Epoch: 10	Batch_num: 480	lr: 0.000	loss: 0.019	acc: 99.160%	train_loss: 0.0175	train_acc: 99.13%
INFO: 2017-09-13 00:07:52: main.py:144 **  Train=>
Epoch: 10	Batch_num: 510	lr: 0.000	loss: 0.019	acc: 99.140%	train_loss: 0.0183	train_acc: 99.16%
INFO: 2017-09-13 00:08:18: main.py:144 **  Train=>
Epoch: 10	Batch_num: 540	lr: 0.000	loss: 0.019	acc: 99.160%	train_loss: 0.0194	train_acc: 99.21%
INFO: 2017-09-13 00:08:44: main.py:144 **  Train=>
Epoch: 10	Batch_num: 570	lr: 0.000	loss: 0.018	acc: 99.190%	train_loss: 0.018	train_acc: 99.12%
INFO: 2017-09-13 00:09:10: main.py:144 **  Train=>
Epoch: 10	Batch_num: 600	lr: 0.000	loss: 0.018	acc: 99.170%	train_loss: 0.0204	train_acc: 98.97%
INFO: 2017-09-13 00:09:36: main.py:144 **  Train=>
Epoch: 10	Batch_num: 630	lr: 0.000	loss: 0.018	acc: 99.200%	train_loss: 0.0164	train_acc: 99.25%
INFO: 2017-09-13 00:10:02: main.py:144 **  Train=>
Epoch: 10	Batch_num: 660	lr: 0.000	loss: 0.018	acc: 99.200%	train_loss: 0.0189	train_acc: 99.24%
INFO: 2017-09-13 00:10:28: main.py:144 **  Train=>
Epoch: 10	Batch_num: 690	lr: 0.000	loss: 0.019	acc: 99.160%	train_loss: 0.0179	train_acc: 99.21%
INFO: 2017-09-13 00:10:55: main.py:144 **  Train=>
Epoch: 10	Batch_num: 720	lr: 0.000	loss: 0.018	acc: 99.160%	train_loss: 0.0212	train_acc: 99.05%
INFO: 2017-09-13 00:11:21: main.py:144 **  Train=>
Epoch: 10	Batch_num: 750	lr: 0.000	loss: 0.018	acc: 99.200%	train_loss: 0.0184	train_acc: 99.16%
INFO: 2017-09-13 00:11:47: main.py:144 **  Train=>
Epoch: 10	Batch_num: 780	lr: 0.000	loss: 0.019	acc: 99.180%	train_loss: 0.0178	train_acc: 99.15%
INFO: 2017-09-13 00:12:13: main.py:144 **  Train=>
Epoch: 10	Batch_num: 810	lr: 0.000	loss: 0.018	acc: 99.180%	train_loss: 0.0177	train_acc: 99.18%
INFO: 2017-09-13 00:12:39: main.py:144 **  Train=>
Epoch: 10	Batch_num: 840	lr: 0.000	loss: 0.018	acc: 99.180%	train_loss: 0.0191	train_acc: 99.16%
INFO: 2017-09-13 00:13:05: main.py:144 **  Train=>
Epoch: 10	Batch_num: 870	lr: 0.000	loss: 0.018	acc: 99.190%	train_loss: 0.016	train_acc: 99.25%
INFO: 2017-09-13 00:13:31: main.py:144 **  Train=>
Epoch: 10	Batch_num: 900	lr: 0.000	loss: 0.018	acc: 99.190%	train_loss: 0.0177	train_acc: 99.2%
INFO: 2017-09-13 00:13:57: main.py:144 **  Train=>
Epoch: 10	Batch_num: 930	lr: 0.000	loss: 0.018	acc: 99.200%	train_loss: 0.0182	train_acc: 99.16%
INFO: 2017-09-13 00:14:23: main.py:144 **  Train=>
Epoch: 10	Batch_num: 960	lr: 0.000	loss: 0.018	acc: 99.190%	train_loss: 0.0189	train_acc: 99.14%
INFO: 2017-09-13 00:14:49: main.py:144 **  Train=>
Epoch: 10	Batch_num: 990	lr: 0.000	loss: 0.018	acc: 99.200%	train_loss: 0.019	train_acc: 99.17%
INFO: 2017-09-13 00:15:12: main.py:144 **  Train=>
Epoch: 10	Batch_num: 1016	lr: 0.000	loss: 0.018	acc: 99.200%	train_loss: 0.0171	train_acc: 99.2%
INFO: 2017-09-13 00:16:15: main.py:83 **  Validate=>
Epoch: 10	Valid_loss: 0.020	Valid_acc: 99.120%
INFO: 2017-09-13 00:16:19: main.py:144 **  Train=>
Epoch: 11	Batch_num: 0	lr: 0.000	loss: 0.016	acc: 99.310%	train_loss: 0.0162	train_acc: 99.31%
INFO: 2017-09-13 00:16:45: main.py:144 **  Train=>
Epoch: 11	Batch_num: 30	lr: 0.000	loss: 0.018	acc: 99.190%	train_loss: 0.0155	train_acc: 99.29%
INFO: 2017-09-13 00:17:11: main.py:144 **  Train=>
Epoch: 11	Batch_num: 60	lr: 0.000	loss: 0.018	acc: 99.180%	train_loss: 0.0192	train_acc: 99.13%
INFO: 2017-09-13 00:17:37: main.py:144 **  Train=>
Epoch: 11	Batch_num: 90	lr: 0.000	loss: 0.018	acc: 99.200%	train_loss: 0.0183	train_acc: 99.16%
INFO: 2017-09-13 00:18:03: main.py:144 **  Train=>
Epoch: 11	Batch_num: 120	lr: 0.000	loss: 0.018	acc: 99.190%	train_loss: 0.0169	train_acc: 99.24%
INFO: 2017-09-13 00:18:29: main.py:144 **  Train=>
Epoch: 11	Batch_num: 150	lr: 0.000	loss: 0.018	acc: 99.190%	train_loss: 0.0195	train_acc: 99.23%
INFO: 2017-09-13 00:18:55: main.py:144 **  Train=>
Epoch: 11	Batch_num: 180	lr: 0.000	loss: 0.018	acc: 99.200%	train_loss: 0.0167	train_acc: 99.23%
INFO: 2017-09-13 00:19:21: main.py:144 **  Train=>
Epoch: 11	Batch_num: 210	lr: 0.000	loss: 0.018	acc: 99.190%	train_loss: 0.0177	train_acc: 99.27%
INFO: 2017-09-13 00:19:47: main.py:144 **  Train=>
Epoch: 11	Batch_num: 240	lr: 0.000	loss: 0.019	acc: 99.150%	train_loss: 0.0186	train_acc: 99.21%
INFO: 2017-09-13 00:20:13: main.py:144 **  Train=>
Epoch: 11	Batch_num: 270	lr: 0.000	loss: 0.019	acc: 99.160%	train_loss: 0.0223	train_acc: 98.97%
INFO: 2017-09-13 00:20:39: main.py:144 **  Train=>
Epoch: 11	Batch_num: 300	lr: 0.000	loss: 0.017	acc: 99.210%	train_loss: 0.0163	train_acc: 99.24%
INFO: 2017-09-13 00:21:05: main.py:144 **  Train=>
Epoch: 11	Batch_num: 330	lr: 0.000	loss: 0.018	acc: 99.190%	train_loss: 0.0187	train_acc: 99.23%
INFO: 2017-09-13 00:21:31: main.py:144 **  Train=>
Epoch: 11	Batch_num: 360	lr: 0.000	loss: 0.018	acc: 99.190%	train_loss: 0.0176	train_acc: 99.24%
INFO: 2017-09-13 00:21:57: main.py:144 **  Train=>
Epoch: 11	Batch_num: 390	lr: 0.000	loss: 0.018	acc: 99.190%	train_loss: 0.0175	train_acc: 99.23%
INFO: 2017-09-13 00:22:23: main.py:144 **  Train=>
Epoch: 11	Batch_num: 420	lr: 0.000	loss: 0.019	acc: 99.160%	train_loss: 0.0195	train_acc: 99.07%
INFO: 2017-09-13 00:22:49: main.py:144 **  Train=>
Epoch: 11	Batch_num: 450	lr: 0.000	loss: 0.019	acc: 99.170%	train_loss: 0.0213	train_acc: 99.05%
INFO: 2017-09-13 00:23:15: main.py:144 **  Train=>
Epoch: 11	Batch_num: 480	lr: 0.000	loss: 0.018	acc: 99.190%	train_loss: 0.0172	train_acc: 99.14%
INFO: 2017-09-13 00:23:41: main.py:144 **  Train=>
Epoch: 11	Batch_num: 510	lr: 0.000	loss: 0.018	acc: 99.180%	train_loss: 0.0175	train_acc: 99.19%
INFO: 2017-09-13 00:24:08: main.py:144 **  Train=>
Epoch: 11	Batch_num: 540	lr: 0.000	loss: 0.018	acc: 99.190%	train_loss: 0.0187	train_acc: 99.23%
INFO: 2017-09-13 00:24:34: main.py:144 **  Train=>
Epoch: 11	Batch_num: 570	lr: 0.000	loss: 0.017	acc: 99.220%	train_loss: 0.0177	train_acc: 99.13%
INFO: 2017-09-13 00:25:00: main.py:144 **  Train=>
Epoch: 11	Batch_num: 600	lr: 0.000	loss: 0.018	acc: 99.190%	train_loss: 0.0192	train_acc: 99.03%
INFO: 2017-09-13 00:25:26: main.py:144 **  Train=>
Epoch: 11	Batch_num: 630	lr: 0.000	loss: 0.018	acc: 99.230%	train_loss: 0.0171	train_acc: 99.22%
INFO: 2017-09-13 00:25:52: main.py:144 **  Train=>
Epoch: 11	Batch_num: 660	lr: 0.000	loss: 0.018	acc: 99.220%	train_loss: 0.0182	train_acc: 99.27%
INFO: 2017-09-13 00:26:18: main.py:144 **  Train=>
Epoch: 11	Batch_num: 690	lr: 0.000	loss: 0.018	acc: 99.190%	train_loss: 0.0173	train_acc: 99.23%
INFO: 2017-09-13 00:26:44: main.py:144 **  Train=>
Epoch: 11	Batch_num: 720	lr: 0.000	loss: 0.018	acc: 99.180%	train_loss: 0.0206	train_acc: 99.07%
INFO: 2017-09-13 00:27:10: main.py:144 **  Train=>
Epoch: 11	Batch_num: 750	lr: 0.000	loss: 0.018	acc: 99.220%	train_loss: 0.0175	train_acc: 99.2%
INFO: 2017-09-13 00:27:36: main.py:144 **  Train=>
Epoch: 11	Batch_num: 780	lr: 0.000	loss: 0.018	acc: 99.190%	train_loss: 0.0172	train_acc: 99.18%
INFO: 2017-09-13 00:28:02: main.py:144 **  Train=>
Epoch: 11	Batch_num: 810	lr: 0.000	loss: 0.018	acc: 99.200%	train_loss: 0.0168	train_acc: 99.22%
INFO: 2017-09-13 00:28:28: main.py:144 **  Train=>
Epoch: 11	Batch_num: 840	lr: 0.000	loss: 0.017	acc: 99.210%	train_loss: 0.0182	train_acc: 99.18%
INFO: 2017-09-13 00:28:54: main.py:144 **  Train=>
Epoch: 11	Batch_num: 870	lr: 0.000	loss: 0.018	acc: 99.210%	train_loss: 0.0158	train_acc: 99.27%
INFO: 2017-09-13 00:29:20: main.py:144 **  Train=>
Epoch: 11	Batch_num: 900	lr: 0.000	loss: 0.018	acc: 99.200%	train_loss: 0.0174	train_acc: 99.21%
INFO: 2017-09-13 00:29:46: main.py:144 **  Train=>
Epoch: 11	Batch_num: 930	lr: 0.000	loss: 0.018	acc: 99.210%	train_loss: 0.0176	train_acc: 99.19%
INFO: 2017-09-13 00:30:12: main.py:144 **  Train=>
Epoch: 11	Batch_num: 960	lr: 0.000	loss: 0.018	acc: 99.200%	train_loss: 0.0183	train_acc: 99.17%
INFO: 2017-09-13 00:30:37: main.py:144 **  Train=>
Epoch: 11	Batch_num: 990	lr: 0.000	loss: 0.018	acc: 99.210%	train_loss: 0.0183	train_acc: 99.19%
INFO: 2017-09-13 00:31:00: main.py:144 **  Train=>
Epoch: 11	Batch_num: 1016	lr: 0.000	loss: 0.018	acc: 99.210%	train_loss: 0.0164	train_acc: 99.24%
INFO: 2017-09-13 00:32:04: main.py:83 **  Validate=>
Epoch: 11	Valid_loss: 0.019	Valid_acc: 99.150%
INFO: 2017-09-13 00:32:08: main.py:144 **  Train=>
Epoch: 12	Batch_num: 0	lr: 0.000	loss: 0.016	acc: 99.320%	train_loss: 0.0158	train_acc: 99.32%
INFO: 2017-09-13 00:32:34: main.py:144 **  Train=>
Epoch: 12	Batch_num: 30	lr: 0.000	loss: 0.018	acc: 99.200%	train_loss: 0.0144	train_acc: 99.34%
INFO: 2017-09-13 00:33:00: main.py:144 **  Train=>
Epoch: 12	Batch_num: 60	lr: 0.000	loss: 0.018	acc: 99.190%	train_loss: 0.0179	train_acc: 99.22%
INFO: 2017-09-13 00:33:26: main.py:144 **  Train=>
Epoch: 12	Batch_num: 90	lr: 0.000	loss: 0.018	acc: 99.210%	train_loss: 0.0183	train_acc: 99.16%
INFO: 2017-09-13 00:33:52: main.py:144 **  Train=>
Epoch: 12	Batch_num: 120	lr: 0.000	loss: 0.018	acc: 99.200%	train_loss: 0.0165	train_acc: 99.26%
INFO: 2017-09-13 00:34:18: main.py:144 **  Train=>
Epoch: 12	Batch_num: 150	lr: 0.000	loss: 0.018	acc: 99.200%	train_loss: 0.0194	train_acc: 99.24%
INFO: 2017-09-13 00:34:44: main.py:144 **  Train=>
Epoch: 12	Batch_num: 180	lr: 0.000	loss: 0.018	acc: 99.210%	train_loss: 0.0164	train_acc: 99.24%
INFO: 2017-09-13 00:35:10: main.py:144 **  Train=>
Epoch: 12	Batch_num: 210	lr: 0.000	loss: 0.018	acc: 99.200%	train_loss: 0.0172	train_acc: 99.29%
INFO: 2017-09-13 00:35:36: main.py:144 **  Train=>
Epoch: 12	Batch_num: 240	lr: 0.000	loss: 0.018	acc: 99.180%	train_loss: 0.0182	train_acc: 99.24%
INFO: 2017-09-13 00:36:02: main.py:144 **  Train=>
Epoch: 12	Batch_num: 270	lr: 0.000	loss: 0.018	acc: 99.180%	train_loss: 0.0216	train_acc: 99.0%
INFO: 2017-09-13 00:36:28: main.py:144 **  Train=>
Epoch: 12	Batch_num: 300	lr: 0.000	loss: 0.017	acc: 99.220%	train_loss: 0.0161	train_acc: 99.25%
INFO: 2017-09-13 00:36:54: main.py:144 **  Train=>
Epoch: 12	Batch_num: 330	lr: 0.000	loss: 0.017	acc: 99.210%	train_loss: 0.0185	train_acc: 99.24%
INFO: 2017-09-13 00:37:20: main.py:144 **  Train=>
Epoch: 12	Batch_num: 360	lr: 0.000	loss: 0.018	acc: 99.210%	train_loss: 0.0171	train_acc: 99.27%
INFO: 2017-09-13 00:37:47: main.py:144 **  Train=>
Epoch: 12	Batch_num: 390	lr: 0.000	loss: 0.018	acc: 99.210%	train_loss: 0.0168	train_acc: 99.26%
INFO: 2017-09-13 00:38:13: main.py:144 **  Train=>
Epoch: 12	Batch_num: 420	lr: 0.000	loss: 0.018	acc: 99.180%	train_loss: 0.019	train_acc: 99.1%
INFO: 2017-09-13 00:38:39: main.py:144 **  Train=>
Epoch: 12	Batch_num: 450	lr: 0.000	loss: 0.018	acc: 99.190%	train_loss: 0.0202	train_acc: 99.1%
INFO: 2017-09-13 00:39:05: main.py:144 **  Train=>
Epoch: 12	Batch_num: 480	lr: 0.000	loss: 0.018	acc: 99.210%	train_loss: 0.0169	train_acc: 99.15%
INFO: 2017-09-13 00:39:31: main.py:144 **  Train=>
Epoch: 12	Batch_num: 510	lr: 0.000	loss: 0.017	acc: 99.200%	train_loss: 0.0171	train_acc: 99.22%
INFO: 2017-09-13 00:39:57: main.py:144 **  Train=>
Epoch: 12	Batch_num: 540	lr: 0.000	loss: 0.018	acc: 99.200%	train_loss: 0.0184	train_acc: 99.25%
INFO: 2017-09-13 00:40:23: main.py:144 **  Train=>
Epoch: 12	Batch_num: 570	lr: 0.000	loss: 0.017	acc: 99.230%	train_loss: 0.0171	train_acc: 99.16%
INFO: 2017-09-13 00:40:49: main.py:144 **  Train=>
Epoch: 12	Batch_num: 600	lr: 0.000	loss: 0.018	acc: 99.210%	train_loss: 0.019	train_acc: 99.04%
INFO: 2017-09-13 00:41:15: main.py:144 **  Train=>
Epoch: 12	Batch_num: 630	lr: 0.000	loss: 0.018	acc: 99.240%	train_loss: 0.0163	train_acc: 99.26%
INFO: 2017-09-13 00:41:41: main.py:144 **  Train=>
Epoch: 12	Batch_num: 660	lr: 0.000	loss: 0.018	acc: 99.230%	train_loss: 0.018	train_acc: 99.27%
INFO: 2017-09-13 00:42:07: main.py:144 **  Train=>
Epoch: 12	Batch_num: 690	lr: 0.000	loss: 0.018	acc: 99.190%	train_loss: 0.0172	train_acc: 99.23%
INFO: 2017-09-13 00:42:33: main.py:144 **  Train=>
Epoch: 12	Batch_num: 720	lr: 0.000	loss: 0.018	acc: 99.190%	train_loss: 0.0202	train_acc: 99.08%
INFO: 2017-09-13 00:42:59: main.py:144 **  Train=>
Epoch: 12	Batch_num: 750	lr: 0.000	loss: 0.017	acc: 99.230%	train_loss: 0.0173	train_acc: 99.2%
INFO: 2017-09-13 00:43:25: main.py:144 **  Train=>
Epoch: 12	Batch_num: 780	lr: 0.000	loss: 0.018	acc: 99.210%	train_loss: 0.0169	train_acc: 99.21%
INFO: 2017-09-13 00:43:51: main.py:144 **  Train=>
Epoch: 12	Batch_num: 810	lr: 0.000	loss: 0.017	acc: 99.210%	train_loss: 0.0161	train_acc: 99.24%
INFO: 2017-09-13 00:44:17: main.py:144 **  Train=>
Epoch: 12	Batch_num: 840	lr: 0.000	loss: 0.017	acc: 99.220%	train_loss: 0.0179	train_acc: 99.19%
INFO: 2017-09-13 00:44:43: main.py:144 **  Train=>
Epoch: 12	Batch_num: 870	lr: 0.000	loss: 0.018	acc: 99.220%	train_loss: 0.0159	train_acc: 99.25%
INFO: 2017-09-13 00:45:09: main.py:144 **  Train=>
Epoch: 12	Batch_num: 900	lr: 0.000	loss: 0.017	acc: 99.210%	train_loss: 0.0175	train_acc: 99.21%
INFO: 2017-09-13 00:45:35: main.py:144 **  Train=>
Epoch: 12	Batch_num: 930	lr: 0.000	loss: 0.017	acc: 99.230%	train_loss: 0.0175	train_acc: 99.2%
INFO: 2017-09-13 00:46:00: main.py:144 **  Train=>
Epoch: 12	Batch_num: 960	lr: 0.000	loss: 0.018	acc: 99.210%	train_loss: 0.0178	train_acc: 99.2%
INFO: 2017-09-13 00:46:26: main.py:144 **  Train=>
Epoch: 12	Batch_num: 990	lr: 0.000	loss: 0.018	acc: 99.220%	train_loss: 0.0179	train_acc: 99.2%
INFO: 2017-09-13 00:46:49: main.py:144 **  Train=>
Epoch: 12	Batch_num: 1016	lr: 0.000	loss: 0.018	acc: 99.220%	train_loss: 0.0158	train_acc: 99.26%
INFO: 2017-09-13 00:47:53: main.py:83 **  Validate=>
Epoch: 12	Valid_loss: 0.019	Valid_acc: 99.150%
INFO: 2017-09-13 00:47:55: main.py:144 **  Train=>
Epoch: 13	Batch_num: 0	lr: 0.000	loss: 0.015	acc: 99.330%	train_loss: 0.0155	train_acc: 99.33%
INFO: 2017-09-13 00:48:21: main.py:144 **  Train=>
Epoch: 13	Batch_num: 30	lr: 0.000	loss: 0.017	acc: 99.220%	train_loss: 0.0142	train_acc: 99.35%
INFO: 2017-09-13 00:48:47: main.py:144 **  Train=>
Epoch: 13	Batch_num: 60	lr: 0.000	loss: 0.018	acc: 99.200%	train_loss: 0.017	train_acc: 99.25%
INFO: 2017-09-13 00:49:13: main.py:144 **  Train=>
Epoch: 13	Batch_num: 90	lr: 0.000	loss: 0.018	acc: 99.220%	train_loss: 0.0182	train_acc: 99.17%
INFO: 2017-09-13 00:49:39: main.py:144 **  Train=>
Epoch: 13	Batch_num: 120	lr: 0.000	loss: 0.018	acc: 99.220%	train_loss: 0.0161	train_acc: 99.28%
INFO: 2017-09-13 00:50:05: main.py:144 **  Train=>
Epoch: 13	Batch_num: 150	lr: 0.000	loss: 0.018	acc: 99.220%	train_loss: 0.0185	train_acc: 99.27%
INFO: 2017-09-13 00:50:31: main.py:144 **  Train=>
Epoch: 13	Batch_num: 180	lr: 0.000	loss: 0.018	acc: 99.220%	train_loss: 0.0163	train_acc: 99.24%
INFO: 2017-09-13 00:50:57: main.py:144 **  Train=>
Epoch: 13	Batch_num: 210	lr: 0.000	loss: 0.018	acc: 99.210%	train_loss: 0.0167	train_acc: 99.32%
INFO: 2017-09-13 00:51:23: main.py:144 **  Train=>
Epoch: 13	Batch_num: 240	lr: 0.000	loss: 0.018	acc: 99.190%	train_loss: 0.0181	train_acc: 99.25%
INFO: 2017-09-13 00:51:49: main.py:144 **  Train=>
Epoch: 13	Batch_num: 270	lr: 0.000	loss: 0.018	acc: 99.190%	train_loss: 0.0211	train_acc: 99.02%
INFO: 2017-09-13 00:52:15: main.py:144 **  Train=>
Epoch: 13	Batch_num: 300	lr: 0.000	loss: 0.017	acc: 99.240%	train_loss: 0.0158	train_acc: 99.24%
INFO: 2017-09-13 00:52:41: main.py:144 **  Train=>
Epoch: 13	Batch_num: 330	lr: 0.000	loss: 0.017	acc: 99.220%	train_loss: 0.0181	train_acc: 99.25%
INFO: 2017-09-13 00:53:07: main.py:144 **  Train=>
Epoch: 13	Batch_num: 360	lr: 0.000	loss: 0.017	acc: 99.230%	train_loss: 0.0169	train_acc: 99.26%
INFO: 2017-09-13 00:53:33: main.py:144 **  Train=>
Epoch: 13	Batch_num: 390	lr: 0.000	loss: 0.017	acc: 99.220%	train_loss: 0.0165	train_acc: 99.26%
INFO: 2017-09-13 00:53:59: main.py:144 **  Train=>
Epoch: 13	Batch_num: 420	lr: 0.000	loss: 0.018	acc: 99.200%	train_loss: 0.0177	train_acc: 99.15%
INFO: 2017-09-13 00:54:25: main.py:144 **  Train=>
Epoch: 13	Batch_num: 450	lr: 0.000	loss: 0.018	acc: 99.200%	train_loss: 0.0203	train_acc: 99.09%
INFO: 2017-09-13 00:54:51: main.py:144 **  Train=>
Epoch: 13	Batch_num: 480	lr: 0.000	loss: 0.017	acc: 99.220%	train_loss: 0.0167	train_acc: 99.16%
INFO: 2017-09-13 00:55:17: main.py:144 **  Train=>
Epoch: 13	Batch_num: 510	lr: 0.000	loss: 0.017	acc: 99.210%	train_loss: 0.017	train_acc: 99.22%
INFO: 2017-09-13 00:55:43: main.py:144 **  Train=>
Epoch: 13	Batch_num: 540	lr: 0.000	loss: 0.018	acc: 99.220%	train_loss: 0.0182	train_acc: 99.26%
INFO: 2017-09-13 00:56:09: main.py:144 **  Train=>
Epoch: 13	Batch_num: 570	lr: 0.000	loss: 0.017	acc: 99.240%	train_loss: 0.0167	train_acc: 99.18%
INFO: 2017-09-13 00:56:35: main.py:144 **  Train=>
Epoch: 13	Batch_num: 600	lr: 0.000	loss: 0.017	acc: 99.220%	train_loss: 0.0186	train_acc: 99.05%
INFO: 2017-09-13 00:57:00: main.py:144 **  Train=>
Epoch: 13	Batch_num: 630	lr: 0.000	loss: 0.017	acc: 99.250%	train_loss: 0.0161	train_acc: 99.27%
INFO: 2017-09-13 00:57:26: main.py:144 **  Train=>
Epoch: 13	Batch_num: 660	lr: 0.000	loss: 0.017	acc: 99.240%	train_loss: 0.0174	train_acc: 99.29%
INFO: 2017-09-13 00:57:52: main.py:144 **  Train=>
Epoch: 13	Batch_num: 690	lr: 0.000	loss: 0.018	acc: 99.210%	train_loss: 0.0172	train_acc: 99.24%
INFO: 2017-09-13 00:58:18: main.py:144 **  Train=>
Epoch: 13	Batch_num: 720	lr: 0.000	loss: 0.018	acc: 99.200%	train_loss: 0.0199	train_acc: 99.08%
INFO: 2017-09-13 00:58:44: main.py:144 **  Train=>
Epoch: 13	Batch_num: 750	lr: 0.000	loss: 0.017	acc: 99.240%	train_loss: 0.0169	train_acc: 99.22%
INFO: 2017-09-13 00:59:10: main.py:144 **  Train=>
Epoch: 13	Batch_num: 780	lr: 0.000	loss: 0.018	acc: 99.220%	train_loss: 0.0168	train_acc: 99.21%
INFO: 2017-09-13 00:59:36: main.py:144 **  Train=>
Epoch: 13	Batch_num: 810	lr: 0.000	loss: 0.017	acc: 99.230%	train_loss: 0.0158	train_acc: 99.27%
INFO: 2017-09-13 01:00:02: main.py:144 **  Train=>
Epoch: 13	Batch_num: 840	lr: 0.000	loss: 0.017	acc: 99.220%	train_loss: 0.0177	train_acc: 99.2%
INFO: 2017-09-13 01:00:28: main.py:144 **  Train=>
Epoch: 13	Batch_num: 870	lr: 0.000	loss: 0.017	acc: 99.240%	train_loss: 0.0155	train_acc: 99.28%
INFO: 2017-09-13 01:00:54: main.py:144 **  Train=>
Epoch: 13	Batch_num: 900	lr: 0.000	loss: 0.017	acc: 99.220%	train_loss: 0.0172	train_acc: 99.22%
INFO: 2017-09-13 01:01:20: main.py:144 **  Train=>
Epoch: 13	Batch_num: 930	lr: 0.000	loss: 0.017	acc: 99.240%	train_loss: 0.0172	train_acc: 99.22%
INFO: 2017-09-13 01:01:46: main.py:144 **  Train=>
Epoch: 13	Batch_num: 960	lr: 0.000	loss: 0.017	acc: 99.230%	train_loss: 0.0172	train_acc: 99.23%
INFO: 2017-09-13 01:02:12: main.py:144 **  Train=>
Epoch: 13	Batch_num: 990	lr: 0.000	loss: 0.017	acc: 99.230%	train_loss: 0.0176	train_acc: 99.22%
INFO: 2017-09-13 01:02:34: main.py:144 **  Train=>
Epoch: 13	Batch_num: 1016	lr: 0.000	loss: 0.017	acc: 99.230%	train_loss: 0.0156	train_acc: 99.27%
INFO: 2017-09-13 01:03:38: main.py:83 **  Validate=>
Epoch: 13	Valid_loss: 0.019	Valid_acc: 99.170%
INFO: 2017-09-13 01:03:42: main.py:144 **  Train=>
Epoch: 14	Batch_num: 0	lr: 0.000	loss: 0.015	acc: 99.340%	train_loss: 0.0152	train_acc: 99.34%
INFO: 2017-09-13 01:04:07: main.py:144 **  Train=>
Epoch: 14	Batch_num: 30	lr: 0.000	loss: 0.017	acc: 99.230%	train_loss: 0.0135	train_acc: 99.38%
INFO: 2017-09-13 01:04:34: main.py:144 **  Train=>
Epoch: 14	Batch_num: 60	lr: 0.000	loss: 0.017	acc: 99.230%	train_loss: 0.0164	train_acc: 99.26%
INFO: 2017-09-13 01:05:00: main.py:144 **  Train=>
Epoch: 14	Batch_num: 90	lr: 0.000	loss: 0.017	acc: 99.240%	train_loss: 0.0182	train_acc: 99.16%
INFO: 2017-09-13 01:05:26: main.py:144 **  Train=>
Epoch: 14	Batch_num: 120	lr: 0.000	loss: 0.017	acc: 99.230%	train_loss: 0.0159	train_acc: 99.28%
INFO: 2017-09-13 01:05:52: main.py:144 **  Train=>
Epoch: 14	Batch_num: 150	lr: 0.000	loss: 0.017	acc: 99.240%	train_loss: 0.0187	train_acc: 99.26%
INFO: 2017-09-13 01:06:18: main.py:144 **  Train=>
Epoch: 14	Batch_num: 180	lr: 0.000	loss: 0.017	acc: 99.230%	train_loss: 0.0161	train_acc: 99.26%
INFO: 2017-09-13 01:06:44: main.py:144 **  Train=>
Epoch: 14	Batch_num: 210	lr: 0.000	loss: 0.017	acc: 99.230%	train_loss: 0.0159	train_acc: 99.35%
INFO: 2017-09-13 01:07:10: main.py:144 **  Train=>
Epoch: 14	Batch_num: 240	lr: 0.000	loss: 0.018	acc: 99.200%	train_loss: 0.0173	train_acc: 99.27%
INFO: 2017-09-13 01:07:36: main.py:144 **  Train=>
Epoch: 14	Batch_num: 270	lr: 0.000	loss: 0.018	acc: 99.210%	train_loss: 0.0211	train_acc: 99.02%
INFO: 2017-09-13 01:08:02: main.py:144 **  Train=>
Epoch: 14	Batch_num: 300	lr: 0.000	loss: 0.017	acc: 99.250%	train_loss: 0.0158	train_acc: 99.26%
INFO: 2017-09-13 01:08:28: main.py:144 **  Train=>
Epoch: 14	Batch_num: 330	lr: 0.000	loss: 0.017	acc: 99.240%	train_loss: 0.0179	train_acc: 99.26%
INFO: 2017-09-13 01:08:54: main.py:144 **  Train=>
Epoch: 14	Batch_num: 360	lr: 0.000	loss: 0.017	acc: 99.240%	train_loss: 0.0168	train_acc: 99.26%
INFO: 2017-09-13 01:09:20: main.py:144 **  Train=>
Epoch: 14	Batch_num: 390	lr: 0.000	loss: 0.017	acc: 99.230%	train_loss: 0.0167	train_acc: 99.26%
INFO: 2017-09-13 01:09:46: main.py:144 **  Train=>
Epoch: 14	Batch_num: 420	lr: 0.000	loss: 0.017	acc: 99.200%	train_loss: 0.0188	train_acc: 99.09%
INFO: 2017-09-13 01:10:12: main.py:144 **  Train=>
Epoch: 14	Batch_num: 450	lr: 0.000	loss: 0.018	acc: 99.200%	train_loss: 0.0203	train_acc: 99.09%
INFO: 2017-09-13 01:10:38: main.py:144 **  Train=>
Epoch: 14	Batch_num: 480	lr: 0.000	loss: 0.017	acc: 99.230%	train_loss: 0.0163	train_acc: 99.18%
INFO: 2017-09-13 01:11:04: main.py:144 **  Train=>
Epoch: 14	Batch_num: 510	lr: 0.000	loss: 0.017	acc: 99.220%	train_loss: 0.0171	train_acc: 99.2%
INFO: 2017-09-13 01:11:30: main.py:144 **  Train=>
Epoch: 14	Batch_num: 540	lr: 0.000	loss: 0.018	acc: 99.210%	train_loss: 0.0185	train_acc: 99.25%
INFO: 2017-09-13 01:11:56: main.py:144 **  Train=>
Epoch: 14	Batch_num: 570	lr: 0.000	loss: 0.017	acc: 99.220%	train_loss: 0.0173	train_acc: 99.16%
INFO: 2017-09-13 01:12:22: main.py:144 **  Train=>
Epoch: 14	Batch_num: 600	lr: 0.000	loss: 0.017	acc: 99.220%	train_loss: 0.0185	train_acc: 99.08%
INFO: 2017-09-13 01:12:48: main.py:144 **  Train=>
Epoch: 14	Batch_num: 630	lr: 0.000	loss: 0.018	acc: 99.240%	train_loss: 0.0167	train_acc: 99.23%
INFO: 2017-09-13 01:13:14: main.py:144 **  Train=>
Epoch: 14	Batch_num: 660	lr: 0.000	loss: 0.017	acc: 99.230%	train_loss: 0.0172	train_acc: 99.29%
INFO: 2017-09-13 01:13:40: main.py:144 **  Train=>
Epoch: 14	Batch_num: 690	lr: 0.000	loss: 0.017	acc: 99.220%	train_loss: 0.0164	train_acc: 99.27%
INFO: 2017-09-13 01:14:06: main.py:144 **  Train=>
Epoch: 14	Batch_num: 720	lr: 0.000	loss: 0.017	acc: 99.220%	train_loss: 0.0192	train_acc: 99.11%
INFO: 2017-09-13 01:14:33: main.py:144 **  Train=>
Epoch: 14	Batch_num: 750	lr: 0.000	loss: 0.017	acc: 99.250%	train_loss: 0.0166	train_acc: 99.24%
INFO: 2017-09-13 01:14:59: main.py:144 **  Train=>
Epoch: 14	Batch_num: 780	lr: 0.000	loss: 0.017	acc: 99.230%	train_loss: 0.0168	train_acc: 99.21%
INFO: 2017-09-13 01:15:25: main.py:144 **  Train=>
Epoch: 14	Batch_num: 810	lr: 0.000	loss: 0.017	acc: 99.230%	train_loss: 0.0154	train_acc: 99.29%
INFO: 2017-09-13 01:15:50: main.py:144 **  Train=>
Epoch: 14	Batch_num: 840	lr: 0.000	loss: 0.017	acc: 99.230%	train_loss: 0.0174	train_acc: 99.21%
INFO: 2017-09-13 01:16:16: main.py:144 **  Train=>
Epoch: 14	Batch_num: 870	lr: 0.000	loss: 0.017	acc: 99.240%	train_loss: 0.0158	train_acc: 99.26%
INFO: 2017-09-13 01:16:42: main.py:144 **  Train=>
Epoch: 14	Batch_num: 900	lr: 0.000	loss: 0.017	acc: 99.230%	train_loss: 0.0169	train_acc: 99.23%
INFO: 2017-09-13 01:17:08: main.py:144 **  Train=>
Epoch: 14	Batch_num: 930	lr: 0.000	loss: 0.017	acc: 99.250%	train_loss: 0.0175	train_acc: 99.21%
INFO: 2017-09-13 01:17:34: main.py:144 **  Train=>
Epoch: 14	Batch_num: 960	lr: 0.000	loss: 0.017	acc: 99.220%	train_loss: 0.0175	train_acc: 99.21%
INFO: 2017-09-13 01:18:00: main.py:144 **  Train=>
Epoch: 14	Batch_num: 990	lr: 0.000	loss: 0.017	acc: 99.230%	train_loss: 0.0179	train_acc: 99.21%
INFO: 2017-09-13 01:18:22: main.py:144 **  Train=>
Epoch: 14	Batch_num: 1016	lr: 0.000	loss: 0.017	acc: 99.230%	train_loss: 0.0157	train_acc: 99.27%
INFO: 2017-09-13 01:19:26: main.py:83 **  Validate=>
Epoch: 14	Valid_loss: 0.019	Valid_acc: 99.170%
INFO: 2017-09-13 01:19:29: main.py:144 **  Train=>
Epoch: 15	Batch_num: 0	lr: 0.000	loss: 0.015	acc: 99.330%	train_loss: 0.0154	train_acc: 99.33%
INFO: 2017-09-13 01:19:54: main.py:144 **  Train=>
Epoch: 15	Batch_num: 30	lr: 0.000	loss: 0.017	acc: 99.230%	train_loss: 0.0136	train_acc: 99.38%
INFO: 2017-09-13 01:20:21: main.py:144 **  Train=>
Epoch: 15	Batch_num: 60	lr: 0.000	loss: 0.017	acc: 99.220%	train_loss: 0.0166	train_acc: 99.25%
INFO: 2017-09-13 01:20:47: main.py:144 **  Train=>
Epoch: 15	Batch_num: 90	lr: 0.000	loss: 0.017	acc: 99.240%	train_loss: 0.0181	train_acc: 99.16%
INFO: 2017-09-13 01:21:13: main.py:144 **  Train=>
Epoch: 15	Batch_num: 120	lr: 0.000	loss: 0.018	acc: 99.210%	train_loss: 0.0167	train_acc: 99.26%
INFO: 2017-09-13 01:21:39: main.py:144 **  Train=>
Epoch: 15	Batch_num: 150	lr: 0.000	loss: 0.017	acc: 99.230%	train_loss: 0.0181	train_acc: 99.27%
INFO: 2017-09-13 01:22:05: main.py:144 **  Train=>
Epoch: 15	Batch_num: 180	lr: 0.000	loss: 0.017	acc: 99.230%	train_loss: 0.0162	train_acc: 99.25%
INFO: 2017-09-13 01:22:31: main.py:144 **  Train=>
Epoch: 15	Batch_num: 210	lr: 0.000	loss: 0.017	acc: 99.230%	train_loss: 0.0158	train_acc: 99.36%
INFO: 2017-09-13 01:22:57: main.py:144 **  Train=>
Epoch: 15	Batch_num: 240	lr: 0.000	loss: 0.018	acc: 99.210%	train_loss: 0.0188	train_acc: 99.21%
INFO: 2017-09-13 01:23:23: main.py:144 **  Train=>
Epoch: 15	Batch_num: 270	lr: 0.000	loss: 0.018	acc: 99.210%	train_loss: 0.0206	train_acc: 99.04%
INFO: 2017-09-13 01:23:49: main.py:144 **  Train=>
Epoch: 15	Batch_num: 300	lr: 0.000	loss: 0.016	acc: 99.260%	train_loss: 0.0156	train_acc: 99.27%
INFO: 2017-09-13 01:24:15: main.py:144 **  Train=>
Epoch: 15	Batch_num: 330	lr: 0.000	loss: 0.017	acc: 99.240%	train_loss: 0.018	train_acc: 99.26%
INFO: 2017-09-13 01:24:41: main.py:144 **  Train=>
Epoch: 15	Batch_num: 360	lr: 0.000	loss: 0.017	acc: 99.250%	train_loss: 0.0169	train_acc: 99.26%
INFO: 2017-09-13 01:25:07: main.py:144 **  Train=>
Epoch: 15	Batch_num: 390	lr: 0.000	loss: 0.017	acc: 99.240%	train_loss: 0.0162	train_acc: 99.29%
INFO: 2017-09-13 01:25:33: main.py:144 **  Train=>
Epoch: 15	Batch_num: 420	lr: 0.000	loss: 0.017	acc: 99.220%	train_loss: 0.0178	train_acc: 99.15%
INFO: 2017-09-13 01:25:59: main.py:144 **  Train=>
Epoch: 15	Batch_num: 450	lr: 0.000	loss: 0.017	acc: 99.230%	train_loss: 0.0193	train_acc: 99.14%
INFO: 2017-09-13 01:26:25: main.py:144 **  Train=>
Epoch: 15	Batch_num: 480	lr: 0.000	loss: 0.017	acc: 99.240%	train_loss: 0.016	train_acc: 99.18%
INFO: 2017-09-13 01:26:51: main.py:144 **  Train=>
Epoch: 15	Batch_num: 510	lr: 0.000	loss: 0.017	acc: 99.230%	train_loss: 0.0168	train_acc: 99.24%
INFO: 2017-09-13 01:27:17: main.py:144 **  Train=>
Epoch: 15	Batch_num: 540	lr: 0.000	loss: 0.017	acc: 99.230%	train_loss: 0.0178	train_acc: 99.28%
INFO: 2017-09-13 01:27:43: main.py:144 **  Train=>
Epoch: 15	Batch_num: 570	lr: 0.000	loss: 0.016	acc: 99.260%	train_loss: 0.0164	train_acc: 99.19%
INFO: 2017-09-13 01:28:09: main.py:144 **  Train=>
Epoch: 15	Batch_num: 600	lr: 0.000	loss: 0.017	acc: 99.220%	train_loss: 0.0181	train_acc: 99.1%
INFO: 2017-09-13 01:28:35: main.py:144 **  Train=>
Epoch: 15	Batch_num: 630	lr: 0.000	loss: 0.017	acc: 99.260%	train_loss: 0.0157	train_acc: 99.29%
INFO: 2017-09-13 01:29:01: main.py:144 **  Train=>
Epoch: 15	Batch_num: 660	lr: 0.000	loss: 0.017	acc: 99.240%	train_loss: 0.0175	train_acc: 99.28%
INFO: 2017-09-13 01:29:27: main.py:144 **  Train=>
Epoch: 15	Batch_num: 690	lr: 0.000	loss: 0.017	acc: 99.230%	train_loss: 0.0161	train_acc: 99.28%
INFO: 2017-09-13 01:29:53: main.py:144 **  Train=>
Epoch: 15	Batch_num: 720	lr: 0.000	loss: 0.017	acc: 99.230%	train_loss: 0.0193	train_acc: 99.12%
INFO: 2017-09-13 01:30:19: main.py:144 **  Train=>
Epoch: 15	Batch_num: 750	lr: 0.000	loss: 0.017	acc: 99.260%	train_loss: 0.0168	train_acc: 99.22%
INFO: 2017-09-13 01:30:45: main.py:144 **  Train=>
Epoch: 15	Batch_num: 780	lr: 0.000	loss: 0.017	acc: 99.240%	train_loss: 0.0164	train_acc: 99.22%
INFO: 2017-09-13 01:31:11: main.py:144 **  Train=>
Epoch: 15	Batch_num: 810	lr: 0.000	loss: 0.017	acc: 99.240%	train_loss: 0.0151	train_acc: 99.3%
INFO: 2017-09-13 01:31:37: main.py:144 **  Train=>
Epoch: 15	Batch_num: 840	lr: 0.000	loss: 0.016	acc: 99.250%	train_loss: 0.017	train_acc: 99.23%
INFO: 2017-09-13 01:32:03: main.py:144 **  Train=>
Epoch: 15	Batch_num: 870	lr: 0.000	loss: 0.017	acc: 99.260%	train_loss: 0.015	train_acc: 99.31%
INFO: 2017-09-13 01:32:29: main.py:144 **  Train=>
Epoch: 15	Batch_num: 900	lr: 0.000	loss: 0.017	acc: 99.240%	train_loss: 0.017	train_acc: 99.22%
INFO: 2017-09-13 01:32:55: main.py:144 **  Train=>
Epoch: 15	Batch_num: 930	lr: 0.000	loss: 0.017	acc: 99.240%	train_loss: 0.0178	train_acc: 99.19%
INFO: 2017-09-13 01:33:21: main.py:144 **  Train=>
Epoch: 15	Batch_num: 960	lr: 0.000	loss: 0.017	acc: 99.240%	train_loss: 0.0167	train_acc: 99.25%
INFO: 2017-09-13 01:33:47: main.py:144 **  Train=>
Epoch: 15	Batch_num: 990	lr: 0.000	loss: 0.017	acc: 99.250%	train_loss: 0.0168	train_acc: 99.24%
INFO: 2017-09-13 01:34:09: main.py:144 **  Train=>
Epoch: 15	Batch_num: 1016	lr: 0.000	loss: 0.017	acc: 99.250%	train_loss: 0.0154	train_acc: 99.29%
INFO: 2017-09-13 01:35:13: main.py:83 **  Validate=>
Epoch: 15	Valid_loss: 0.019	Valid_acc: 99.170%
INFO: 2017-09-13 01:35:16: main.py:144 **  Train=>
Epoch: 16	Batch_num: 0	lr: 0.000	loss: 0.015	acc: 99.350%	train_loss: 0.0153	train_acc: 99.35%
INFO: 2017-09-13 01:35:42: main.py:144 **  Train=>
Epoch: 16	Batch_num: 30	lr: 0.000	loss: 0.017	acc: 99.250%	train_loss: 0.0132	train_acc: 99.39%
INFO: 2017-09-13 01:36:08: main.py:144 **  Train=>
Epoch: 16	Batch_num: 60	lr: 0.000	loss: 0.017	acc: 99.240%	train_loss: 0.0159	train_acc: 99.28%
INFO: 2017-09-13 01:36:34: main.py:144 **  Train=>
Epoch: 16	Batch_num: 90	lr: 0.000	loss: 0.017	acc: 99.260%	train_loss: 0.0176	train_acc: 99.18%
INFO: 2017-09-13 01:37:00: main.py:144 **  Train=>
Epoch: 16	Batch_num: 120	lr: 0.000	loss: 0.017	acc: 99.240%	train_loss: 0.0152	train_acc: 99.32%
INFO: 2017-09-13 01:37:27: main.py:144 **  Train=>
Epoch: 16	Batch_num: 150	lr: 0.000	loss: 0.017	acc: 99.250%	train_loss: 0.0179	train_acc: 99.29%
INFO: 2017-09-13 01:37:53: main.py:144 **  Train=>
Epoch: 16	Batch_num: 180	lr: 0.000	loss: 0.017	acc: 99.240%	train_loss: 0.016	train_acc: 99.27%
INFO: 2017-09-13 01:38:19: main.py:144 **  Train=>
Epoch: 16	Batch_num: 210	lr: 0.000	loss: 0.017	acc: 99.250%	train_loss: 0.0158	train_acc: 99.35%
INFO: 2017-09-13 01:38:45: main.py:144 **  Train=>
Epoch: 16	Batch_num: 240	lr: 0.000	loss: 0.017	acc: 99.220%	train_loss: 0.0168	train_acc: 99.29%
INFO: 2017-09-13 01:39:11: main.py:144 **  Train=>
Epoch: 16	Batch_num: 270	lr: 0.000	loss: 0.017	acc: 99.220%	train_loss: 0.0206	train_acc: 99.04%
INFO: 2017-09-13 01:39:37: main.py:144 **  Train=>
Epoch: 16	Batch_num: 300	lr: 0.000	loss: 0.016	acc: 99.270%	train_loss: 0.0154	train_acc: 99.25%
INFO: 2017-09-13 01:40:03: main.py:144 **  Train=>
Epoch: 16	Batch_num: 330	lr: 0.000	loss: 0.016	acc: 99.260%	train_loss: 0.0176	train_acc: 99.27%
INFO: 2017-09-13 01:40:29: main.py:144 **  Train=>
Epoch: 16	Batch_num: 360	lr: 0.000	loss: 0.018	acc: 99.200%	train_loss: 0.019	train_acc: 99.16%
INFO: 2017-09-13 01:40:55: main.py:144 **  Train=>
Epoch: 16	Batch_num: 390	lr: 0.000	loss: 0.019	acc: 99.160%	train_loss: 0.0174	train_acc: 99.25%
INFO: 2017-09-13 01:41:21: main.py:144 **  Train=>
Epoch: 16	Batch_num: 420	lr: 0.000	loss: 0.018	acc: 99.190%	train_loss: 0.0177	train_acc: 99.15%
INFO: 2017-09-13 01:41:47: main.py:144 **  Train=>
Epoch: 16	Batch_num: 450	lr: 0.000	loss: 0.018	acc: 99.210%	train_loss: 0.0196	train_acc: 99.12%
INFO: 2017-09-13 01:42:13: main.py:144 **  Train=>
Epoch: 16	Batch_num: 480	lr: 0.000	loss: 0.017	acc: 99.240%	train_loss: 0.016	train_acc: 99.2%
INFO: 2017-09-13 01:42:39: main.py:144 **  Train=>
Epoch: 16	Batch_num: 510	lr: 0.000	loss: 0.017	acc: 99.220%	train_loss: 0.017	train_acc: 99.22%
INFO: 2017-09-13 01:43:05: main.py:144 **  Train=>
Epoch: 16	Batch_num: 540	lr: 0.000	loss: 0.017	acc: 99.230%	train_loss: 0.018	train_acc: 99.27%
INFO: 2017-09-13 01:43:31: main.py:144 **  Train=>
Epoch: 16	Batch_num: 570	lr: 0.000	loss: 0.016	acc: 99.260%	train_loss: 0.0164	train_acc: 99.2%
INFO: 2017-09-13 01:43:57: main.py:144 **  Train=>
Epoch: 16	Batch_num: 600	lr: 0.000	loss: 0.017	acc: 99.230%	train_loss: 0.0185	train_acc: 99.07%
INFO: 2017-09-15 22:40:22: utils.py:92 **  All dataset size is 5088
INFO: 2017-09-15 22:40:22: utils.py:93 **  Train dataset size is 4071
INFO: 2017-09-15 22:40:22: utils.py:95 **  Valid dataset size is 1017
INFO: 2017-09-15 22:40:22: main.py:218 **  Loading dataset...
INFO: 2017-09-15 22:40:22: main.py:232 **  All data sample counts 5088
INFO: 2017-09-15 22:40:22: main.py:233 **  Train data batch size 8
INFO: 2017-09-15 22:40:22: main.py:234 **  Train data sample counts 8136
INFO: 2017-09-15 22:40:22: main.py:235 **  Valid data sample counts 1018
INFO: 2017-09-15 22:42:06: utils.py:92 **  All dataset size is 5088
INFO: 2017-09-15 22:42:06: utils.py:93 **  Train dataset size is 4071
INFO: 2017-09-15 22:42:06: utils.py:95 **  Valid dataset size is 1017
INFO: 2017-09-15 22:42:06: main.py:218 **  Loading dataset...
INFO: 2017-09-15 22:42:06: main.py:232 **  All data sample counts 5088
INFO: 2017-09-15 22:42:06: main.py:233 **  Train data batch size 4
INFO: 2017-09-15 22:42:06: main.py:234 **  Train data sample counts 4068
INFO: 2017-09-15 22:42:06: main.py:235 **  Valid data sample counts 1018
INFO: 2017-09-15 22:43:57: utils.py:92 **  All dataset size is 5088
INFO: 2017-09-15 22:43:57: utils.py:93 **  Train dataset size is 4071
INFO: 2017-09-15 22:43:57: utils.py:95 **  Valid dataset size is 1017
INFO: 2017-09-15 22:43:57: main.py:218 **  Loading dataset...
INFO: 2017-09-15 22:43:57: main.py:232 **  All data sample counts 5088
INFO: 2017-09-15 22:43:57: main.py:233 **  Train data batch size 2
INFO: 2017-09-15 22:43:57: main.py:234 **  Train data sample counts 4070
INFO: 2017-09-15 22:43:57: main.py:235 **  Valid data sample counts 1018
INFO: 2017-09-15 22:44:00: main.py:144 **  Train=>
Epoch: 0	Batch_num: 0	lr: 0.000	loss: 1.418	acc: 37.300%	train_loss: 1.4184	train_acc: 37.3%
INFO: 2017-09-15 22:44:21: main.py:144 **  Train=>
Epoch: 0	Batch_num: 30	lr: 0.000	loss: 1.317	acc: 42.370%	train_loss: 1.165	train_acc: 29.29%
INFO: 2017-09-15 22:44:42: main.py:144 **  Train=>
Epoch: 0	Batch_num: 60	lr: 0.000	loss: 1.049	acc: 44.450%	train_loss: 1.0266	train_acc: 45.73%
INFO: 2017-09-15 22:45:02: main.py:144 **  Train=>
Epoch: 0	Batch_num: 90	lr: 0.000	loss: 0.843	acc: 61.970%	train_loss: 0.8832	train_acc: 54.57%
INFO: 2017-09-15 22:48:23: utils.py:92 **  All dataset size is 5088
INFO: 2017-09-15 22:48:23: utils.py:93 **  Train dataset size is 4071
INFO: 2017-09-15 22:48:23: utils.py:95 **  Valid dataset size is 1017
INFO: 2017-09-15 22:48:23: main.py:218 **  Loading dataset...
INFO: 2017-09-15 22:48:23: main.py:232 **  All data sample counts 5088
INFO: 2017-09-15 22:48:23: main.py:233 **  Train data batch size 4
INFO: 2017-09-15 22:48:23: main.py:234 **  Train data sample counts 4068
INFO: 2017-09-15 22:48:23: main.py:235 **  Valid data sample counts 1018
INFO: 2017-09-15 22:49:34: utils.py:92 **  All dataset size is 5088
INFO: 2017-09-15 22:49:34: utils.py:93 **  Train dataset size is 4071
INFO: 2017-09-15 22:49:34: utils.py:95 **  Valid dataset size is 1017
INFO: 2017-09-15 22:49:34: main.py:218 **  Loading dataset...
INFO: 2017-09-15 22:49:34: main.py:232 **  All data sample counts 5088
INFO: 2017-09-15 22:49:34: main.py:233 **  Train data batch size 4
INFO: 2017-09-15 22:49:34: main.py:234 **  Train data sample counts 4068
INFO: 2017-09-15 22:49:34: main.py:235 **  Valid data sample counts 1018
INFO: 2017-09-15 22:51:47: utils.py:92 **  All dataset size is 5088
INFO: 2017-09-15 22:51:47: utils.py:93 **  Train dataset size is 4071
INFO: 2017-09-15 22:51:47: utils.py:95 **  Valid dataset size is 1017
INFO: 2017-09-15 22:51:47: main.py:218 **  Loading dataset...
INFO: 2017-09-15 22:51:47: main.py:232 **  All data sample counts 5088
INFO: 2017-09-15 22:51:47: main.py:233 **  Train data batch size 4
INFO: 2017-09-15 22:51:47: main.py:234 **  Train data sample counts 4068
INFO: 2017-09-15 22:51:47: main.py:235 **  Valid data sample counts 1018
INFO: 2017-09-15 22:51:50: main.py:144 **  Train=>
Epoch: 0	Batch_num: 0	lr: 0.000	loss: 1.402	acc: 38.320%	train_loss: 1.4023	train_acc: 38.32%
INFO: 2017-09-15 22:52:14: utils.py:92 **  All dataset size is 5088
INFO: 2017-09-15 22:52:14: utils.py:93 **  Train dataset size is 4071
INFO: 2017-09-15 22:52:14: utils.py:95 **  Valid dataset size is 1017
INFO: 2017-09-15 22:52:14: main.py:218 **  Loading dataset...
INFO: 2017-09-15 22:52:14: main.py:232 **  All data sample counts 5088
INFO: 2017-09-15 22:52:14: main.py:233 **  Train data batch size 8
INFO: 2017-09-15 22:52:14: main.py:234 **  Train data sample counts 4064
INFO: 2017-09-15 22:52:14: main.py:235 **  Valid data sample counts 1018
INFO: 2017-09-15 22:58:07: utils.py:92 **  All dataset size is 5088
INFO: 2017-09-15 22:58:07: utils.py:93 **  Train dataset size is 4071
INFO: 2017-09-15 22:58:07: utils.py:95 **  Valid dataset size is 1017
INFO: 2017-09-15 22:58:07: main.py:218 **  Loading dataset...
INFO: 2017-09-15 22:58:07: main.py:232 **  All data sample counts 5088
INFO: 2017-09-15 22:58:07: main.py:233 **  Train data batch size 4
INFO: 2017-09-15 22:58:07: main.py:234 **  Train data sample counts 4068
INFO: 2017-09-15 22:58:07: main.py:235 **  Valid data sample counts 1018
INFO: 2017-09-15 22:59:44: utils.py:92 **  All dataset size is 5088
INFO: 2017-09-15 22:59:44: utils.py:93 **  Train dataset size is 4071
INFO: 2017-09-15 22:59:44: utils.py:95 **  Valid dataset size is 1017
INFO: 2017-09-15 22:59:44: main.py:218 **  Loading dataset...
INFO: 2017-09-15 22:59:44: main.py:232 **  All data sample counts 5088
INFO: 2017-09-15 22:59:44: main.py:233 **  Train data batch size 4
INFO: 2017-09-15 22:59:44: main.py:234 **  Train data sample counts 4068
INFO: 2017-09-15 22:59:44: main.py:235 **  Valid data sample counts 1018
INFO: 2017-09-15 23:00:15: utils.py:92 **  All dataset size is 5088
INFO: 2017-09-15 23:00:15: utils.py:93 **  Train dataset size is 4071
INFO: 2017-09-15 23:00:15: utils.py:95 **  Valid dataset size is 1017
INFO: 2017-09-15 23:00:15: main.py:218 **  Loading dataset...
INFO: 2017-09-15 23:00:15: main.py:232 **  All data sample counts 5088
INFO: 2017-09-15 23:00:15: main.py:233 **  Train data batch size 4
INFO: 2017-09-15 23:00:15: main.py:234 **  Train data sample counts 4068
INFO: 2017-09-15 23:00:15: main.py:235 **  Valid data sample counts 1018
INFO: 2017-09-15 23:01:22: utils.py:92 **  All dataset size is 5088
INFO: 2017-09-15 23:01:22: utils.py:93 **  Train dataset size is 4071
INFO: 2017-09-15 23:01:22: utils.py:95 **  Valid dataset size is 1017
INFO: 2017-09-15 23:01:22: main.py:218 **  Loading dataset...
INFO: 2017-09-15 23:01:22: main.py:232 **  All data sample counts 5088
INFO: 2017-09-15 23:01:22: main.py:233 **  Train data batch size 4
INFO: 2017-09-15 23:01:22: main.py:234 **  Train data sample counts 4068
INFO: 2017-09-15 23:01:22: main.py:235 **  Valid data sample counts 1018
INFO: 2017-09-15 23:01:25: main.py:144 **  Train=>
Epoch: 0	Batch_num: 0	lr: 0.000	loss: 1.323	acc: 11.000%	train_loss: 1.3233	train_acc: 11.0%
INFO: 2017-09-15 23:02:31: utils.py:92 **  All dataset size is 5088
INFO: 2017-09-15 23:02:31: utils.py:93 **  Train dataset size is 4071
INFO: 2017-09-15 23:02:31: utils.py:95 **  Valid dataset size is 1017
INFO: 2017-09-15 23:02:31: main.py:218 **  Loading dataset...
INFO: 2017-09-15 23:02:31: main.py:232 **  All data sample counts 5088
INFO: 2017-09-15 23:02:31: main.py:233 **  Train data batch size 4
INFO: 2017-09-15 23:02:31: main.py:234 **  Train data sample counts 4068
INFO: 2017-09-15 23:02:31: main.py:235 **  Valid data sample counts 1018
INFO: 2017-09-15 23:03:05: utils.py:92 **  All dataset size is 5088
INFO: 2017-09-15 23:03:05: utils.py:93 **  Train dataset size is 4071
INFO: 2017-09-15 23:03:05: utils.py:95 **  Valid dataset size is 1017
INFO: 2017-09-15 23:03:05: main.py:218 **  Loading dataset...
INFO: 2017-09-15 23:03:05: main.py:232 **  All data sample counts 5088
INFO: 2017-09-15 23:03:05: main.py:233 **  Train data batch size 2
INFO: 2017-09-15 23:03:05: main.py:234 **  Train data sample counts 4070
INFO: 2017-09-15 23:03:05: main.py:235 **  Valid data sample counts 1018
INFO: 2017-09-15 23:03:08: main.py:144 **  Train=>
Epoch: 0	Batch_num: 0	lr: 0.000	loss: 1.369	acc: 16.360%	train_loss: 1.3694	train_acc: 16.36%
INFO: 2017-09-15 23:03:29: main.py:144 **  Train=>
Epoch: 0	Batch_num: 30	lr: 0.000	loss: 1.216	acc: 35.080%	train_loss: 1.0628	train_acc: 42.83%
INFO: 2017-09-15 23:04:41: utils.py:92 **  All dataset size is 5088
INFO: 2017-09-15 23:04:41: utils.py:93 **  Train dataset size is 4071
INFO: 2017-09-15 23:04:41: utils.py:95 **  Valid dataset size is 1017
INFO: 2017-09-15 23:04:41: main.py:218 **  Loading dataset...
INFO: 2017-09-15 23:04:41: main.py:232 **  All data sample counts 5088
INFO: 2017-09-15 23:04:41: main.py:233 **  Train data batch size 2
INFO: 2017-09-15 23:04:41: main.py:234 **  Train data sample counts 4070
INFO: 2017-09-15 23:04:41: main.py:235 **  Valid data sample counts 1018
INFO: 2017-09-15 23:04:44: main.py:144 **  Train=>
Epoch: 0	Batch_num: 0	lr: 0.000	loss: 1.383	acc: 30.590%	train_loss: 1.383	train_acc: 30.59%
INFO: 2017-09-15 23:05:05: main.py:144 **  Train=>
Epoch: 0	Batch_num: 30	lr: 0.000	loss: 1.260	acc: 42.410%	train_loss: 1.1257	train_acc: 48.99%
INFO: 2017-09-15 23:05:26: main.py:144 **  Train=>
Epoch: 0	Batch_num: 60	lr: 0.000	loss: 1.027	acc: 55.430%	train_loss: 1.044	train_acc: 46.08%
INFO: 2017-09-15 23:05:47: main.py:144 **  Train=>
Epoch: 0	Batch_num: 90	lr: 0.000	loss: 0.844	acc: 62.270%	train_loss: 0.9095	train_acc: 53.67%
INFO: 2017-09-15 23:06:08: main.py:144 **  Train=>
Epoch: 0	Batch_num: 120	lr: 0.000	loss: 0.829	acc: 60.880%	train_loss: 0.8002	train_acc: 62.54%
INFO: 2017-09-15 23:06:30: main.py:144 **  Train=>
Epoch: 0	Batch_num: 150	lr: 0.000	loss: 0.817	acc: 61.440%	train_loss: 0.6531	train_acc: 77.1%
INFO: 2017-09-15 23:06:53: main.py:144 **  Train=>
Epoch: 0	Batch_num: 180	lr: 0.000	loss: 0.775	acc: 64.870%	train_loss: 0.7828	train_acc: 66.61%
INFO: 2017-09-15 23:07:15: main.py:144 **  Train=>
Epoch: 0	Batch_num: 210	lr: 0.000	loss: 0.793	acc: 62.040%	train_loss: 0.7629	train_acc: 61.1%
INFO: 2017-09-15 23:07:38: main.py:144 **  Train=>
Epoch: 0	Batch_num: 240	lr: 0.000	loss: 0.751	acc: 65.870%	train_loss: 0.7159	train_acc: 68.71%
INFO: 2017-09-15 23:08:01: main.py:144 **  Train=>
Epoch: 0	Batch_num: 270	lr: 0.000	loss: 0.746	acc: 65.800%	train_loss: 0.6779	train_acc: 70.21%
INFO: 2017-09-15 23:08:24: main.py:144 **  Train=>
Epoch: 0	Batch_num: 300	lr: 0.000	loss: 0.784	acc: 63.560%	train_loss: 0.7965	train_acc: 62.03%
INFO: 2017-09-15 23:08:47: main.py:144 **  Train=>
Epoch: 0	Batch_num: 330	lr: 0.000	loss: 0.757	acc: 64.990%	train_loss: 0.699	train_acc: 67.08%
INFO: 2017-09-15 23:09:10: main.py:144 **  Train=>
Epoch: 0	Batch_num: 360	lr: 0.000	loss: 0.771	acc: 62.880%	train_loss: 0.6932	train_acc: 67.63%
INFO: 2017-09-15 23:09:33: main.py:144 **  Train=>
Epoch: 0	Batch_num: 390	lr: 0.000	loss: 0.741	acc: 65.010%	train_loss: 0.7294	train_acc: 64.59%
INFO: 2017-09-15 23:09:57: main.py:144 **  Train=>
Epoch: 0	Batch_num: 420	lr: 0.000	loss: 0.762	acc: 63.350%	train_loss: 0.5982	train_acc: 75.14%
INFO: 2017-09-15 23:10:20: main.py:144 **  Train=>
Epoch: 0	Batch_num: 450	lr: 0.000	loss: 0.756	acc: 63.450%	train_loss: 0.8758	train_acc: 53.43%
INFO: 2017-09-15 23:10:43: main.py:144 **  Train=>
Epoch: 0	Batch_num: 480	lr: 0.000	loss: 0.743	acc: 64.250%	train_loss: 0.7659	train_acc: 62.45%
INFO: 2017-09-15 23:11:06: main.py:144 **  Train=>
Epoch: 0	Batch_num: 510	lr: 0.000	loss: 0.749	acc: 63.250%	train_loss: 0.621	train_acc: 73.24%
INFO: 2017-09-15 23:11:29: main.py:144 **  Train=>
Epoch: 0	Batch_num: 540	lr: 0.000	loss: 0.736	acc: 64.480%	train_loss: 0.6456	train_acc: 72.81%
INFO: 2017-09-15 23:11:52: main.py:144 **  Train=>
Epoch: 0	Batch_num: 570	lr: 0.000	loss: 0.749	acc: 63.170%	train_loss: 0.7956	train_acc: 58.24%
INFO: 2017-09-15 23:12:16: main.py:144 **  Train=>
Epoch: 0	Batch_num: 600	lr: 0.000	loss: 0.730	acc: 64.220%	train_loss: 0.7059	train_acc: 61.0%
INFO: 2017-09-15 23:12:39: main.py:144 **  Train=>
Epoch: 0	Batch_num: 630	lr: 0.000	loss: 0.749	acc: 62.650%	train_loss: 0.8279	train_acc: 56.3%
INFO: 2017-09-15 23:13:02: main.py:144 **  Train=>
Epoch: 0	Batch_num: 660	lr: 0.000	loss: 0.761	acc: 61.410%	train_loss: 0.7407	train_acc: 64.63%
INFO: 2017-09-15 23:13:25: main.py:144 **  Train=>
Epoch: 0	Batch_num: 690	lr: 0.000	loss: 0.699	acc: 67.240%	train_loss: 0.6123	train_acc: 72.41%
INFO: 2017-09-15 23:13:48: main.py:144 **  Train=>
Epoch: 0	Batch_num: 720	lr: 0.000	loss: 0.767	acc: 61.270%	train_loss: 0.7449	train_acc: 60.86%
INFO: 2017-09-15 23:14:11: main.py:144 **  Train=>
Epoch: 0	Batch_num: 750	lr: 0.000	loss: 0.748	acc: 62.480%	train_loss: 0.8364	train_acc: 54.68%
INFO: 2017-09-15 23:14:35: main.py:144 **  Train=>
Epoch: 0	Batch_num: 780	lr: 0.000	loss: 0.729	acc: 64.420%	train_loss: 0.6399	train_acc: 72.14%
INFO: 2017-09-15 23:14:58: main.py:144 **  Train=>
Epoch: 0	Batch_num: 810	lr: 0.000	loss: 0.723	acc: 64.690%	train_loss: 0.7309	train_acc: 59.49%
INFO: 2017-09-15 23:15:21: main.py:144 **  Train=>
Epoch: 0	Batch_num: 840	lr: 0.000	loss: 0.743	acc: 61.110%	train_loss: 0.842	train_acc: 53.67%
INFO: 2017-09-15 23:15:44: main.py:144 **  Train=>
Epoch: 0	Batch_num: 870	lr: 0.000	loss: 0.760	acc: 61.470%	train_loss: 0.6903	train_acc: 67.17%
INFO: 2017-09-15 23:16:07: main.py:144 **  Train=>
Epoch: 0	Batch_num: 900	lr: 0.000	loss: 0.713	acc: 65.490%	train_loss: 0.8016	train_acc: 58.71%
INFO: 2017-09-15 23:16:30: main.py:144 **  Train=>
Epoch: 0	Batch_num: 930	lr: 0.000	loss: 0.696	acc: 66.470%	train_loss: 0.7168	train_acc: 61.12%
INFO: 2017-09-15 23:16:53: main.py:144 **  Train=>
Epoch: 0	Batch_num: 960	lr: 0.000	loss: 0.747	acc: 61.620%	train_loss: 0.8271	train_acc: 54.42%
INFO: 2017-09-15 23:17:16: main.py:144 **  Train=>
Epoch: 0	Batch_num: 990	lr: 0.000	loss: 0.722	acc: 64.270%	train_loss: 0.7576	train_acc: 62.98%
INFO: 2017-09-15 23:17:39: main.py:144 **  Train=>
Epoch: 0	Batch_num: 1020	lr: 0.000	loss: 0.744	acc: 62.410%	train_loss: 0.7869	train_acc: 62.17%
INFO: 2017-09-15 23:18:02: main.py:144 **  Train=>
Epoch: 0	Batch_num: 1050	lr: 0.000	loss: 0.728	acc: 64.290%	train_loss: 0.6722	train_acc: 70.54%
INFO: 2017-09-15 23:18:25: main.py:144 **  Train=>
Epoch: 0	Batch_num: 1080	lr: 0.000	loss: 0.725	acc: 63.840%	train_loss: 0.6528	train_acc: 71.09%
INFO: 2017-09-15 23:18:48: main.py:144 **  Train=>
Epoch: 0	Batch_num: 1110	lr: 0.000	loss: 0.727	acc: 63.660%	train_loss: 0.6204	train_acc: 72.71%
INFO: 2017-09-15 23:19:11: main.py:144 **  Train=>
Epoch: 0	Batch_num: 1140	lr: 0.000	loss: 0.737	acc: 62.550%	train_loss: 0.7434	train_acc: 59.99%
INFO: 2017-09-15 23:19:34: main.py:144 **  Train=>
Epoch: 0	Batch_num: 1170	lr: 0.000	loss: 0.722	acc: 63.950%	train_loss: 0.8178	train_acc: 55.89%
INFO: 2017-09-15 23:19:58: main.py:144 **  Train=>
Epoch: 0	Batch_num: 1200	lr: 0.000	loss: 0.747	acc: 61.540%	train_loss: 0.7988	train_acc: 57.97%
INFO: 2017-09-15 23:20:21: main.py:144 **  Train=>
Epoch: 0	Batch_num: 1230	lr: 0.000	loss: 0.690	acc: 67.550%	train_loss: 0.7048	train_acc: 67.43%
INFO: 2017-09-15 23:20:44: main.py:144 **  Train=>
Epoch: 0	Batch_num: 1260	lr: 0.000	loss: 0.707	acc: 65.490%	train_loss: 0.6782	train_acc: 65.99%
INFO: 2017-09-15 23:21:07: main.py:144 **  Train=>
Epoch: 0	Batch_num: 1290	lr: 0.000	loss: 0.710	acc: 65.640%	train_loss: 0.6722	train_acc: 71.8%
INFO: 2017-09-15 23:21:30: main.py:144 **  Train=>
Epoch: 0	Batch_num: 1320	lr: 0.000	loss: 0.695	acc: 67.120%	train_loss: 0.6197	train_acc: 75.7%
INFO: 2017-09-15 23:22:27: utils.py:92 **  All dataset size is 5088
INFO: 2017-09-15 23:22:27: utils.py:93 **  Train dataset size is 4071
INFO: 2017-09-15 23:22:27: utils.py:95 **  Valid dataset size is 1017
INFO: 2017-09-15 23:22:27: main.py:218 **  Loading dataset...
INFO: 2017-09-15 23:22:27: main.py:232 **  All data sample counts 5088
INFO: 2017-09-15 23:22:27: main.py:233 **  Train data batch size 2
INFO: 2017-09-15 23:22:27: main.py:234 **  Train data sample counts 4070
INFO: 2017-09-15 23:22:27: main.py:235 **  Valid data sample counts 1018
INFO: 2017-09-15 23:23:17: utils.py:92 **  All dataset size is 5088
INFO: 2017-09-15 23:23:17: utils.py:93 **  Train dataset size is 4071
INFO: 2017-09-15 23:23:17: utils.py:95 **  Valid dataset size is 1017
INFO: 2017-09-15 23:23:17: main.py:218 **  Loading dataset...
INFO: 2017-09-15 23:23:17: main.py:232 **  All data sample counts 5088
INFO: 2017-09-15 23:23:17: main.py:233 **  Train data batch size 2
INFO: 2017-09-15 23:23:17: main.py:234 **  Train data sample counts 4070
INFO: 2017-09-15 23:23:17: main.py:235 **  Valid data sample counts 1018
INFO: 2017-09-15 23:23:19: main.py:144 **  Train=>
Epoch: 0	Batch_num: 0	lr: 0.000	loss: 1.307	acc: 12.710%	train_loss: 1.3071	train_acc: 12.71%
INFO: 2017-09-15 23:23:40: utils.py:92 **  All dataset size is 5088
INFO: 2017-09-15 23:23:40: utils.py:93 **  Train dataset size is 4071
INFO: 2017-09-15 23:23:40: utils.py:95 **  Valid dataset size is 1017
INFO: 2017-09-15 23:23:40: main.py:218 **  Loading dataset...
INFO: 2017-09-15 23:23:40: main.py:232 **  All data sample counts 5088
INFO: 2017-09-15 23:23:40: main.py:233 **  Train data batch size 8
INFO: 2017-09-15 23:23:40: main.py:234 **  Train data sample counts 4064
INFO: 2017-09-15 23:23:40: main.py:235 **  Valid data sample counts 1018
INFO: 2017-09-15 23:23:55: utils.py:92 **  All dataset size is 5088
INFO: 2017-09-15 23:23:55: utils.py:93 **  Train dataset size is 4071
INFO: 2017-09-15 23:23:55: utils.py:95 **  Valid dataset size is 1017
INFO: 2017-09-15 23:23:55: main.py:218 **  Loading dataset...
INFO: 2017-09-15 23:23:55: main.py:232 **  All data sample counts 5088
INFO: 2017-09-15 23:23:55: main.py:233 **  Train data batch size 6
INFO: 2017-09-15 23:23:55: main.py:234 **  Train data sample counts 4068
INFO: 2017-09-15 23:23:55: main.py:235 **  Valid data sample counts 1018
INFO: 2017-09-15 23:24:07: utils.py:92 **  All dataset size is 5088
INFO: 2017-09-15 23:24:07: utils.py:93 **  Train dataset size is 4071
INFO: 2017-09-15 23:24:07: utils.py:95 **  Valid dataset size is 1017
INFO: 2017-09-15 23:24:07: main.py:218 **  Loading dataset...
INFO: 2017-09-15 23:24:07: main.py:232 **  All data sample counts 5088
INFO: 2017-09-15 23:24:07: main.py:233 **  Train data batch size 4
INFO: 2017-09-15 23:24:07: main.py:234 **  Train data sample counts 4068
INFO: 2017-09-15 23:24:07: main.py:235 **  Valid data sample counts 1018
INFO: 2017-09-15 23:24:10: main.py:144 **  Train=>
Epoch: 0	Batch_num: 0	lr: 0.000	loss: 1.385	acc: 23.760%	train_loss: 1.385	train_acc: 23.76%
INFO: 2017-09-15 23:24:34: main.py:144 **  Train=>
Epoch: 0	Batch_num: 30	lr: 0.000	loss: 0.953	acc: 80.100%	train_loss: 0.6542	train_acc: 92.7%
INFO: 2017-09-15 23:24:58: main.py:144 **  Train=>
Epoch: 0	Batch_num: 60	lr: 0.000	loss: 0.379	acc: 94.270%	train_loss: 0.2165	train_acc: 94.93%
INFO: 2017-09-15 23:25:23: main.py:144 **  Train=>
Epoch: 0	Batch_num: 90	lr: 0.000	loss: 0.200	acc: 94.230%	train_loss: 0.1982	train_acc: 93.26%
INFO: 2017-09-15 23:25:49: main.py:144 **  Train=>
Epoch: 0	Batch_num: 120	lr: 0.000	loss: 0.121	acc: 96.350%	train_loss: 0.1127	train_acc: 96.35%
INFO: 2017-09-15 23:26:15: main.py:144 **  Train=>
Epoch: 0	Batch_num: 150	lr: 0.000	loss: 0.100	acc: 96.830%	train_loss: 0.0957	train_acc: 97.23%
INFO: 2017-09-15 23:26:42: main.py:144 **  Train=>
Epoch: 0	Batch_num: 180	lr: 0.000	loss: 0.078	acc: 97.360%	train_loss: 0.0599	train_acc: 97.93%
INFO: 2017-09-15 23:27:09: main.py:144 **  Train=>
Epoch: 0	Batch_num: 210	lr: 0.000	loss: 0.071	acc: 97.540%	train_loss: 0.0758	train_acc: 97.32%
INFO: 2017-09-15 23:27:35: main.py:144 **  Train=>
Epoch: 0	Batch_num: 240	lr: 0.000	loss: 0.064	acc: 97.770%	train_loss: 0.0535	train_acc: 98.2%
INFO: 2017-09-15 23:28:02: main.py:144 **  Train=>
Epoch: 0	Batch_num: 270	lr: 0.000	loss: 0.056	acc: 97.960%	train_loss: 0.0487	train_acc: 98.2%
INFO: 2017-09-15 23:28:28: main.py:144 **  Train=>
Epoch: 0	Batch_num: 300	lr: 0.000	loss: 0.053	acc: 98.000%	train_loss: 0.0498	train_acc: 98.04%
INFO: 2017-09-15 23:28:55: main.py:144 **  Train=>
Epoch: 0	Batch_num: 330	lr: 0.000	loss: 0.051	acc: 98.050%	train_loss: 0.0517	train_acc: 98.13%
INFO: 2017-09-15 23:29:22: main.py:144 **  Train=>
Epoch: 0	Batch_num: 360	lr: 0.000	loss: 0.049	acc: 98.140%	train_loss: 0.0418	train_acc: 98.47%
INFO: 2017-09-15 23:29:48: main.py:144 **  Train=>
Epoch: 0	Batch_num: 390	lr: 0.000	loss: 0.045	acc: 98.280%	train_loss: 0.041	train_acc: 98.44%
INFO: 2017-09-15 23:31:05: utils.py:92 **  All dataset size is 5088
INFO: 2017-09-15 23:31:05: utils.py:93 **  Train dataset size is 4071
INFO: 2017-09-15 23:31:05: utils.py:95 **  Valid dataset size is 1017
INFO: 2017-09-15 23:31:05: main.py:223 **  Loading dataset...
INFO: 2017-09-15 23:31:05: main.py:237 **  All data sample counts 5088
INFO: 2017-09-15 23:31:05: main.py:238 **  Train data batch size 4
INFO: 2017-09-15 23:31:05: main.py:239 **  Train data sample counts 4068
INFO: 2017-09-15 23:31:05: main.py:240 **  Valid data sample counts 1018
INFO: 2017-09-15 23:31:08: main.py:144 **  Train=>
Epoch: 0	Batch_num: 0	lr: 0.000	loss: 1.424	acc: 35.180%	train_loss: 1.4241	train_acc: 35.18%
INFO: 2017-09-15 23:31:58: main.py:144 **  Train=>
Epoch: 0	Batch_num: 60	lr: 0.000	loss: 0.752	acc: 81.820%	train_loss: 0.1964	train_acc: 94.59%
INFO: 2017-09-15 23:32:50: main.py:144 **  Train=>
Epoch: 0	Batch_num: 120	lr: 0.000	loss: 0.145	acc: 95.510%	train_loss: 0.1158	train_acc: 95.76%
INFO: 2017-09-15 23:33:42: main.py:144 **  Train=>
Epoch: 0	Batch_num: 180	lr: 0.000	loss: 0.092	acc: 96.810%	train_loss: 0.071	train_acc: 97.33%
INFO: 2017-09-15 23:34:34: main.py:144 **  Train=>
Epoch: 0	Batch_num: 240	lr: 0.000	loss: 0.072	acc: 97.430%	train_loss: 0.0648	train_acc: 97.72%
INFO: 2017-09-15 23:35:26: main.py:144 **  Train=>
Epoch: 0	Batch_num: 300	lr: 0.000	loss: 0.053	acc: 98.010%	train_loss: 0.0461	train_acc: 98.11%
INFO: 2017-09-15 23:36:18: main.py:144 **  Train=>
Epoch: 0	Batch_num: 360	lr: 0.000	loss: 0.051	acc: 98.020%	train_loss: 0.0411	train_acc: 98.47%
INFO: 2017-09-15 23:37:10: main.py:144 **  Train=>
Epoch: 0	Batch_num: 420	lr: 0.000	loss: 0.046	acc: 98.170%	train_loss: 0.059	train_acc: 97.5%
INFO: 2017-09-15 23:38:02: main.py:144 **  Train=>
Epoch: 0	Batch_num: 480	lr: 0.000	loss: 0.044	acc: 98.260%	train_loss: 0.0687	train_acc: 97.22%
INFO: 2017-09-15 23:38:55: main.py:144 **  Train=>
Epoch: 0	Batch_num: 540	lr: 0.000	loss: 0.058	acc: 97.670%	train_loss: 0.045	train_acc: 98.45%
INFO: 2017-09-15 23:39:48: main.py:144 **  Train=>
Epoch: 0	Batch_num: 600	lr: 0.000	loss: 0.043	acc: 98.250%	train_loss: 0.0439	train_acc: 97.89%
INFO: 2017-09-15 23:40:40: main.py:144 **  Train=>
Epoch: 0	Batch_num: 660	lr: 0.000	loss: 0.039	acc: 98.430%	train_loss: 0.0488	train_acc: 98.18%
INFO: 2017-09-15 23:41:32: main.py:144 **  Train=>
Epoch: 0	Batch_num: 720	lr: 0.000	loss: 0.038	acc: 98.410%	train_loss: 0.0357	train_acc: 98.47%
INFO: 2017-09-15 23:42:26: main.py:144 **  Train=>
Epoch: 0	Batch_num: 780	lr: 0.000	loss: 0.036	acc: 98.500%	train_loss: 0.0366	train_acc: 98.32%
INFO: 2017-09-15 23:43:19: main.py:144 **  Train=>
Epoch: 0	Batch_num: 840	lr: 0.000	loss: 0.034	acc: 98.550%	train_loss: 0.0374	train_acc: 98.4%
INFO: 2017-09-15 23:44:11: main.py:144 **  Train=>
Epoch: 0	Batch_num: 900	lr: 0.000	loss: 0.035	acc: 98.520%	train_loss: 0.0308	train_acc: 98.64%
INFO: 2017-09-15 23:45:03: main.py:144 **  Train=>
Epoch: 0	Batch_num: 960	lr: 0.000	loss: 0.034	acc: 98.560%	train_loss: 0.039	train_acc: 98.32%
INFO: 2017-09-15 23:45:53: main.py:144 **  Train=>
Epoch: 0	Batch_num: 1016	lr: 0.000	loss: 0.034	acc: 98.560%	train_loss: 0.034	train_acc: 98.51%
INFO: 2017-09-15 23:46:58: main.py:83 **  Validate=>
Epoch: 0	Valid_loss: 0.033	Valid_acc: 98.570%
INFO: 2017-09-15 23:47:00: main.py:144 **  Train=>
Epoch: 1	Batch_num: 0	lr: 0.000	loss: 0.027	acc: 98.840%	train_loss: 0.0269	train_acc: 98.84%
INFO: 2017-09-15 23:47:53: main.py:144 **  Train=>
Epoch: 1	Batch_num: 60	lr: 0.000	loss: 0.034	acc: 98.530%	train_loss: 0.0321	train_acc: 98.58%
INFO: 2017-09-15 23:48:45: main.py:144 **  Train=>
Epoch: 1	Batch_num: 120	lr: 0.000	loss: 0.033	acc: 98.590%	train_loss: 0.0308	train_acc: 98.68%
INFO: 2017-09-15 23:49:37: main.py:144 **  Train=>
Epoch: 1	Batch_num: 180	lr: 0.000	loss: 0.034	acc: 98.560%	train_loss: 0.0287	train_acc: 98.64%
INFO: 2017-09-15 23:50:29: main.py:144 **  Train=>
Epoch: 1	Batch_num: 240	lr: 0.000	loss: 0.032	acc: 98.640%	train_loss: 0.0301	train_acc: 98.75%
INFO: 2017-09-15 23:51:21: main.py:144 **  Train=>
Epoch: 1	Batch_num: 300	lr: 0.000	loss: 0.030	acc: 98.650%	train_loss: 0.0298	train_acc: 98.62%
INFO: 2017-09-15 23:52:13: main.py:144 **  Train=>
Epoch: 1	Batch_num: 360	lr: 0.000	loss: 0.030	acc: 98.650%	train_loss: 0.028	train_acc: 98.78%
INFO: 2017-09-15 23:53:07: main.py:144 **  Train=>
Epoch: 1	Batch_num: 420	lr: 0.000	loss: 0.031	acc: 98.630%	train_loss: 0.0388	train_acc: 98.3%
INFO: 2017-09-15 23:54:00: main.py:144 **  Train=>
Epoch: 1	Batch_num: 480	lr: 0.000	loss: 0.030	acc: 98.650%	train_loss: 0.028	train_acc: 98.6%
INFO: 2017-09-15 23:54:52: main.py:144 **  Train=>
Epoch: 1	Batch_num: 540	lr: 0.000	loss: 0.030	acc: 98.670%	train_loss: 0.0299	train_acc: 98.79%
INFO: 2017-09-15 23:55:44: main.py:144 **  Train=>
Epoch: 1	Batch_num: 600	lr: 0.000	loss: 0.029	acc: 98.690%	train_loss: 0.0317	train_acc: 98.43%
INFO: 2017-09-15 23:56:37: main.py:144 **  Train=>
Epoch: 1	Batch_num: 660	lr: 0.000	loss: 0.029	acc: 98.730%	train_loss: 0.0313	train_acc: 98.79%
INFO: 2017-09-15 23:57:30: main.py:144 **  Train=>
Epoch: 1	Batch_num: 720	lr: 0.000	loss: 0.028	acc: 98.710%	train_loss: 0.0311	train_acc: 98.56%
INFO: 2017-09-15 23:58:22: main.py:144 **  Train=>
Epoch: 1	Batch_num: 780	lr: 0.000	loss: 0.028	acc: 98.750%	train_loss: 0.0293	train_acc: 98.57%
INFO: 2017-09-15 23:59:14: main.py:144 **  Train=>
Epoch: 1	Batch_num: 840	lr: 0.000	loss: 0.028	acc: 98.720%	train_loss: 0.0321	train_acc: 98.58%
INFO: 2017-09-16 00:00:09: main.py:144 **  Train=>
Epoch: 1	Batch_num: 900	lr: 0.000	loss: 0.027	acc: 98.760%	train_loss: 0.0234	train_acc: 98.93%
INFO: 2017-09-16 00:01:02: main.py:144 **  Train=>
Epoch: 1	Batch_num: 960	lr: 0.000	loss: 0.027	acc: 98.740%	train_loss: 0.0309	train_acc: 98.56%
INFO: 2017-09-16 00:03:09: utils.py:92 **  All dataset size is 5088
INFO: 2017-09-16 00:03:09: utils.py:93 **  Train dataset size is 4071
INFO: 2017-09-16 00:03:09: utils.py:95 **  Valid dataset size is 1017
INFO: 2017-09-16 00:03:09: main.py:223 **  Loading dataset...
INFO: 2017-09-16 00:03:09: main.py:237 **  All data sample counts 5088
INFO: 2017-09-16 00:03:09: main.py:238 **  Train data batch size 4
INFO: 2017-09-16 00:03:09: main.py:239 **  Train data sample counts 4068
INFO: 2017-09-16 00:03:09: main.py:240 **  Valid data sample counts 1018
INFO: 2017-09-16 00:06:10: utils.py:92 **  All dataset size is 5088
INFO: 2017-09-16 00:06:10: utils.py:93 **  Train dataset size is 4071
INFO: 2017-09-16 00:06:10: utils.py:95 **  Valid dataset size is 1017
INFO: 2017-09-16 00:06:10: main.py:223 **  Loading dataset...
INFO: 2017-09-16 00:06:10: main.py:237 **  All data sample counts 5088
INFO: 2017-09-16 00:06:10: main.py:238 **  Train data batch size 4
INFO: 2017-09-16 00:06:10: main.py:239 **  Train data sample counts 4068
INFO: 2017-09-16 00:06:10: main.py:240 **  Valid data sample counts 1018
INFO: 2017-09-16 00:06:13: main.py:144 **  Train=>
Epoch: 0	Batch_num: 0	lr: 0.000	loss: 1.432	acc: 35.950%	train_loss: 1.4316	train_acc: 35.95%
INFO: 2017-09-16 00:07:03: main.py:144 **  Train=>
Epoch: 0	Batch_num: 60	lr: 0.000	loss: 0.994	acc: 72.860%	train_loss: 0.3185	train_acc: 95.75%
INFO: 2017-09-16 00:07:57: main.py:144 **  Train=>
Epoch: 0	Batch_num: 120	lr: 0.000	loss: 0.189	acc: 94.900%	train_loss: 0.2263	train_acc: 91.63%
INFO: 2017-09-16 00:08:52: main.py:144 **  Train=>
Epoch: 0	Batch_num: 180	lr: 0.000	loss: 0.100	acc: 96.660%	train_loss: 0.0735	train_acc: 97.2%
INFO: 2017-09-16 00:09:48: main.py:144 **  Train=>
Epoch: 0	Batch_num: 240	lr: 0.000	loss: 0.066	acc: 97.680%	train_loss: 0.0586	train_acc: 97.92%
INFO: 2017-09-16 00:10:45: main.py:144 **  Train=>
Epoch: 0	Batch_num: 300	lr: 0.000	loss: 0.056	acc: 97.900%	train_loss: 0.0458	train_acc: 98.2%
INFO: 2017-09-16 00:11:41: main.py:144 **  Train=>
Epoch: 0	Batch_num: 360	lr: 0.000	loss: 0.052	acc: 98.020%	train_loss: 0.0391	train_acc: 98.53%
INFO: 2017-09-16 00:12:37: main.py:144 **  Train=>
Epoch: 0	Batch_num: 420	lr: 0.000	loss: 0.046	acc: 98.200%	train_loss: 0.0584	train_acc: 97.55%
INFO: 2017-09-16 00:13:34: main.py:144 **  Train=>
Epoch: 0	Batch_num: 480	lr: 0.000	loss: 0.043	acc: 98.300%	train_loss: 0.0399	train_acc: 98.26%
INFO: 2017-09-16 00:14:30: main.py:144 **  Train=>
Epoch: 0	Batch_num: 540	lr: 0.000	loss: 0.040	acc: 98.390%	train_loss: 0.0409	train_acc: 98.59%
INFO: 2017-09-16 00:15:27: main.py:144 **  Train=>
Epoch: 0	Batch_num: 600	lr: 0.000	loss: 0.038	acc: 98.440%	train_loss: 0.0425	train_acc: 98.02%
INFO: 2017-09-16 00:16:24: main.py:144 **  Train=>
Epoch: 0	Batch_num: 660	lr: 0.000	loss: 0.039	acc: 98.460%	train_loss: 0.0451	train_acc: 98.35%
INFO: 2017-09-16 00:17:20: main.py:144 **  Train=>
Epoch: 0	Batch_num: 720	lr: 0.000	loss: 0.037	acc: 98.460%	train_loss: 0.0349	train_acc: 98.54%
INFO: 2017-09-16 00:18:17: main.py:144 **  Train=>
Epoch: 0	Batch_num: 780	lr: 0.000	loss: 0.037	acc: 98.480%	train_loss: 0.0382	train_acc: 98.33%
INFO: 2017-09-16 00:19:13: main.py:144 **  Train=>
Epoch: 0	Batch_num: 840	lr: 0.000	loss: 0.043	acc: 98.220%	train_loss: 0.0457	train_acc: 98.04%
INFO: 2017-09-16 00:20:10: main.py:144 **  Train=>
Epoch: 0	Batch_num: 900	lr: 0.000	loss: 0.037	acc: 98.470%	train_loss: 0.0283	train_acc: 98.81%
INFO: 2017-09-16 00:21:06: main.py:144 **  Train=>
Epoch: 0	Batch_num: 960	lr: 0.000	loss: 0.036	acc: 98.520%	train_loss: 0.0381	train_acc: 98.37%
INFO: 2017-09-16 00:21:58: main.py:144 **  Train=>
Epoch: 0	Batch_num: 1016	lr: 0.000	loss: 0.036	acc: 98.520%	train_loss: 0.0354	train_acc: 98.49%
INFO: 2017-09-16 00:23:07: main.py:83 **  Validate=>
Epoch: 0	Valid_loss: 0.034	Valid_acc: 98.540%
INFO: 2017-09-16 00:23:11: main.py:144 **  Train=>
Epoch: 1	Batch_num: 0	lr: 0.000	loss: 0.028	acc: 98.850%	train_loss: 0.0281	train_acc: 98.85%
INFO: 2017-09-16 00:24:07: main.py:144 **  Train=>
Epoch: 1	Batch_num: 60	lr: 0.000	loss: 0.032	acc: 98.640%	train_loss: 0.0281	train_acc: 98.77%
INFO: 2017-09-16 00:25:03: main.py:144 **  Train=>
Epoch: 1	Batch_num: 120	lr: 0.000	loss: 0.033	acc: 98.640%	train_loss: 0.0288	train_acc: 98.76%
INFO: 2017-09-16 00:26:00: main.py:144 **  Train=>
Epoch: 1	Batch_num: 180	lr: 0.000	loss: 0.033	acc: 98.620%	train_loss: 0.0294	train_acc: 98.7%
INFO: 2017-09-16 00:26:56: main.py:144 **  Train=>
Epoch: 1	Batch_num: 240	lr: 0.000	loss: 0.032	acc: 98.660%	train_loss: 0.0324	train_acc: 98.68%
INFO: 2017-09-16 00:27:52: main.py:144 **  Train=>
Epoch: 1	Batch_num: 300	lr: 0.000	loss: 0.029	acc: 98.720%	train_loss: 0.032	train_acc: 98.47%
INFO: 2017-09-16 00:28:48: main.py:144 **  Train=>
Epoch: 1	Batch_num: 360	lr: 0.000	loss: 0.030	acc: 98.710%	train_loss: 0.0267	train_acc: 98.9%
INFO: 2017-09-16 00:29:44: main.py:144 **  Train=>
Epoch: 1	Batch_num: 420	lr: 0.000	loss: 0.030	acc: 98.700%	train_loss: 0.0422	train_acc: 98.13%
INFO: 2017-09-16 00:30:41: main.py:144 **  Train=>
Epoch: 1	Batch_num: 480	lr: 0.000	loss: 0.032	acc: 98.640%	train_loss: 0.0316	train_acc: 98.54%
INFO: 2017-09-16 00:31:37: main.py:144 **  Train=>
Epoch: 1	Batch_num: 540	lr: 0.000	loss: 0.030	acc: 98.710%	train_loss: 0.0305	train_acc: 98.86%
INFO: 2017-09-16 00:32:33: main.py:144 **  Train=>
Epoch: 1	Batch_num: 600	lr: 0.000	loss: 0.029	acc: 98.750%	train_loss: 0.0339	train_acc: 98.38%
INFO: 2017-09-16 00:33:29: main.py:144 **  Train=>
Epoch: 1	Batch_num: 660	lr: 0.000	loss: 0.029	acc: 98.770%	train_loss: 0.0355	train_acc: 98.62%
INFO: 2017-09-16 00:34:26: main.py:144 **  Train=>
Epoch: 1	Batch_num: 720	lr: 0.000	loss: 0.029	acc: 98.740%	train_loss: 0.0319	train_acc: 98.56%
INFO: 2017-09-16 00:35:22: main.py:144 **  Train=>
Epoch: 1	Batch_num: 780	lr: 0.000	loss: 0.028	acc: 98.840%	train_loss: 0.0286	train_acc: 98.67%
INFO: 2017-09-16 00:36:18: main.py:144 **  Train=>
Epoch: 1	Batch_num: 840	lr: 0.000	loss: 0.027	acc: 98.820%	train_loss: 0.0269	train_acc: 98.84%
INFO: 2017-09-16 00:37:14: main.py:144 **  Train=>
Epoch: 1	Batch_num: 900	lr: 0.000	loss: 0.027	acc: 98.850%	train_loss: 0.0225	train_acc: 99.03%
INFO: 2017-09-16 00:38:10: main.py:144 **  Train=>
Epoch: 1	Batch_num: 960	lr: 0.000	loss: 0.027	acc: 98.860%	train_loss: 0.0309	train_acc: 98.66%
INFO: 2017-09-16 00:39:03: main.py:144 **  Train=>
Epoch: 1	Batch_num: 1016	lr: 0.000	loss: 0.027	acc: 98.860%	train_loss: 0.0306	train_acc: 98.64%
INFO: 2017-09-16 00:40:11: main.py:83 **  Validate=>
Epoch: 1	Valid_loss: 0.026	Valid_acc: 98.880%
INFO: 2017-09-16 00:40:15: main.py:144 **  Train=>
Epoch: 2	Batch_num: 0	lr: 0.000	loss: 0.022	acc: 99.080%	train_loss: 0.0224	train_acc: 99.08%
INFO: 2017-09-16 00:41:11: main.py:144 **  Train=>
Epoch: 2	Batch_num: 60	lr: 0.000	loss: 0.025	acc: 98.890%	train_loss: 0.0242	train_acc: 98.93%
INFO: 2017-09-16 00:42:07: main.py:144 **  Train=>
Epoch: 2	Batch_num: 120	lr: 0.000	loss: 0.026	acc: 98.900%	train_loss: 0.0239	train_acc: 98.98%
INFO: 2017-09-16 00:43:03: main.py:144 **  Train=>
Epoch: 2	Batch_num: 180	lr: 0.000	loss: 0.026	acc: 98.890%	train_loss: 0.0219	train_acc: 99.01%
INFO: 2017-09-16 00:43:59: main.py:144 **  Train=>
Epoch: 2	Batch_num: 240	lr: 0.000	loss: 0.026	acc: 98.890%	train_loss: 0.0271	train_acc: 98.89%
INFO: 2017-09-16 00:44:56: main.py:144 **  Train=>
Epoch: 2	Batch_num: 300	lr: 0.000	loss: 0.025	acc: 98.900%	train_loss: 0.0253	train_acc: 98.82%
INFO: 2017-09-16 00:45:52: main.py:144 **  Train=>
Epoch: 2	Batch_num: 360	lr: 0.000	loss: 0.025	acc: 98.940%	train_loss: 0.0225	train_acc: 99.08%
INFO: 2017-09-16 00:46:48: main.py:144 **  Train=>
Epoch: 2	Batch_num: 420	lr: 0.000	loss: 0.025	acc: 98.900%	train_loss: 0.0316	train_acc: 98.63%
INFO: 2017-09-16 00:47:44: main.py:144 **  Train=>
Epoch: 2	Batch_num: 480	lr: 0.000	loss: 0.026	acc: 98.870%	train_loss: 0.0263	train_acc: 98.72%
INFO: 2017-09-16 00:48:40: main.py:144 **  Train=>
Epoch: 2	Batch_num: 540	lr: 0.000	loss: 0.025	acc: 98.910%	train_loss: 0.026	train_acc: 99.02%
INFO: 2017-09-16 00:49:37: main.py:144 **  Train=>
Epoch: 2	Batch_num: 600	lr: 0.000	loss: 0.024	acc: 98.960%	train_loss: 0.0294	train_acc: 98.61%
INFO: 2017-09-16 00:50:33: main.py:144 **  Train=>
Epoch: 2	Batch_num: 660	lr: 0.000	loss: 0.024	acc: 98.970%	train_loss: 0.027	train_acc: 98.96%
INFO: 2017-09-16 00:51:29: main.py:144 **  Train=>
Epoch: 2	Batch_num: 720	lr: 0.000	loss: 0.025	acc: 98.900%	train_loss: 0.0297	train_acc: 98.67%
INFO: 2017-09-16 00:52:25: main.py:144 **  Train=>
Epoch: 2	Batch_num: 780	lr: 0.000	loss: 0.024	acc: 98.980%	train_loss: 0.0239	train_acc: 98.88%
INFO: 2017-09-16 00:53:21: main.py:144 **  Train=>
Epoch: 2	Batch_num: 840	lr: 0.000	loss: 0.023	acc: 98.970%	train_loss: 0.0243	train_acc: 98.95%
INFO: 2017-09-16 00:54:17: main.py:144 **  Train=>
Epoch: 2	Batch_num: 900	lr: 0.000	loss: 0.024	acc: 98.970%	train_loss: 0.0212	train_acc: 99.08%
INFO: 2017-09-16 00:55:13: main.py:144 **  Train=>
Epoch: 2	Batch_num: 960	lr: 0.000	loss: 0.024	acc: 98.950%	train_loss: 0.025	train_acc: 98.89%
INFO: 2017-09-16 00:56:06: main.py:144 **  Train=>
Epoch: 2	Batch_num: 1016	lr: 0.000	loss: 0.024	acc: 98.950%	train_loss: 0.027	train_acc: 98.79%
INFO: 2017-09-16 00:57:14: main.py:83 **  Validate=>
Epoch: 2	Valid_loss: 0.025	Valid_acc: 98.920%
INFO: 2017-09-16 00:57:17: main.py:144 **  Train=>
Epoch: 3	Batch_num: 0	lr: 0.000	loss: 0.021	acc: 99.140%	train_loss: 0.0206	train_acc: 99.14%
INFO: 2017-09-16 00:58:13: main.py:144 **  Train=>
Epoch: 3	Batch_num: 60	lr: 0.000	loss: 0.023	acc: 98.990%	train_loss: 0.0216	train_acc: 99.03%
INFO: 2017-09-16 00:59:09: main.py:144 **  Train=>
Epoch: 3	Batch_num: 120	lr: 0.000	loss: 0.023	acc: 99.010%	train_loss: 0.0218	train_acc: 99.06%
INFO: 2017-09-16 01:00:06: main.py:144 **  Train=>
Epoch: 3	Batch_num: 180	lr: 0.000	loss: 0.024	acc: 98.990%	train_loss: 0.0205	train_acc: 99.05%
INFO: 2017-09-16 01:01:02: main.py:144 **  Train=>
Epoch: 3	Batch_num: 240	lr: 0.000	loss: 0.024	acc: 98.960%	train_loss: 0.0256	train_acc: 98.95%
INFO: 2017-09-16 01:01:58: main.py:144 **  Train=>
Epoch: 3	Batch_num: 300	lr: 0.000	loss: 0.023	acc: 99.000%	train_loss: 0.0205	train_acc: 99.05%
INFO: 2017-09-16 01:02:55: main.py:144 **  Train=>
Epoch: 3	Batch_num: 360	lr: 0.000	loss: 0.022	acc: 99.030%	train_loss: 0.0208	train_acc: 99.12%
INFO: 2017-09-16 01:03:51: main.py:144 **  Train=>
Epoch: 3	Batch_num: 420	lr: 0.000	loss: 0.023	acc: 98.990%	train_loss: 0.0276	train_acc: 98.79%
INFO: 2017-09-16 01:04:47: main.py:144 **  Train=>
Epoch: 3	Batch_num: 480	lr: 0.000	loss: 0.023	acc: 98.990%	train_loss: 0.0216	train_acc: 98.96%
INFO: 2017-09-16 01:05:43: main.py:144 **  Train=>
Epoch: 3	Batch_num: 540	lr: 0.000	loss: 0.023	acc: 99.000%	train_loss: 0.0243	train_acc: 99.06%
INFO: 2017-09-16 01:06:39: main.py:144 **  Train=>
Epoch: 3	Batch_num: 600	lr: 0.000	loss: 0.021	acc: 99.050%	train_loss: 0.0255	train_acc: 98.81%
INFO: 2017-09-16 01:07:36: main.py:144 **  Train=>
Epoch: 3	Batch_num: 660	lr: 0.000	loss: 0.022	acc: 99.060%	train_loss: 0.022	train_acc: 99.16%
INFO: 2017-09-16 01:08:32: main.py:144 **  Train=>
Epoch: 3	Batch_num: 720	lr: 0.000	loss: 0.022	acc: 99.020%	train_loss: 0.0261	train_acc: 98.86%
INFO: 2017-09-16 01:09:28: main.py:144 **  Train=>
Epoch: 3	Batch_num: 780	lr: 0.000	loss: 0.022	acc: 99.060%	train_loss: 0.0214	train_acc: 98.98%
INFO: 2017-09-16 01:10:24: main.py:144 **  Train=>
Epoch: 3	Batch_num: 840	lr: 0.000	loss: 0.021	acc: 99.050%	train_loss: 0.0223	train_acc: 99.02%
INFO: 2017-09-16 01:11:21: main.py:144 **  Train=>
Epoch: 3	Batch_num: 900	lr: 0.000	loss: 0.021	acc: 99.060%	train_loss: 0.0199	train_acc: 99.12%
INFO: 2017-09-16 01:12:17: main.py:144 **  Train=>
Epoch: 3	Batch_num: 960	lr: 0.000	loss: 0.022	acc: 99.050%	train_loss: 0.0228	train_acc: 98.97%
INFO: 2017-09-16 01:13:09: main.py:144 **  Train=>
Epoch: 3	Batch_num: 1016	lr: 0.000	loss: 0.022	acc: 99.050%	train_loss: 0.0226	train_acc: 98.96%
INFO: 2017-09-16 01:14:18: main.py:83 **  Validate=>
Epoch: 3	Valid_loss: 0.024	Valid_acc: 98.970%
INFO: 2017-09-16 01:14:21: main.py:144 **  Train=>
Epoch: 4	Batch_num: 0	lr: 0.000	loss: 0.019	acc: 99.190%	train_loss: 0.0192	train_acc: 99.19%
INFO: 2017-09-16 01:15:18: main.py:144 **  Train=>
Epoch: 4	Batch_num: 60	lr: 0.000	loss: 0.021	acc: 99.060%	train_loss: 0.0209	train_acc: 99.07%
INFO: 2017-09-16 01:16:14: main.py:144 **  Train=>
Epoch: 4	Batch_num: 120	lr: 0.000	loss: 0.021	acc: 99.070%	train_loss: 0.0198	train_acc: 99.16%
INFO: 2017-09-16 01:17:10: main.py:144 **  Train=>
Epoch: 4	Batch_num: 180	lr: 0.000	loss: 0.021	acc: 99.070%	train_loss: 0.019	train_acc: 99.11%
INFO: 2017-09-16 01:18:06: main.py:144 **  Train=>
Epoch: 4	Batch_num: 240	lr: 0.000	loss: 0.022	acc: 99.060%	train_loss: 0.0225	train_acc: 99.06%
INFO: 2017-09-16 01:19:03: main.py:144 **  Train=>
Epoch: 4	Batch_num: 300	lr: 0.000	loss: 0.021	acc: 99.070%	train_loss: 0.0201	train_acc: 99.07%
INFO: 2017-09-16 01:19:59: main.py:144 **  Train=>
Epoch: 4	Batch_num: 360	lr: 0.000	loss: 0.021	acc: 99.080%	train_loss: 0.0191	train_acc: 99.17%
INFO: 2017-09-16 01:20:55: main.py:144 **  Train=>
Epoch: 4	Batch_num: 420	lr: 0.000	loss: 0.022	acc: 99.030%	train_loss: 0.0254	train_acc: 98.85%
INFO: 2017-09-16 01:21:51: main.py:144 **  Train=>
Epoch: 4	Batch_num: 480	lr: 0.000	loss: 0.021	acc: 99.060%	train_loss: 0.0205	train_acc: 99.01%
INFO: 2017-09-16 01:22:48: main.py:144 **  Train=>
Epoch: 4	Batch_num: 540	lr: 0.000	loss: 0.021	acc: 99.060%	train_loss: 0.0219	train_acc: 99.14%
INFO: 2017-09-16 01:23:44: main.py:144 **  Train=>
Epoch: 4	Batch_num: 600	lr: 0.000	loss: 0.020	acc: 99.100%	train_loss: 0.0233	train_acc: 98.86%
INFO: 2017-09-16 01:24:40: main.py:144 **  Train=>
Epoch: 4	Batch_num: 660	lr: 0.000	loss: 0.021	acc: 99.100%	train_loss: 0.0213	train_acc: 99.17%
INFO: 2017-09-16 01:25:36: main.py:144 **  Train=>
Epoch: 4	Batch_num: 720	lr: 0.000	loss: 0.021	acc: 99.070%	train_loss: 0.0251	train_acc: 98.91%
INFO: 2017-09-16 01:26:33: main.py:144 **  Train=>
Epoch: 4	Batch_num: 780	lr: 0.000	loss: 0.022	acc: 99.050%	train_loss: 0.0219	train_acc: 98.96%
INFO: 2017-09-16 01:27:29: main.py:144 **  Train=>
Epoch: 4	Batch_num: 840	lr: 0.000	loss: 0.020	acc: 99.090%	train_loss: 0.0222	train_acc: 99.02%
INFO: 2017-09-16 01:28:25: main.py:144 **  Train=>
Epoch: 4	Batch_num: 900	lr: 0.000	loss: 0.020	acc: 99.100%	train_loss: 0.0201	train_acc: 99.1%
INFO: 2017-09-16 01:29:21: main.py:144 **  Train=>
Epoch: 4	Batch_num: 960	lr: 0.000	loss: 0.021	acc: 99.090%	train_loss: 0.0215	train_acc: 99.02%
INFO: 2017-09-16 01:30:13: main.py:144 **  Train=>
Epoch: 4	Batch_num: 1016	lr: 0.000	loss: 0.021	acc: 99.090%	train_loss: 0.0202	train_acc: 99.07%
INFO: 2017-09-16 01:31:22: main.py:83 **  Validate=>
Epoch: 4	Valid_loss: 0.023	Valid_acc: 98.990%
INFO: 2017-09-16 01:31:26: main.py:144 **  Train=>
Epoch: 5	Batch_num: 0	lr: 0.000	loss: 0.019	acc: 99.190%	train_loss: 0.0189	train_acc: 99.19%
INFO: 2017-09-16 01:32:22: main.py:144 **  Train=>
Epoch: 5	Batch_num: 60	lr: 0.000	loss: 0.020	acc: 99.080%	train_loss: 0.0185	train_acc: 99.18%
INFO: 2017-09-16 01:33:18: main.py:144 **  Train=>
Epoch: 5	Batch_num: 120	lr: 0.000	loss: 0.020	acc: 99.110%	train_loss: 0.0183	train_acc: 99.19%
INFO: 2017-09-16 01:34:14: main.py:144 **  Train=>
Epoch: 5	Batch_num: 180	lr: 0.000	loss: 0.020	acc: 99.110%	train_loss: 0.0183	train_acc: 99.14%
INFO: 2017-09-16 01:35:10: main.py:144 **  Train=>
Epoch: 5	Batch_num: 240	lr: 0.000	loss: 0.020	acc: 99.110%	train_loss: 0.0212	train_acc: 99.11%
INFO: 2017-09-16 01:36:06: main.py:144 **  Train=>
Epoch: 5	Batch_num: 300	lr: 0.000	loss: 0.020	acc: 99.110%	train_loss: 0.0193	train_acc: 99.11%
INFO: 2017-09-16 01:37:02: main.py:144 **  Train=>
Epoch: 5	Batch_num: 360	lr: 0.000	loss: 0.021	acc: 99.080%	train_loss: 0.0187	train_acc: 99.2%
INFO: 2017-09-16 01:37:58: main.py:144 **  Train=>
Epoch: 5	Batch_num: 420	lr: 0.000	loss: 0.021	acc: 99.070%	train_loss: 0.0245	train_acc: 98.89%
INFO: 2017-09-16 01:38:54: main.py:144 **  Train=>
Epoch: 5	Batch_num: 480	lr: 0.000	loss: 0.021	acc: 99.090%	train_loss: 0.0186	train_acc: 99.09%
INFO: 2017-09-16 01:39:50: main.py:144 **  Train=>
Epoch: 5	Batch_num: 540	lr: 0.000	loss: 0.020	acc: 99.110%	train_loss: 0.021	train_acc: 99.19%
INFO: 2017-09-16 01:40:46: main.py:144 **  Train=>
Epoch: 5	Batch_num: 600	lr: 0.000	loss: 0.019	acc: 99.140%	train_loss: 0.0234	train_acc: 98.86%
INFO: 2017-09-16 01:41:42: main.py:144 **  Train=>
Epoch: 5	Batch_num: 660	lr: 0.000	loss: 0.020	acc: 99.140%	train_loss: 0.0216	train_acc: 99.15%
INFO: 2017-09-16 01:42:38: main.py:144 **  Train=>
Epoch: 5	Batch_num: 720	lr: 0.000	loss: 0.020	acc: 99.120%	train_loss: 0.0227	train_acc: 98.99%
INFO: 2017-09-16 01:43:34: main.py:144 **  Train=>
Epoch: 5	Batch_num: 780	lr: 0.000	loss: 0.020	acc: 99.130%	train_loss: 0.0204	train_acc: 99.03%
INFO: 2017-09-16 01:44:30: main.py:144 **  Train=>
Epoch: 5	Batch_num: 840	lr: 0.000	loss: 0.019	acc: 99.120%	train_loss: 0.0207	train_acc: 99.07%
INFO: 2017-09-16 01:45:27: main.py:144 **  Train=>
Epoch: 5	Batch_num: 900	lr: 0.000	loss: 0.020	acc: 99.130%	train_loss: 0.0201	train_acc: 99.11%
INFO: 2017-09-16 01:46:23: main.py:144 **  Train=>
Epoch: 5	Batch_num: 960	lr: 0.000	loss: 0.020	acc: 99.110%	train_loss: 0.0211	train_acc: 99.04%
INFO: 2017-09-16 01:47:15: main.py:144 **  Train=>
Epoch: 5	Batch_num: 1016	lr: 0.000	loss: 0.020	acc: 99.110%	train_loss: 0.0193	train_acc: 99.13%
INFO: 2017-09-16 01:48:23: main.py:83 **  Validate=>
Epoch: 5	Valid_loss: 0.021	Valid_acc: 99.070%
INFO: 2017-09-16 01:48:27: main.py:144 **  Train=>
Epoch: 6	Batch_num: 0	lr: 0.000	loss: 0.018	acc: 99.230%	train_loss: 0.0177	train_acc: 99.23%
INFO: 2017-09-16 01:49:23: main.py:144 **  Train=>
Epoch: 6	Batch_num: 60	lr: 0.000	loss: 0.019	acc: 99.130%	train_loss: 0.0175	train_acc: 99.21%
INFO: 2017-09-16 01:50:19: main.py:144 **  Train=>
Epoch: 6	Batch_num: 120	lr: 0.000	loss: 0.019	acc: 99.150%	train_loss: 0.0166	train_acc: 99.26%
INFO: 2017-09-16 01:51:15: main.py:144 **  Train=>
Epoch: 6	Batch_num: 180	lr: 0.000	loss: 0.019	acc: 99.140%	train_loss: 0.0181	train_acc: 99.16%
INFO: 2017-09-16 01:52:12: main.py:144 **  Train=>
Epoch: 6	Batch_num: 240	lr: 0.000	loss: 0.019	acc: 99.140%	train_loss: 0.0196	train_acc: 99.16%
INFO: 2017-09-16 01:53:08: main.py:144 **  Train=>
Epoch: 6	Batch_num: 300	lr: 0.000	loss: 0.020	acc: 99.110%	train_loss: 0.0198	train_acc: 99.09%
INFO: 2017-09-16 01:54:04: main.py:144 **  Train=>
Epoch: 6	Batch_num: 360	lr: 0.000	loss: 0.072	acc: 97.270%	train_loss: 0.0576	train_acc: 97.81%
INFO: 2017-09-16 01:55:00: main.py:144 **  Train=>
Epoch: 6	Batch_num: 420	lr: 0.000	loss: 0.059	acc: 97.600%	train_loss: 0.0641	train_acc: 97.13%
INFO: 2017-09-16 01:55:56: main.py:144 **  Train=>
Epoch: 6	Batch_num: 480	lr: 0.000	loss: 0.037	acc: 98.420%	train_loss: 0.0321	train_acc: 98.5%
INFO: 2017-09-16 01:56:52: main.py:144 **  Train=>
Epoch: 6	Batch_num: 540	lr: 0.000	loss: 0.035	acc: 98.500%	train_loss: 0.0322	train_acc: 98.84%
INFO: 2017-09-16 01:57:48: main.py:144 **  Train=>
Epoch: 6	Batch_num: 600	lr: 0.000	loss: 0.030	acc: 98.690%	train_loss: 0.032	train_acc: 98.44%
INFO: 2017-09-16 01:58:45: main.py:144 **  Train=>
Epoch: 6	Batch_num: 660	lr: 0.000	loss: 0.029	acc: 98.790%	train_loss: 0.0327	train_acc: 98.78%
INFO: 2017-09-16 01:59:41: main.py:144 **  Train=>
Epoch: 6	Batch_num: 720	lr: 0.000	loss: 0.028	acc: 98.800%	train_loss: 0.029	train_acc: 98.71%
INFO: 2017-09-16 02:00:37: main.py:144 **  Train=>
Epoch: 6	Batch_num: 780	lr: 0.000	loss: 0.026	acc: 98.880%	train_loss: 0.0288	train_acc: 98.71%
INFO: 2017-09-16 02:01:32: main.py:144 **  Train=>
Epoch: 6	Batch_num: 840	lr: 0.000	loss: 0.025	acc: 98.890%	train_loss: 0.027	train_acc: 98.83%
INFO: 2017-09-16 02:02:29: main.py:144 **  Train=>
Epoch: 6	Batch_num: 900	lr: 0.000	loss: 0.026	acc: 98.900%	train_loss: 0.0235	train_acc: 98.97%
INFO: 2017-09-16 02:03:25: main.py:144 **  Train=>
Epoch: 6	Batch_num: 960	lr: 0.000	loss: 0.026	acc: 98.890%	train_loss: 0.0281	train_acc: 98.78%
INFO: 2017-09-16 02:04:17: main.py:144 **  Train=>
Epoch: 6	Batch_num: 1016	lr: 0.000	loss: 0.026	acc: 98.890%	train_loss: 0.0278	train_acc: 98.79%
INFO: 2017-09-16 02:05:25: main.py:83 **  Validate=>
Epoch: 6	Valid_loss: 0.073	Valid_acc: 97.100%
INFO: 2017-09-16 02:05:28: main.py:144 **  Train=>
Epoch: 7	Batch_num: 0	lr: 0.000	loss: 0.021	acc: 99.140%	train_loss: 0.0208	train_acc: 99.14%
INFO: 2017-09-16 02:06:24: main.py:144 **  Train=>
Epoch: 7	Batch_num: 60	lr: 0.000	loss: 0.023	acc: 98.970%	train_loss: 0.023	train_acc: 98.97%
INFO: 2017-09-16 09:18:40: utils.py:92 **  All dataset size is 5088
INFO: 2017-09-16 09:18:40: utils.py:93 **  Train dataset size is 4071
INFO: 2017-09-16 09:18:40: utils.py:95 **  Valid dataset size is 1017
INFO: 2017-09-16 09:18:40: main.py:219 **  Loading dataset...
INFO: 2017-09-16 09:18:40: main.py:233 **  All data sample counts 5088
INFO: 2017-09-16 09:18:40: main.py:234 **  Train data batch size 4
INFO: 2017-09-16 09:18:40: main.py:235 **  Train data sample counts 4068
INFO: 2017-09-16 09:18:40: main.py:236 **  Valid data sample counts 1018
INFO: 2017-09-16 09:18:44: main.py:139 **  Train=>
Epoch: 0	Batch_num: 0	lr: 0.000	loss: 1.435	acc: 37.700%	train_loss: 1.4355	train_acc: 37.7%
INFO: 2017-09-16 09:19:29: main.py:139 **  Train=>
Epoch: 0	Batch_num: 60	lr: 0.000	loss: 0.862	acc: 81.060%	train_loss: 0.2752	train_acc: 94.26%
INFO: 2017-09-16 09:20:16: main.py:139 **  Train=>
Epoch: 0	Batch_num: 120	lr: 0.000	loss: 0.173	acc: 95.070%	train_loss: 0.1661	train_acc: 94.15%
INFO: 2017-09-16 09:21:07: main.py:139 **  Train=>
Epoch: 0	Batch_num: 180	lr: 0.000	loss: 0.126	acc: 95.790%	train_loss: 0.0736	train_acc: 97.22%
INFO: 2017-09-16 09:21:58: main.py:139 **  Train=>
Epoch: 0	Batch_num: 240	lr: 0.000	loss: 0.076	acc: 97.360%	train_loss: 0.0627	train_acc: 97.83%
INFO: 2017-09-16 09:22:49: main.py:139 **  Train=>
Epoch: 0	Batch_num: 300	lr: 0.000	loss: 0.059	acc: 97.830%	train_loss: 0.0642	train_acc: 97.51%
INFO: 2017-09-16 09:23:41: main.py:139 **  Train=>
Epoch: 0	Batch_num: 360	lr: 0.000	loss: 0.053	acc: 97.970%	train_loss: 0.043	train_acc: 98.41%
INFO: 2017-09-16 09:24:33: main.py:139 **  Train=>
Epoch: 0	Batch_num: 420	lr: 0.000	loss: 0.048	acc: 98.170%	train_loss: 0.0597	train_acc: 97.56%
INFO: 2017-09-16 09:25:25: main.py:139 **  Train=>
Epoch: 0	Batch_num: 480	lr: 0.000	loss: 0.048	acc: 98.110%	train_loss: 0.0426	train_acc: 98.08%
INFO: 2017-09-16 09:26:17: main.py:139 **  Train=>
Epoch: 0	Batch_num: 540	lr: 0.000	loss: 0.045	acc: 98.230%	train_loss: 0.0428	train_acc: 98.52%
INFO: 2017-09-16 09:27:09: main.py:139 **  Train=>
Epoch: 0	Batch_num: 600	lr: 0.000	loss: 0.054	acc: 97.870%	train_loss: 0.0767	train_acc: 96.42%
INFO: 2017-09-16 09:28:02: main.py:139 **  Train=>
Epoch: 0	Batch_num: 660	lr: 0.000	loss: 0.046	acc: 98.210%	train_loss: 0.0532	train_acc: 98.02%
INFO: 2017-09-16 09:28:54: main.py:139 **  Train=>
Epoch: 0	Batch_num: 720	lr: 0.000	loss: 0.043	acc: 98.260%	train_loss: 0.0365	train_acc: 98.47%
INFO: 2017-09-16 09:29:46: main.py:139 **  Train=>
Epoch: 0	Batch_num: 780	lr: 0.000	loss: 0.038	acc: 98.450%	train_loss: 0.0383	train_acc: 98.23%
INFO: 2017-09-16 09:30:38: main.py:139 **  Train=>
Epoch: 0	Batch_num: 840	lr: 0.000	loss: 0.035	acc: 98.510%	train_loss: 0.0395	train_acc: 98.33%
INFO: 2017-09-16 09:31:31: main.py:139 **  Train=>
Epoch: 0	Batch_num: 900	lr: 0.000	loss: 0.035	acc: 98.530%	train_loss: 0.0294	train_acc: 98.8%
INFO: 2017-09-16 09:32:24: main.py:139 **  Train=>
Epoch: 0	Batch_num: 960	lr: 0.000	loss: 0.036	acc: 98.480%	train_loss: 0.0404	train_acc: 98.27%
INFO: 2017-09-16 09:33:14: main.py:139 **  Train=>
Epoch: 0	Batch_num: 1016	lr: 0.000	loss: 0.036	acc: 98.480%	train_loss: 0.0372	train_acc: 98.43%
INFO: 2017-09-16 09:34:18: main.py:78 **  Validate=>
Epoch: 0	Valid_loss: 0.052	Valid_acc: 97.780%
INFO: 2017-09-16 09:34:22: main.py:139 **  Train=>
Epoch: 1	Batch_num: 0	lr: 0.000	loss: 0.030	acc: 98.750%	train_loss: 0.0305	train_acc: 98.75%
INFO: 2017-09-16 09:35:14: main.py:139 **  Train=>
Epoch: 1	Batch_num: 60	lr: 0.000	loss: 0.032	acc: 98.620%	train_loss: 0.03	train_acc: 98.69%
INFO: 2017-09-16 09:36:06: main.py:139 **  Train=>
Epoch: 1	Batch_num: 120	lr: 0.000	loss: 0.033	acc: 98.640%	train_loss: 0.0296	train_acc: 98.74%
INFO: 2017-09-16 09:36:59: main.py:139 **  Train=>
Epoch: 1	Batch_num: 180	lr: 0.000	loss: 0.036	acc: 98.510%	train_loss: 0.0312	train_acc: 98.62%
INFO: 2017-09-16 09:37:51: main.py:139 **  Train=>
Epoch: 1	Batch_num: 240	lr: 0.000	loss: 0.033	acc: 98.630%	train_loss: 0.0331	train_acc: 98.67%
INFO: 2017-09-16 09:38:44: main.py:139 **  Train=>
Epoch: 1	Batch_num: 300	lr: 0.000	loss: 0.030	acc: 98.700%	train_loss: 0.0297	train_acc: 98.59%
INFO: 2017-09-16 09:39:37: main.py:139 **  Train=>
Epoch: 1	Batch_num: 360	lr: 0.000	loss: 0.030	acc: 98.680%	train_loss: 0.0296	train_acc: 98.72%
INFO: 2017-09-16 09:40:29: main.py:139 **  Train=>
Epoch: 1	Batch_num: 420	lr: 0.000	loss: 0.032	acc: 98.640%	train_loss: 0.04	train_acc: 98.27%
INFO: 2017-09-16 09:41:23: main.py:139 **  Train=>
Epoch: 1	Batch_num: 480	lr: 0.000	loss: 0.031	acc: 98.660%	train_loss: 0.0304	train_acc: 98.58%
INFO: 2017-09-16 09:42:17: main.py:139 **  Train=>
Epoch: 1	Batch_num: 540	lr: 0.000	loss: 0.031	acc: 98.660%	train_loss: 0.0299	train_acc: 98.87%
INFO: 2017-09-16 09:43:10: main.py:139 **  Train=>
Epoch: 1	Batch_num: 600	lr: 0.000	loss: 0.029	acc: 98.720%	train_loss: 0.0339	train_acc: 98.38%
INFO: 2017-09-16 09:44:03: main.py:139 **  Train=>
Epoch: 1	Batch_num: 660	lr: 0.000	loss: 0.030	acc: 98.740%	train_loss: 0.034	train_acc: 98.66%
INFO: 2017-09-16 09:44:55: main.py:139 **  Train=>
Epoch: 1	Batch_num: 720	lr: 0.000	loss: 0.030	acc: 98.700%	train_loss: 0.0305	train_acc: 98.65%
INFO: 2017-09-16 09:45:50: main.py:139 **  Train=>
Epoch: 1	Batch_num: 780	lr: 0.000	loss: 0.028	acc: 98.800%	train_loss: 0.0272	train_acc: 98.72%
INFO: 2017-09-16 09:46:43: main.py:139 **  Train=>
Epoch: 1	Batch_num: 840	lr: 0.000	loss: 0.027	acc: 98.800%	train_loss: 0.0282	train_acc: 98.77%
INFO: 2017-09-16 09:47:37: main.py:139 **  Train=>
Epoch: 1	Batch_num: 900	lr: 0.000	loss: 0.027	acc: 98.800%	train_loss: 0.0232	train_acc: 98.96%
INFO: 2017-09-16 09:48:37: main.py:139 **  Train=>
Epoch: 1	Batch_num: 960	lr: 0.000	loss: 0.028	acc: 98.780%	train_loss: 0.0305	train_acc: 98.62%
INFO: 2017-09-16 09:49:30: main.py:139 **  Train=>
Epoch: 1	Batch_num: 1016	lr: 0.000	loss: 0.028	acc: 98.780%	train_loss: 0.032	train_acc: 98.58%
INFO: 2017-09-16 09:50:37: main.py:78 **  Validate=>
Epoch: 1	Valid_loss: 0.028	Valid_acc: 98.770%
INFO: 2017-09-16 09:50:41: main.py:139 **  Train=>
Epoch: 2	Batch_num: 0	lr: 0.000	loss: 0.023	acc: 98.970%	train_loss: 0.0233	train_acc: 98.97%
INFO: 2017-09-16 09:51:42: main.py:139 **  Train=>
Epoch: 2	Batch_num: 60	lr: 0.000	loss: 0.026	acc: 98.830%	train_loss: 0.0231	train_acc: 98.95%
INFO: 2017-09-16 09:52:45: main.py:139 **  Train=>
Epoch: 2	Batch_num: 120	lr: 0.000	loss: 0.026	acc: 98.850%	train_loss: 0.0244	train_acc: 98.93%
INFO: 2017-09-16 09:53:49: main.py:139 **  Train=>
Epoch: 2	Batch_num: 180	lr: 0.000	loss: 0.027	acc: 98.820%	train_loss: 0.0236	train_acc: 98.89%
INFO: 2017-09-16 09:54:52: main.py:139 **  Train=>
Epoch: 2	Batch_num: 240	lr: 0.000	loss: 0.027	acc: 98.830%	train_loss: 0.027	train_acc: 98.88%
INFO: 2017-09-16 09:56:00: main.py:139 **  Train=>
Epoch: 2	Batch_num: 300	lr: 0.000	loss: 0.025	acc: 98.860%	train_loss: 0.0276	train_acc: 98.72%
INFO: 2017-09-16 09:56:55: main.py:139 **  Train=>
Epoch: 2	Batch_num: 360	lr: 0.000	loss: 0.027	acc: 98.800%	train_loss: 0.0283	train_acc: 98.8%
INFO: 2017-09-16 09:57:50: main.py:139 **  Train=>
Epoch: 2	Batch_num: 420	lr: 0.000	loss: 0.027	acc: 98.810%	train_loss: 0.033	train_acc: 98.53%
INFO: 2017-09-16 09:58:44: main.py:139 **  Train=>
Epoch: 2	Batch_num: 480	lr: 0.000	loss: 0.026	acc: 98.830%	train_loss: 0.0284	train_acc: 98.64%
INFO: 2017-09-16 09:59:36: main.py:139 **  Train=>
Epoch: 2	Batch_num: 540	lr: 0.000	loss: 0.028	acc: 98.780%	train_loss: 0.0278	train_acc: 98.93%
INFO: 2017-09-16 10:00:28: main.py:139 **  Train=>
Epoch: 2	Batch_num: 600	lr: 0.000	loss: 0.025	acc: 98.860%	train_loss: 0.0298	train_acc: 98.59%
INFO: 2017-09-16 10:01:21: main.py:139 **  Train=>
Epoch: 2	Batch_num: 660	lr: 0.000	loss: 0.026	acc: 98.890%	train_loss: 0.0287	train_acc: 98.89%
INFO: 2017-09-16 10:02:13: main.py:139 **  Train=>
Epoch: 2	Batch_num: 720	lr: 0.000	loss: 0.026	acc: 98.860%	train_loss: 0.0275	train_acc: 98.79%
INFO: 2017-09-16 10:03:07: main.py:139 **  Train=>
Epoch: 2	Batch_num: 780	lr: 0.000	loss: 0.025	acc: 98.930%	train_loss: 0.024	train_acc: 98.89%
INFO: 2017-09-16 10:04:02: main.py:139 **  Train=>
Epoch: 2	Batch_num: 840	lr: 0.000	loss: 0.024	acc: 98.920%	train_loss: 0.0267	train_acc: 98.87%
INFO: 2017-09-16 10:04:56: main.py:139 **  Train=>
Epoch: 2	Batch_num: 900	lr: 0.000	loss: 0.024	acc: 98.940%	train_loss: 0.0218	train_acc: 99.04%
INFO: 2017-09-16 10:05:51: main.py:139 **  Train=>
Epoch: 2	Batch_num: 960	lr: 0.000	loss: 0.024	acc: 98.930%	train_loss: 0.0249	train_acc: 98.86%
INFO: 2017-09-16 10:06:42: main.py:139 **  Train=>
Epoch: 2	Batch_num: 1016	lr: 0.000	loss: 0.024	acc: 98.930%	train_loss: 0.026	train_acc: 98.82%
INFO: 2017-09-16 10:07:54: main.py:78 **  Validate=>
Epoch: 2	Valid_loss: 0.025	Valid_acc: 98.900%
INFO: 2017-09-16 10:07:59: main.py:139 **  Train=>
Epoch: 3	Batch_num: 0	lr: 0.000	loss: 0.022	acc: 99.080%	train_loss: 0.0216	train_acc: 99.08%
INFO: 2017-09-16 10:08:58: main.py:139 **  Train=>
Epoch: 3	Batch_num: 60	lr: 0.000	loss: 0.023	acc: 98.950%	train_loss: 0.0216	train_acc: 99.02%
INFO: 2017-09-16 10:09:53: main.py:139 **  Train=>
Epoch: 3	Batch_num: 120	lr: 0.000	loss: 0.024	acc: 98.980%	train_loss: 0.023	train_acc: 99.02%
INFO: 2017-09-16 10:10:51: main.py:139 **  Train=>
Epoch: 3	Batch_num: 180	lr: 0.000	loss: 0.025	acc: 98.930%	train_loss: 0.0213	train_acc: 99.02%
INFO: 2017-09-16 10:11:51: main.py:139 **  Train=>
Epoch: 3	Batch_num: 240	lr: 0.000	loss: 0.024	acc: 98.940%	train_loss: 0.0241	train_acc: 99.02%
INFO: 2017-09-16 10:12:51: main.py:139 **  Train=>
Epoch: 3	Batch_num: 300	lr: 0.000	loss: 0.023	acc: 98.970%	train_loss: 0.0231	train_acc: 98.93%
INFO: 2017-09-16 10:13:50: main.py:139 **  Train=>
Epoch: 3	Batch_num: 360	lr: 0.000	loss: 0.024	acc: 98.920%	train_loss: 0.0219	train_acc: 99.11%
INFO: 2017-09-16 10:14:51: main.py:139 **  Train=>
Epoch: 3	Batch_num: 420	lr: 0.000	loss: 0.024	acc: 98.930%	train_loss: 0.0284	train_acc: 98.73%
INFO: 2017-09-16 10:15:53: main.py:139 **  Train=>
Epoch: 3	Batch_num: 480	lr: 0.000	loss: 0.024	acc: 98.940%	train_loss: 0.0234	train_acc: 98.86%
INFO: 2017-09-16 10:16:45: main.py:139 **  Train=>
Epoch: 3	Batch_num: 540	lr: 0.000	loss: 0.024	acc: 98.950%	train_loss: 0.0257	train_acc: 99.0%
INFO: 2017-09-16 10:17:37: main.py:139 **  Train=>
Epoch: 3	Batch_num: 600	lr: 0.000	loss: 0.023	acc: 98.990%	train_loss: 0.0254	train_acc: 98.73%
INFO: 2017-09-16 10:18:29: main.py:139 **  Train=>
Epoch: 3	Batch_num: 660	lr: 0.000	loss: 0.023	acc: 99.010%	train_loss: 0.025	train_acc: 99.04%
INFO: 2017-09-16 10:19:22: main.py:139 **  Train=>
Epoch: 3	Batch_num: 720	lr: 0.000	loss: 0.023	acc: 98.960%	train_loss: 0.0257	train_acc: 98.87%
INFO: 2017-09-16 10:20:14: main.py:139 **  Train=>
Epoch: 3	Batch_num: 780	lr: 0.000	loss: 0.022	acc: 99.020%	train_loss: 0.0218	train_acc: 98.98%
INFO: 2017-09-16 10:21:06: main.py:139 **  Train=>
Epoch: 3	Batch_num: 840	lr: 0.000	loss: 0.022	acc: 99.000%	train_loss: 0.0239	train_acc: 98.97%
INFO: 2017-09-16 10:21:58: main.py:139 **  Train=>
Epoch: 3	Batch_num: 900	lr: 0.000	loss: 0.022	acc: 99.030%	train_loss: 0.0203	train_acc: 99.11%
INFO: 2017-09-16 10:22:50: main.py:139 **  Train=>
Epoch: 3	Batch_num: 960	lr: 0.000	loss: 0.022	acc: 99.000%	train_loss: 0.023	train_acc: 98.96%
INFO: 2017-09-16 10:23:39: main.py:139 **  Train=>
Epoch: 3	Batch_num: 1016	lr: 0.000	loss: 0.022	acc: 99.000%	train_loss: 0.0235	train_acc: 98.94%
INFO: 2017-09-16 10:24:43: main.py:78 **  Validate=>
Epoch: 3	Valid_loss: 0.025	Valid_acc: 98.920%
INFO: 2017-09-16 10:24:47: main.py:139 **  Train=>
Epoch: 4	Batch_num: 0	lr: 0.000	loss: 0.020	acc: 99.140%	train_loss: 0.02	train_acc: 99.14%
INFO: 2017-09-16 10:25:39: main.py:139 **  Train=>
Epoch: 4	Batch_num: 60	lr: 0.000	loss: 0.022	acc: 99.000%	train_loss: 0.0243	train_acc: 98.91%
INFO: 2017-09-16 10:26:31: main.py:139 **  Train=>
Epoch: 4	Batch_num: 120	lr: 0.000	loss: 0.023	acc: 99.000%	train_loss: 0.021	train_acc: 99.11%
INFO: 2017-09-16 10:27:27: main.py:139 **  Train=>
Epoch: 4	Batch_num: 180	lr: 0.000	loss: 0.023	acc: 98.990%	train_loss: 0.0201	train_acc: 99.06%
INFO: 2017-09-16 10:28:32: main.py:139 **  Train=>
Epoch: 4	Batch_num: 240	lr: 0.000	loss: 0.023	acc: 99.000%	train_loss: 0.0231	train_acc: 99.05%
INFO: 2017-09-16 10:29:25: main.py:139 **  Train=>
Epoch: 4	Batch_num: 300	lr: 0.000	loss: 0.022	acc: 99.020%	train_loss: 0.0218	train_acc: 99.0%
INFO: 2017-09-16 10:30:19: main.py:139 **  Train=>
Epoch: 4	Batch_num: 360	lr: 0.000	loss: 0.022	acc: 99.010%	train_loss: 0.0198	train_acc: 99.16%
INFO: 2017-09-16 10:31:21: main.py:139 **  Train=>
Epoch: 4	Batch_num: 420	lr: 0.000	loss: 0.023	acc: 98.990%	train_loss: 0.0257	train_acc: 98.77%
INFO: 2017-09-16 10:32:18: main.py:139 **  Train=>
Epoch: 4	Batch_num: 480	lr: 0.000	loss: 0.022	acc: 99.010%	train_loss: 0.0207	train_acc: 98.99%
INFO: 2017-09-16 10:33:10: main.py:139 **  Train=>
Epoch: 4	Batch_num: 540	lr: 0.000	loss: 0.022	acc: 99.030%	train_loss: 0.0236	train_acc: 99.08%
INFO: 2017-09-16 10:34:02: main.py:139 **  Train=>
Epoch: 4	Batch_num: 600	lr: 0.000	loss: 0.021	acc: 99.050%	train_loss: 0.0249	train_acc: 98.75%
INFO: 2017-09-16 10:34:54: main.py:139 **  Train=>
Epoch: 4	Batch_num: 660	lr: 0.000	loss: 0.022	acc: 99.060%	train_loss: 0.022	train_acc: 99.14%
INFO: 2017-09-16 10:35:46: main.py:139 **  Train=>
Epoch: 4	Batch_num: 720	lr: 0.000	loss: 0.022	acc: 99.030%	train_loss: 0.0247	train_acc: 98.91%
INFO: 2017-09-16 10:36:39: main.py:139 **  Train=>
Epoch: 4	Batch_num: 780	lr: 0.000	loss: 0.021	acc: 99.070%	train_loss: 0.0202	train_acc: 99.05%
INFO: 2017-09-16 10:37:31: main.py:139 **  Train=>
Epoch: 4	Batch_num: 840	lr: 0.000	loss: 0.021	acc: 99.060%	train_loss: 0.0226	train_acc: 99.02%
INFO: 2017-09-16 10:38:23: main.py:139 **  Train=>
Epoch: 4	Batch_num: 900	lr: 0.000	loss: 0.021	acc: 99.080%	train_loss: 0.019	train_acc: 99.15%
INFO: 2017-09-16 10:39:15: main.py:139 **  Train=>
Epoch: 4	Batch_num: 960	lr: 0.000	loss: 0.021	acc: 99.060%	train_loss: 0.0227	train_acc: 98.97%
INFO: 2017-09-16 10:40:04: main.py:139 **  Train=>
Epoch: 4	Batch_num: 1016	lr: 0.000	loss: 0.021	acc: 99.060%	train_loss: 0.0225	train_acc: 98.99%
INFO: 2017-09-16 10:41:08: main.py:78 **  Validate=>
Epoch: 4	Valid_loss: 0.022	Valid_acc: 99.020%
INFO: 2017-09-16 10:41:12: main.py:139 **  Train=>
Epoch: 5	Batch_num: 0	lr: 0.000	loss: 0.019	acc: 99.200%	train_loss: 0.0187	train_acc: 99.2%
INFO: 2017-09-16 10:42:04: main.py:139 **  Train=>
Epoch: 5	Batch_num: 60	lr: 0.000	loss: 0.020	acc: 99.080%	train_loss: 0.0197	train_acc: 99.11%
INFO: 2017-09-16 10:42:56: main.py:139 **  Train=>
Epoch: 5	Batch_num: 120	lr: 0.000	loss: 0.021	acc: 99.080%	train_loss: 0.0198	train_acc: 99.16%
INFO: 2017-09-16 10:43:48: main.py:139 **  Train=>
Epoch: 5	Batch_num: 180	lr: 0.000	loss: 0.021	acc: 99.080%	train_loss: 0.0188	train_acc: 99.13%
INFO: 2017-09-16 10:44:40: main.py:139 **  Train=>
Epoch: 5	Batch_num: 240	lr: 0.000	loss: 0.021	acc: 99.050%	train_loss: 0.0218	train_acc: 99.08%
INFO: 2017-09-16 10:45:33: main.py:139 **  Train=>
Epoch: 5	Batch_num: 300	lr: 0.000	loss: 0.021	acc: 99.060%	train_loss: 0.0188	train_acc: 99.12%
INFO: 2017-09-16 10:46:25: main.py:139 **  Train=>
Epoch: 5	Batch_num: 360	lr: 0.000	loss: 0.020	acc: 99.080%	train_loss: 0.0186	train_acc: 99.21%
INFO: 2017-09-16 10:47:17: main.py:139 **  Train=>
Epoch: 5	Batch_num: 420	lr: 0.000	loss: 0.021	acc: 99.050%	train_loss: 0.0237	train_acc: 98.87%
INFO: 2017-09-16 10:48:09: main.py:139 **  Train=>
Epoch: 5	Batch_num: 480	lr: 0.000	loss: 0.021	acc: 99.050%	train_loss: 0.0196	train_acc: 99.04%
INFO: 2017-09-16 10:49:01: main.py:139 **  Train=>
Epoch: 5	Batch_num: 540	lr: 0.000	loss: 0.021	acc: 99.070%	train_loss: 0.0223	train_acc: 99.13%
INFO: 2017-09-16 10:49:53: main.py:139 **  Train=>
Epoch: 5	Batch_num: 600	lr: 0.000	loss: 0.020	acc: 99.090%	train_loss: 0.023	train_acc: 98.85%
INFO: 2017-09-16 10:50:46: main.py:139 **  Train=>
Epoch: 5	Batch_num: 660	lr: 0.000	loss: 0.021	acc: 99.110%	train_loss: 0.0207	train_acc: 99.19%
INFO: 2017-09-16 10:51:38: main.py:139 **  Train=>
Epoch: 5	Batch_num: 720	lr: 0.000	loss: 0.021	acc: 99.080%	train_loss: 0.0239	train_acc: 98.93%
INFO: 2017-09-16 10:52:30: main.py:139 **  Train=>
Epoch: 5	Batch_num: 780	lr: 0.000	loss: 0.020	acc: 99.110%	train_loss: 0.0194	train_acc: 99.08%
INFO: 2017-09-16 10:53:22: main.py:139 **  Train=>
Epoch: 5	Batch_num: 840	lr: 0.000	loss: 0.020	acc: 99.110%	train_loss: 0.0221	train_acc: 99.05%
INFO: 2017-09-16 10:54:14: main.py:139 **  Train=>
Epoch: 5	Batch_num: 900	lr: 0.000	loss: 0.020	acc: 99.120%	train_loss: 0.0185	train_acc: 99.18%
INFO: 2017-09-16 10:55:06: main.py:139 **  Train=>
Epoch: 5	Batch_num: 960	lr: 0.000	loss: 0.020	acc: 99.110%	train_loss: 0.0223	train_acc: 98.98%
INFO: 2017-09-16 10:55:55: main.py:139 **  Train=>
Epoch: 5	Batch_num: 1016	lr: 0.000	loss: 0.020	acc: 99.110%	train_loss: 0.0198	train_acc: 99.11%
INFO: 2017-09-16 10:56:59: main.py:78 **  Validate=>
Epoch: 5	Valid_loss: 0.021	Valid_acc: 99.070%
INFO: 2017-09-16 10:57:03: main.py:139 **  Train=>
Epoch: 6	Batch_num: 0	lr: 0.000	loss: 0.018	acc: 99.240%	train_loss: 0.0178	train_acc: 99.24%
INFO: 2017-09-16 10:57:56: main.py:139 **  Train=>
Epoch: 6	Batch_num: 60	lr: 0.000	loss: 0.019	acc: 99.110%	train_loss: 0.0187	train_acc: 99.15%
INFO: 2017-09-16 10:58:50: main.py:139 **  Train=>
Epoch: 6	Batch_num: 120	lr: 0.000	loss: 0.020	acc: 99.120%	train_loss: 0.019	train_acc: 99.19%
INFO: 2017-09-16 10:59:43: main.py:139 **  Train=>
Epoch: 6	Batch_num: 180	lr: 0.000	loss: 0.020	acc: 99.120%	train_loss: 0.0181	train_acc: 99.16%
INFO: 2017-09-16 11:00:36: main.py:139 **  Train=>
Epoch: 6	Batch_num: 240	lr: 0.000	loss: 0.020	acc: 99.100%	train_loss: 0.0211	train_acc: 99.12%
INFO: 2017-09-16 11:01:31: main.py:139 **  Train=>
Epoch: 6	Batch_num: 300	lr: 0.000	loss: 0.020	acc: 99.110%	train_loss: 0.0186	train_acc: 99.13%
INFO: 2017-09-16 11:02:24: main.py:139 **  Train=>
Epoch: 6	Batch_num: 360	lr: 0.000	loss: 0.019	acc: 99.120%	train_loss: 0.0184	train_acc: 99.22%
INFO: 2017-09-16 11:03:19: main.py:139 **  Train=>
Epoch: 6	Batch_num: 420	lr: 0.000	loss: 0.020	acc: 99.090%	train_loss: 0.0223	train_acc: 98.94%
INFO: 2017-09-16 11:04:23: main.py:139 **  Train=>
Epoch: 6	Batch_num: 480	lr: 0.000	loss: 0.020	acc: 99.100%	train_loss: 0.0187	train_acc: 99.07%
INFO: 2017-09-16 11:05:18: main.py:139 **  Train=>
Epoch: 6	Batch_num: 540	lr: 0.000	loss: 0.020	acc: 99.110%	train_loss: 0.0216	train_acc: 99.14%
INFO: 2017-09-16 11:06:13: main.py:139 **  Train=>
Epoch: 6	Batch_num: 600	lr: 0.000	loss: 0.019	acc: 99.130%	train_loss: 0.0223	train_acc: 98.89%
INFO: 2017-09-16 11:07:17: main.py:139 **  Train=>
Epoch: 6	Batch_num: 660	lr: 0.000	loss: 0.020	acc: 99.150%	train_loss: 0.0198	train_acc: 99.2%
INFO: 2017-09-16 11:08:13: main.py:139 **  Train=>
Epoch: 6	Batch_num: 720	lr: 0.000	loss: 0.019	acc: 99.120%	train_loss: 0.0227	train_acc: 98.98%
INFO: 2017-09-16 11:09:06: main.py:139 **  Train=>
Epoch: 6	Batch_num: 780	lr: 0.000	loss: 0.019	acc: 99.140%	train_loss: 0.0197	train_acc: 99.06%
INFO: 2017-09-16 11:10:01: main.py:139 **  Train=>
Epoch: 6	Batch_num: 840	lr: 0.000	loss: 0.019	acc: 99.130%	train_loss: 0.0213	train_acc: 99.08%
INFO: 2017-09-16 11:10:58: main.py:139 **  Train=>
Epoch: 6	Batch_num: 900	lr: 0.000	loss: 0.019	acc: 99.140%	train_loss: 0.0183	train_acc: 99.18%
INFO: 2017-09-16 11:11:56: main.py:139 **  Train=>
Epoch: 6	Batch_num: 960	lr: 0.000	loss: 0.019	acc: 99.140%	train_loss: 0.021	train_acc: 99.03%
INFO: 2017-09-16 11:12:50: main.py:139 **  Train=>
Epoch: 6	Batch_num: 1016	lr: 0.000	loss: 0.019	acc: 99.140%	train_loss: 0.0196	train_acc: 99.11%
INFO: 2017-09-16 11:13:56: main.py:78 **  Validate=>
Epoch: 6	Valid_loss: 0.021	Valid_acc: 99.050%
INFO: 2017-09-16 11:13:58: main.py:139 **  Train=>
Epoch: 7	Batch_num: 0	lr: 0.000	loss: 0.018	acc: 99.240%	train_loss: 0.0177	train_acc: 99.24%
INFO: 2017-09-16 11:14:55: main.py:139 **  Train=>
Epoch: 7	Batch_num: 60	lr: 0.000	loss: 0.019	acc: 99.140%	train_loss: 0.0176	train_acc: 99.21%
INFO: 2017-09-16 11:15:53: main.py:139 **  Train=>
Epoch: 7	Batch_num: 120	lr: 0.000	loss: 0.019	acc: 99.150%	train_loss: 0.019	train_acc: 99.17%
INFO: 2017-09-16 11:16:51: main.py:139 **  Train=>
Epoch: 7	Batch_num: 180	lr: 0.000	loss: 0.019	acc: 99.140%	train_loss: 0.0178	train_acc: 99.18%
INFO: 2017-09-16 11:17:45: main.py:139 **  Train=>
Epoch: 7	Batch_num: 240	lr: 0.000	loss: 0.020	acc: 99.130%	train_loss: 0.0199	train_acc: 99.17%
INFO: 2017-09-16 11:18:37: main.py:139 **  Train=>
Epoch: 7	Batch_num: 300	lr: 0.000	loss: 0.019	acc: 99.130%	train_loss: 0.0178	train_acc: 99.16%
INFO: 2017-09-16 11:19:32: main.py:139 **  Train=>
Epoch: 7	Batch_num: 360	lr: 0.000	loss: 0.019	acc: 99.150%	train_loss: 0.0181	train_acc: 99.23%
INFO: 2017-09-16 11:20:27: main.py:139 **  Train=>
Epoch: 7	Batch_num: 420	lr: 0.000	loss: 0.019	acc: 99.130%	train_loss: 0.0219	train_acc: 98.97%
INFO: 2017-09-16 11:21:21: main.py:139 **  Train=>
Epoch: 7	Batch_num: 480	lr: 0.000	loss: 0.019	acc: 99.130%	train_loss: 0.018	train_acc: 99.11%
INFO: 2017-09-16 11:22:14: main.py:139 **  Train=>
Epoch: 7	Batch_num: 540	lr: 0.000	loss: 0.019	acc: 99.140%	train_loss: 0.0204	train_acc: 99.18%
INFO: 2017-09-16 11:23:07: main.py:139 **  Train=>
Epoch: 7	Batch_num: 600	lr: 0.000	loss: 0.018	acc: 99.160%	train_loss: 0.0213	train_acc: 98.93%
INFO: 2017-09-16 11:24:01: main.py:139 **  Train=>
Epoch: 7	Batch_num: 660	lr: 0.000	loss: 0.019	acc: 99.170%	train_loss: 0.0191	train_acc: 99.22%
INFO: 2017-09-16 11:24:54: main.py:139 **  Train=>
Epoch: 7	Batch_num: 720	lr: 0.000	loss: 0.019	acc: 99.150%	train_loss: 0.0217	train_acc: 99.02%
INFO: 2017-09-16 11:25:46: main.py:139 **  Train=>
Epoch: 7	Batch_num: 780	lr: 0.000	loss: 0.019	acc: 99.160%	train_loss: 0.0183	train_acc: 99.13%
INFO: 2017-09-16 11:26:39: main.py:139 **  Train=>
Epoch: 7	Batch_num: 840	lr: 0.000	loss: 0.018	acc: 99.160%	train_loss: 0.021	train_acc: 99.08%
INFO: 2017-09-16 11:27:31: main.py:139 **  Train=>
Epoch: 7	Batch_num: 900	lr: 0.000	loss: 0.018	acc: 99.170%	train_loss: 0.0176	train_acc: 99.21%
INFO: 2017-09-16 11:28:23: main.py:139 **  Train=>
Epoch: 7	Batch_num: 960	lr: 0.000	loss: 0.019	acc: 99.150%	train_loss: 0.0199	train_acc: 99.09%
INFO: 2017-09-16 11:29:12: main.py:139 **  Train=>
Epoch: 7	Batch_num: 1016	lr: 0.000	loss: 0.019	acc: 99.150%	train_loss: 0.0177	train_acc: 99.2%
INFO: 2017-09-16 11:30:16: main.py:78 **  Validate=>
Epoch: 7	Valid_loss: 0.021	Valid_acc: 99.080%
INFO: 2017-09-16 11:30:19: main.py:139 **  Train=>
Epoch: 8	Batch_num: 0	lr: 0.000	loss: 0.017	acc: 99.260%	train_loss: 0.0173	train_acc: 99.26%
INFO: 2017-09-16 11:31:11: main.py:139 **  Train=>
Epoch: 8	Batch_num: 60	lr: 0.000	loss: 0.018	acc: 99.170%	train_loss: 0.0168	train_acc: 99.24%
INFO: 2017-09-16 11:32:03: main.py:139 **  Train=>
Epoch: 8	Batch_num: 120	lr: 0.000	loss: 0.019	acc: 99.170%	train_loss: 0.0177	train_acc: 99.21%
INFO: 2017-09-16 11:32:56: main.py:139 **  Train=>
Epoch: 8	Batch_num: 180	lr: 0.000	loss: 0.019	acc: 99.170%	train_loss: 0.0179	train_acc: 99.19%
INFO: 2017-09-16 11:33:48: main.py:139 **  Train=>
Epoch: 8	Batch_num: 240	lr: 0.000	loss: 0.019	acc: 99.150%	train_loss: 0.0193	train_acc: 99.19%
INFO: 2017-09-16 11:34:41: main.py:139 **  Train=>
Epoch: 8	Batch_num: 300	lr: 0.000	loss: 0.019	acc: 99.150%	train_loss: 0.0178	train_acc: 99.15%
INFO: 2017-09-16 11:35:33: main.py:139 **  Train=>
Epoch: 8	Batch_num: 360	lr: 0.000	loss: 0.018	acc: 99.160%	train_loss: 0.0175	train_acc: 99.26%
INFO: 2017-09-16 11:36:25: main.py:139 **  Train=>
Epoch: 8	Batch_num: 420	lr: 0.000	loss: 0.019	acc: 99.150%	train_loss: 0.0206	train_acc: 99.02%
INFO: 2017-09-16 11:37:17: main.py:139 **  Train=>
Epoch: 8	Batch_num: 480	lr: 0.000	loss: 0.019	acc: 99.160%	train_loss: 0.0178	train_acc: 99.1%
INFO: 2017-09-16 11:38:09: main.py:139 **  Train=>
Epoch: 8	Batch_num: 540	lr: 0.000	loss: 0.019	acc: 99.160%	train_loss: 0.0202	train_acc: 99.21%
INFO: 2017-09-16 11:39:02: main.py:139 **  Train=>
Epoch: 8	Batch_num: 600	lr: 0.000	loss: 0.018	acc: 99.170%	train_loss: 0.0193	train_acc: 99.03%
INFO: 2017-09-16 11:39:54: main.py:139 **  Train=>
Epoch: 8	Batch_num: 660	lr: 0.000	loss: 0.018	acc: 99.200%	train_loss: 0.0189	train_acc: 99.23%
INFO: 2017-09-16 11:40:46: main.py:139 **  Train=>
Epoch: 8	Batch_num: 720	lr: 0.000	loss: 0.018	acc: 99.170%	train_loss: 0.0214	train_acc: 99.04%
INFO: 2017-09-16 11:41:38: main.py:139 **  Train=>
Epoch: 8	Batch_num: 780	lr: 0.000	loss: 0.019	acc: 99.180%	train_loss: 0.0186	train_acc: 99.13%
INFO: 2017-09-16 11:42:30: main.py:139 **  Train=>
Epoch: 8	Batch_num: 840	lr: 0.000	loss: 0.018	acc: 99.170%	train_loss: 0.0206	train_acc: 99.09%
INFO: 2017-09-16 11:43:22: main.py:139 **  Train=>
Epoch: 8	Batch_num: 900	lr: 0.000	loss: 0.018	acc: 99.180%	train_loss: 0.0174	train_acc: 99.2%
INFO: 2017-09-16 11:44:15: main.py:139 **  Train=>
Epoch: 8	Batch_num: 960	lr: 0.000	loss: 0.018	acc: 99.180%	train_loss: 0.019	train_acc: 99.14%
INFO: 2017-09-16 11:45:03: main.py:139 **  Train=>
Epoch: 8	Batch_num: 1016	lr: 0.000	loss: 0.018	acc: 99.180%	train_loss: 0.0167	train_acc: 99.25%
INFO: 2017-09-16 11:46:08: main.py:78 **  Validate=>
Epoch: 8	Valid_loss: 0.020	Valid_acc: 99.110%
INFO: 2017-09-16 11:46:11: main.py:139 **  Train=>
Epoch: 9	Batch_num: 0	lr: 0.000	loss: 0.017	acc: 99.300%	train_loss: 0.0165	train_acc: 99.3%
INFO: 2017-09-16 11:47:03: main.py:139 **  Train=>
Epoch: 9	Batch_num: 60	lr: 0.000	loss: 0.018	acc: 99.180%	train_loss: 0.0171	train_acc: 99.22%
INFO: 2017-09-16 11:47:55: main.py:139 **  Train=>
Epoch: 9	Batch_num: 120	lr: 0.000	loss: 0.018	acc: 99.190%	train_loss: 0.0163	train_acc: 99.26%
INFO: 2017-09-16 11:48:47: main.py:139 **  Train=>
Epoch: 9	Batch_num: 180	lr: 0.000	loss: 0.019	acc: 99.160%	train_loss: 0.0175	train_acc: 99.2%
INFO: 2017-09-16 11:49:39: main.py:139 **  Train=>
Epoch: 9	Batch_num: 240	lr: 0.000	loss: 0.019	acc: 99.160%	train_loss: 0.0213	train_acc: 99.12%
INFO: 2017-09-16 11:50:31: main.py:139 **  Train=>
Epoch: 9	Batch_num: 300	lr: 0.000	loss: 0.018	acc: 99.170%	train_loss: 0.0175	train_acc: 99.16%
INFO: 2017-09-16 11:51:24: main.py:139 **  Train=>
Epoch: 9	Batch_num: 360	lr: 0.000	loss: 0.018	acc: 99.180%	train_loss: 0.0172	train_acc: 99.27%
INFO: 2017-09-16 11:52:16: main.py:139 **  Train=>
Epoch: 9	Batch_num: 420	lr: 0.000	loss: 0.018	acc: 99.160%	train_loss: 0.0203	train_acc: 99.04%
INFO: 2017-09-16 11:53:08: main.py:139 **  Train=>
Epoch: 9	Batch_num: 480	lr: 0.000	loss: 0.018	acc: 99.170%	train_loss: 0.0173	train_acc: 99.12%
INFO: 2017-09-16 11:54:03: main.py:139 **  Train=>
Epoch: 9	Batch_num: 540	lr: 0.000	loss: 0.018	acc: 99.170%	train_loss: 0.0193	train_acc: 99.22%
INFO: 2017-09-16 11:54:58: main.py:139 **  Train=>
Epoch: 9	Batch_num: 600	lr: 0.000	loss: 0.018	acc: 99.200%	train_loss: 0.019	train_acc: 99.04%
INFO: 2017-09-16 11:55:50: main.py:139 **  Train=>
Epoch: 9	Batch_num: 660	lr: 0.000	loss: 0.018	acc: 99.210%	train_loss: 0.0184	train_acc: 99.24%
INFO: 2017-09-16 11:56:42: main.py:139 **  Train=>
Epoch: 9	Batch_num: 720	lr: 0.000	loss: 0.018	acc: 99.180%	train_loss: 0.0204	train_acc: 99.08%
INFO: 2017-09-16 11:57:34: main.py:139 **  Train=>
Epoch: 9	Batch_num: 780	lr: 0.000	loss: 0.018	acc: 99.200%	train_loss: 0.0175	train_acc: 99.18%
INFO: 2017-09-16 11:58:26: main.py:139 **  Train=>
Epoch: 9	Batch_num: 840	lr: 0.000	loss: 0.018	acc: 99.180%	train_loss: 0.0199	train_acc: 99.12%
INFO: 2017-09-16 11:59:18: main.py:139 **  Train=>
Epoch: 9	Batch_num: 900	lr: 0.000	loss: 0.018	acc: 99.190%	train_loss: 0.0171	train_acc: 99.23%
INFO: 2017-09-16 12:00:10: main.py:139 **  Train=>
Epoch: 9	Batch_num: 960	lr: 0.000	loss: 0.018	acc: 99.200%	train_loss: 0.0185	train_acc: 99.16%
INFO: 2017-09-16 12:00:58: main.py:139 **  Train=>
Epoch: 9	Batch_num: 1016	lr: 0.000	loss: 0.018	acc: 99.200%	train_loss: 0.0167	train_acc: 99.25%
INFO: 2017-09-16 12:02:02: main.py:78 **  Validate=>
Epoch: 9	Valid_loss: 0.020	Valid_acc: 99.120%
INFO: 2017-09-16 12:02:06: main.py:139 **  Train=>
Epoch: 10	Batch_num: 0	lr: 0.000	loss: 0.016	acc: 99.310%	train_loss: 0.0162	train_acc: 99.31%
INFO: 2017-09-16 12:02:58: main.py:139 **  Train=>
Epoch: 10	Batch_num: 60	lr: 0.000	loss: 0.018	acc: 99.200%	train_loss: 0.0168	train_acc: 99.24%
INFO: 2017-09-16 12:03:50: main.py:139 **  Train=>
Epoch: 10	Batch_num: 120	lr: 0.000	loss: 0.018	acc: 99.210%	train_loss: 0.0157	train_acc: 99.31%
INFO: 2017-09-16 12:04:42: main.py:139 **  Train=>
Epoch: 10	Batch_num: 180	lr: 0.000	loss: 0.018	acc: 99.190%	train_loss: 0.0171	train_acc: 99.21%
INFO: 2017-09-16 12:05:34: main.py:139 **  Train=>
Epoch: 10	Batch_num: 240	lr: 0.000	loss: 0.019	acc: 99.150%	train_loss: 0.0201	train_acc: 99.16%
INFO: 2017-09-16 12:06:26: main.py:139 **  Train=>
Epoch: 10	Batch_num: 300	lr: 0.000	loss: 0.018	acc: 99.170%	train_loss: 0.0173	train_acc: 99.18%
INFO: 2017-09-16 12:07:18: main.py:139 **  Train=>
Epoch: 10	Batch_num: 360	lr: 0.000	loss: 0.018	acc: 99.200%	train_loss: 0.0173	train_acc: 99.25%
INFO: 2017-09-16 12:08:10: main.py:139 **  Train=>
Epoch: 10	Batch_num: 420	lr: 0.000	loss: 0.018	acc: 99.180%	train_loss: 0.0191	train_acc: 99.08%
INFO: 2017-09-16 12:09:02: main.py:139 **  Train=>
Epoch: 10	Batch_num: 480	lr: 0.000	loss: 0.018	acc: 99.200%	train_loss: 0.0174	train_acc: 99.12%
INFO: 2017-09-16 12:09:54: main.py:139 **  Train=>
Epoch: 10	Batch_num: 540	lr: 0.000	loss: 0.018	acc: 99.180%	train_loss: 0.0194	train_acc: 99.22%
INFO: 2017-09-16 12:10:46: main.py:139 **  Train=>
Epoch: 10	Batch_num: 600	lr: 0.000	loss: 0.017	acc: 99.210%	train_loss: 0.0182	train_acc: 99.07%
INFO: 2017-09-16 12:11:38: main.py:139 **  Train=>
Epoch: 10	Batch_num: 660	lr: 0.000	loss: 0.018	acc: 99.230%	train_loss: 0.0175	train_acc: 99.28%
INFO: 2017-09-16 12:12:30: main.py:139 **  Train=>
Epoch: 10	Batch_num: 720	lr: 0.000	loss: 0.018	acc: 99.190%	train_loss: 0.0208	train_acc: 99.06%
INFO: 2017-09-16 12:13:22: main.py:139 **  Train=>
Epoch: 10	Batch_num: 780	lr: 0.000	loss: 0.018	acc: 99.210%	train_loss: 0.0172	train_acc: 99.2%
INFO: 2017-09-16 12:14:14: main.py:139 **  Train=>
Epoch: 10	Batch_num: 840	lr: 0.000	loss: 0.017	acc: 99.200%	train_loss: 0.0195	train_acc: 99.13%
INFO: 2017-09-16 12:15:06: main.py:139 **  Train=>
Epoch: 10	Batch_num: 900	lr: 0.000	loss: 0.018	acc: 99.200%	train_loss: 0.017	train_acc: 99.23%
INFO: 2017-09-16 12:15:58: main.py:139 **  Train=>
Epoch: 10	Batch_num: 960	lr: 0.000	loss: 0.017	acc: 99.210%	train_loss: 0.0179	train_acc: 99.19%
INFO: 2017-09-16 12:16:46: main.py:139 **  Train=>
Epoch: 10	Batch_num: 1016	lr: 0.000	loss: 0.017	acc: 99.210%	train_loss: 0.016	train_acc: 99.28%
INFO: 2017-09-16 12:17:50: main.py:78 **  Validate=>
Epoch: 10	Valid_loss: 0.019	Valid_acc: 99.140%
INFO: 2017-09-16 12:17:55: main.py:139 **  Train=>
Epoch: 11	Batch_num: 0	lr: 0.000	loss: 0.016	acc: 99.310%	train_loss: 0.0162	train_acc: 99.31%
INFO: 2017-09-16 12:18:46: main.py:139 **  Train=>
Epoch: 11	Batch_num: 60	lr: 0.000	loss: 0.017	acc: 99.210%	train_loss: 0.0161	train_acc: 99.27%
INFO: 2017-09-16 12:19:39: main.py:139 **  Train=>
Epoch: 11	Batch_num: 120	lr: 0.000	loss: 0.018	acc: 99.220%	train_loss: 0.0155	train_acc: 99.3%
INFO: 2017-09-16 12:20:31: main.py:139 **  Train=>
Epoch: 11	Batch_num: 180	lr: 0.000	loss: 0.018	acc: 99.210%	train_loss: 0.0174	train_acc: 99.19%
INFO: 2017-09-16 12:21:23: main.py:139 **  Train=>
Epoch: 11	Batch_num: 240	lr: 0.000	loss: 0.018	acc: 99.190%	train_loss: 0.0185	train_acc: 99.21%
INFO: 2017-09-16 12:22:15: main.py:139 **  Train=>
Epoch: 11	Batch_num: 300	lr: 0.000	loss: 0.018	acc: 99.190%	train_loss: 0.0164	train_acc: 99.2%
INFO: 2017-09-16 12:23:07: main.py:139 **  Train=>
Epoch: 11	Batch_num: 360	lr: 0.000	loss: 0.017	acc: 99.220%	train_loss: 0.0171	train_acc: 99.26%
INFO: 2017-09-16 12:24:00: main.py:139 **  Train=>
Epoch: 11	Batch_num: 420	lr: 0.000	loss: 0.018	acc: 99.190%	train_loss: 0.0202	train_acc: 99.04%
INFO: 2017-09-16 12:24:52: main.py:139 **  Train=>
Epoch: 11	Batch_num: 480	lr: 0.000	loss: 0.018	acc: 99.200%	train_loss: 0.0167	train_acc: 99.15%
INFO: 2017-09-16 12:25:44: main.py:139 **  Train=>
Epoch: 11	Batch_num: 540	lr: 0.000	loss: 0.018	acc: 99.190%	train_loss: 0.0187	train_acc: 99.25%
INFO: 2017-09-16 12:26:36: main.py:139 **  Train=>
Epoch: 11	Batch_num: 600	lr: 0.000	loss: 0.017	acc: 99.220%	train_loss: 0.018	train_acc: 99.09%
INFO: 2017-09-16 12:27:29: main.py:139 **  Train=>
Epoch: 11	Batch_num: 660	lr: 0.000	loss: 0.017	acc: 99.230%	train_loss: 0.017	train_acc: 99.3%
INFO: 2017-09-16 12:28:21: main.py:139 **  Train=>
Epoch: 11	Batch_num: 720	lr: 0.000	loss: 0.019	acc: 99.160%	train_loss: 0.0225	train_acc: 98.98%
INFO: 2017-09-16 12:29:13: main.py:139 **  Train=>
Epoch: 11	Batch_num: 780	lr: 0.000	loss: 0.018	acc: 99.200%	train_loss: 0.0168	train_acc: 99.19%
INFO: 2017-09-16 12:30:05: main.py:139 **  Train=>
Epoch: 11	Batch_num: 840	lr: 0.000	loss: 0.017	acc: 99.210%	train_loss: 0.0194	train_acc: 99.14%
INFO: 2017-09-16 12:30:57: main.py:139 **  Train=>
Epoch: 11	Batch_num: 900	lr: 0.000	loss: 0.017	acc: 99.220%	train_loss: 0.0176	train_acc: 99.22%
INFO: 2017-09-16 12:31:50: main.py:139 **  Train=>
Epoch: 11	Batch_num: 960	lr: 0.000	loss: 0.018	acc: 99.200%	train_loss: 0.0189	train_acc: 99.14%
INFO: 2017-09-16 12:32:38: main.py:139 **  Train=>
Epoch: 11	Batch_num: 1016	lr: 0.000	loss: 0.018	acc: 99.200%	train_loss: 0.0186	train_acc: 99.15%
INFO: 2017-09-16 12:33:43: main.py:78 **  Validate=>
Epoch: 11	Valid_loss: 0.022	Valid_acc: 99.050%
INFO: 2017-09-16 12:33:45: main.py:139 **  Train=>
Epoch: 12	Batch_num: 0	lr: 0.000	loss: 0.018	acc: 99.240%	train_loss: 0.0177	train_acc: 99.24%
INFO: 2017-09-16 12:34:37: main.py:139 **  Train=>
Epoch: 12	Batch_num: 60	lr: 0.000	loss: 0.019	acc: 99.140%	train_loss: 0.0172	train_acc: 99.24%
INFO: 2017-09-16 12:35:30: main.py:139 **  Train=>
Epoch: 12	Batch_num: 120	lr: 0.000	loss: 0.018	acc: 99.190%	train_loss: 0.017	train_acc: 99.26%
INFO: 2017-09-16 12:36:22: main.py:139 **  Train=>
Epoch: 12	Batch_num: 180	lr: 0.000	loss: 0.018	acc: 99.190%	train_loss: 0.0167	train_acc: 99.22%
INFO: 2017-09-16 12:37:14: main.py:139 **  Train=>
Epoch: 12	Batch_num: 240	lr: 0.000	loss: 0.019	acc: 99.160%	train_loss: 0.0186	train_acc: 99.22%
INFO: 2017-09-16 12:38:06: main.py:139 **  Train=>
Epoch: 12	Batch_num: 300	lr: 0.000	loss: 0.018	acc: 99.190%	train_loss: 0.0165	train_acc: 99.21%
INFO: 2017-09-16 12:38:59: main.py:139 **  Train=>
Epoch: 12	Batch_num: 360	lr: 0.000	loss: 0.017	acc: 99.210%	train_loss: 0.0179	train_acc: 99.21%
INFO: 2017-09-16 12:39:51: main.py:139 **  Train=>
Epoch: 12	Batch_num: 420	lr: 0.000	loss: 0.018	acc: 99.190%	train_loss: 0.0198	train_acc: 99.04%
INFO: 2017-09-16 12:40:43: main.py:139 **  Train=>
Epoch: 12	Batch_num: 480	lr: 0.000	loss: 0.017	acc: 99.210%	train_loss: 0.017	train_acc: 99.14%
INFO: 2017-09-16 12:41:35: main.py:139 **  Train=>
Epoch: 12	Batch_num: 540	lr: 0.000	loss: 0.018	acc: 99.200%	train_loss: 0.0189	train_acc: 99.25%
INFO: 2017-09-16 12:42:28: main.py:139 **  Train=>
Epoch: 12	Batch_num: 600	lr: 0.000	loss: 0.017	acc: 99.220%	train_loss: 0.0193	train_acc: 99.02%
INFO: 2017-09-16 12:43:20: main.py:139 **  Train=>
Epoch: 12	Batch_num: 660	lr: 0.000	loss: 0.017	acc: 99.240%	train_loss: 0.0169	train_acc: 99.31%
INFO: 2017-09-16 12:44:12: main.py:139 **  Train=>
Epoch: 12	Batch_num: 720	lr: 0.000	loss: 0.018	acc: 99.190%	train_loss: 0.0204	train_acc: 99.08%
INFO: 2017-09-16 12:45:04: main.py:139 **  Train=>
Epoch: 12	Batch_num: 780	lr: 0.000	loss: 0.017	acc: 99.230%	train_loss: 0.0163	train_acc: 99.23%
INFO: 2017-09-16 12:45:57: main.py:139 **  Train=>
Epoch: 12	Batch_num: 840	lr: 0.000	loss: 0.017	acc: 99.220%	train_loss: 0.0186	train_acc: 99.17%
INFO: 2017-09-16 12:46:49: main.py:139 **  Train=>
Epoch: 12	Batch_num: 900	lr: 0.000	loss: 0.017	acc: 99.230%	train_loss: 0.017	train_acc: 99.24%
INFO: 2017-09-16 12:47:41: main.py:139 **  Train=>
Epoch: 12	Batch_num: 960	lr: 0.000	loss: 0.018	acc: 99.220%	train_loss: 0.0186	train_acc: 99.16%
INFO: 2017-09-16 12:48:30: main.py:139 **  Train=>
Epoch: 12	Batch_num: 1016	lr: 0.000	loss: 0.018	acc: 99.220%	train_loss: 0.016	train_acc: 99.28%
INFO: 2017-09-16 12:49:34: main.py:78 **  Validate=>
Epoch: 12	Valid_loss: 0.019	Valid_acc: 99.140%
INFO: 2017-09-16 12:49:37: main.py:139 **  Train=>
Epoch: 13	Batch_num: 0	lr: 0.000	loss: 0.016	acc: 99.330%	train_loss: 0.0158	train_acc: 99.33%
INFO: 2017-09-16 12:50:29: main.py:139 **  Train=>
Epoch: 13	Batch_num: 60	lr: 0.000	loss: 0.017	acc: 99.230%	train_loss: 0.0157	train_acc: 99.29%
INFO: 2017-09-16 12:51:21: main.py:139 **  Train=>
Epoch: 13	Batch_num: 120	lr: 0.000	loss: 0.017	acc: 99.230%	train_loss: 0.0158	train_acc: 99.3%
INFO: 2017-09-16 12:52:13: main.py:139 **  Train=>
Epoch: 13	Batch_num: 180	lr: 0.000	loss: 0.017	acc: 99.230%	train_loss: 0.0162	train_acc: 99.25%
INFO: 2017-09-16 12:53:05: main.py:139 **  Train=>
Epoch: 13	Batch_num: 240	lr: 0.000	loss: 0.018	acc: 99.210%	train_loss: 0.0177	train_acc: 99.24%
INFO: 2017-09-16 12:53:58: main.py:139 **  Train=>
Epoch: 13	Batch_num: 300	lr: 0.000	loss: 0.018	acc: 99.200%	train_loss: 0.0168	train_acc: 99.17%
INFO: 2017-09-16 12:54:50: main.py:139 **  Train=>
Epoch: 13	Batch_num: 360	lr: 0.000	loss: 0.017	acc: 99.230%	train_loss: 0.0172	train_acc: 99.25%
INFO: 2017-09-16 12:55:42: main.py:139 **  Train=>
Epoch: 13	Batch_num: 420	lr: 0.000	loss: 0.017	acc: 99.210%	train_loss: 0.0185	train_acc: 99.12%
INFO: 2017-09-16 12:56:34: main.py:139 **  Train=>
Epoch: 13	Batch_num: 480	lr: 0.000	loss: 0.017	acc: 99.240%	train_loss: 0.0161	train_acc: 99.18%
INFO: 2017-09-16 12:57:26: main.py:139 **  Train=>
Epoch: 13	Batch_num: 540	lr: 0.000	loss: 0.017	acc: 99.220%	train_loss: 0.0183	train_acc: 99.26%
INFO: 2017-09-16 12:58:19: main.py:139 **  Train=>
Epoch: 13	Batch_num: 600	lr: 0.000	loss: 0.017	acc: 99.240%	train_loss: 0.0176	train_acc: 99.11%
INFO: 2017-09-16 12:59:11: main.py:139 **  Train=>
Epoch: 13	Batch_num: 660	lr: 0.000	loss: 0.017	acc: 99.260%	train_loss: 0.0166	train_acc: 99.32%
INFO: 2017-09-16 13:00:03: main.py:139 **  Train=>
Epoch: 13	Batch_num: 720	lr: 0.000	loss: 0.018	acc: 99.200%	train_loss: 0.0207	train_acc: 99.06%
INFO: 2017-09-16 13:00:55: main.py:139 **  Train=>
Epoch: 13	Batch_num: 780	lr: 0.000	loss: 0.017	acc: 99.230%	train_loss: 0.017	train_acc: 99.2%
INFO: 2017-09-16 13:01:48: main.py:139 **  Train=>
Epoch: 13	Batch_num: 840	lr: 0.000	loss: 0.017	acc: 99.230%	train_loss: 0.0181	train_acc: 99.18%
INFO: 2017-09-16 13:02:40: main.py:139 **  Train=>
Epoch: 13	Batch_num: 900	lr: 0.000	loss: 0.017	acc: 99.240%	train_loss: 0.0182	train_acc: 99.19%
INFO: 2017-09-16 13:03:32: main.py:139 **  Train=>
Epoch: 13	Batch_num: 960	lr: 0.000	loss: 0.017	acc: 99.240%	train_loss: 0.0177	train_acc: 99.21%
INFO: 2017-09-16 13:04:20: main.py:139 **  Train=>
Epoch: 13	Batch_num: 1016	lr: 0.000	loss: 0.017	acc: 99.240%	train_loss: 0.0155	train_acc: 99.31%
INFO: 2017-09-16 13:05:24: main.py:78 **  Validate=>
Epoch: 13	Valid_loss: 0.019	Valid_acc: 99.160%
INFO: 2017-09-16 13:05:28: main.py:139 **  Train=>
Epoch: 14	Batch_num: 0	lr: 0.000	loss: 0.016	acc: 99.320%	train_loss: 0.0156	train_acc: 99.32%
INFO: 2017-09-16 13:06:20: main.py:139 **  Train=>
Epoch: 14	Batch_num: 60	lr: 0.000	loss: 0.017	acc: 99.230%	train_loss: 0.0157	train_acc: 99.29%
INFO: 2017-09-16 13:07:12: main.py:139 **  Train=>
Epoch: 14	Batch_num: 120	lr: 0.000	loss: 0.017	acc: 99.250%	train_loss: 0.0155	train_acc: 99.31%
INFO: 2017-09-16 13:08:05: main.py:139 **  Train=>
Epoch: 14	Batch_num: 180	lr: 0.000	loss: 0.017	acc: 99.250%	train_loss: 0.016	train_acc: 99.26%
INFO: 2017-09-16 13:08:57: main.py:139 **  Train=>
Epoch: 14	Batch_num: 240	lr: 0.000	loss: 0.017	acc: 99.230%	train_loss: 0.0175	train_acc: 99.24%
INFO: 2017-09-16 13:09:49: main.py:139 **  Train=>
Epoch: 14	Batch_num: 300	lr: 0.000	loss: 0.017	acc: 99.230%	train_loss: 0.0163	train_acc: 99.22%
INFO: 2017-09-16 13:10:41: main.py:139 **  Train=>
Epoch: 14	Batch_num: 360	lr: 0.000	loss: 0.017	acc: 99.240%	train_loss: 0.0165	train_acc: 99.29%
INFO: 2017-09-16 13:11:34: main.py:139 **  Train=>
Epoch: 14	Batch_num: 420	lr: 0.000	loss: 0.017	acc: 99.230%	train_loss: 0.0178	train_acc: 99.13%
INFO: 2017-09-16 13:12:26: main.py:139 **  Train=>
Epoch: 14	Batch_num: 480	lr: 0.000	loss: 0.017	acc: 99.250%	train_loss: 0.0158	train_acc: 99.21%
INFO: 2017-09-16 13:13:18: main.py:139 **  Train=>
Epoch: 14	Batch_num: 540	lr: 0.000	loss: 0.017	acc: 99.230%	train_loss: 0.0181	train_acc: 99.27%
INFO: 2017-09-16 13:14:10: main.py:139 **  Train=>
Epoch: 14	Batch_num: 600	lr: 0.000	loss: 0.016	acc: 99.260%	train_loss: 0.0175	train_acc: 99.11%
INFO: 2017-09-16 13:15:03: main.py:139 **  Train=>
Epoch: 14	Batch_num: 660	lr: 0.000	loss: 0.017	acc: 99.270%	train_loss: 0.0168	train_acc: 99.31%
INFO: 2017-09-16 13:15:55: main.py:139 **  Train=>
Epoch: 14	Batch_num: 720	lr: 0.000	loss: 0.017	acc: 99.210%	train_loss: 0.0206	train_acc: 99.06%
INFO: 2017-09-16 13:16:47: main.py:139 **  Train=>
Epoch: 14	Batch_num: 780	lr: 0.000	loss: 0.017	acc: 99.250%	train_loss: 0.016	train_acc: 99.23%
INFO: 2017-09-16 13:17:39: main.py:139 **  Train=>
Epoch: 14	Batch_num: 840	lr: 0.000	loss: 0.016	acc: 99.250%	train_loss: 0.0173	train_acc: 99.22%
INFO: 2017-09-16 13:18:32: main.py:139 **  Train=>
Epoch: 14	Batch_num: 900	lr: 0.000	loss: 0.017	acc: 99.240%	train_loss: 0.0175	train_acc: 99.21%
INFO: 2017-09-16 13:19:24: main.py:139 **  Train=>
Epoch: 14	Batch_num: 960	lr: 0.000	loss: 0.017	acc: 99.240%	train_loss: 0.0175	train_acc: 99.22%
INFO: 2017-09-16 13:20:13: main.py:139 **  Train=>
Epoch: 14	Batch_num: 1016	lr: 0.000	loss: 0.017	acc: 99.240%	train_loss: 0.0153	train_acc: 99.32%
INFO: 2017-09-16 13:21:17: main.py:78 **  Validate=>
Epoch: 14	Valid_loss: 0.018	Valid_acc: 99.180%
INFO: 2017-09-16 13:21:21: main.py:139 **  Train=>
Epoch: 15	Batch_num: 0	lr: 0.000	loss: 0.015	acc: 99.340%	train_loss: 0.0153	train_acc: 99.34%
INFO: 2017-09-16 13:22:13: main.py:139 **  Train=>
Epoch: 15	Batch_num: 60	lr: 0.000	loss: 0.017	acc: 99.240%	train_loss: 0.0153	train_acc: 99.31%
INFO: 2017-09-16 13:23:05: main.py:139 **  Train=>
Epoch: 15	Batch_num: 120	lr: 0.000	loss: 0.016	acc: 99.270%	train_loss: 0.0152	train_acc: 99.33%
INFO: 2017-09-16 13:23:57: main.py:139 **  Train=>
Epoch: 15	Batch_num: 180	lr: 0.000	loss: 0.017	acc: 99.260%	train_loss: 0.0156	train_acc: 99.27%
INFO: 2017-09-16 13:24:50: main.py:139 **  Train=>
Epoch: 15	Batch_num: 240	lr: 0.000	loss: 0.017	acc: 99.240%	train_loss: 0.0176	train_acc: 99.25%
INFO: 2017-09-16 13:25:42: main.py:139 **  Train=>
Epoch: 15	Batch_num: 300	lr: 0.000	loss: 0.017	acc: 99.240%	train_loss: 0.0161	train_acc: 99.22%
INFO: 2017-09-16 13:26:34: main.py:139 **  Train=>
Epoch: 15	Batch_num: 360	lr: 0.000	loss: 0.016	acc: 99.260%	train_loss: 0.0171	train_acc: 99.24%
INFO: 2017-09-16 13:27:27: main.py:139 **  Train=>
Epoch: 15	Batch_num: 420	lr: 0.000	loss: 0.017	acc: 99.230%	train_loss: 0.019	train_acc: 99.08%
INFO: 2017-09-16 13:28:19: main.py:139 **  Train=>
Epoch: 15	Batch_num: 480	lr: 0.000	loss: 0.018	acc: 99.220%	train_loss: 0.0166	train_acc: 99.16%
INFO: 2017-09-16 13:29:11: main.py:139 **  Train=>
Epoch: 15	Batch_num: 540	lr: 0.000	loss: 0.018	acc: 99.200%	train_loss: 0.0184	train_acc: 99.24%
INFO: 2017-09-16 13:30:03: main.py:139 **  Train=>
Epoch: 15	Batch_num: 600	lr: 0.000	loss: 0.016	acc: 99.250%	train_loss: 0.0174	train_acc: 99.12%
INFO: 2017-09-16 13:30:56: main.py:139 **  Train=>
Epoch: 15	Batch_num: 660	lr: 0.000	loss: 0.017	acc: 99.270%	train_loss: 0.016	train_acc: 99.34%
INFO: 2017-09-16 13:31:48: main.py:139 **  Train=>
Epoch: 15	Batch_num: 720	lr: 0.000	loss: 0.017	acc: 99.230%	train_loss: 0.0197	train_acc: 99.1%
INFO: 2017-09-16 13:32:40: main.py:139 **  Train=>
Epoch: 15	Batch_num: 780	lr: 0.000	loss: 0.017	acc: 99.260%	train_loss: 0.0159	train_acc: 99.24%
INFO: 2017-09-16 13:33:32: main.py:139 **  Train=>
Epoch: 15	Batch_num: 840	lr: 0.000	loss: 0.016	acc: 99.260%	train_loss: 0.0176	train_acc: 99.21%
INFO: 2017-09-16 13:34:25: main.py:139 **  Train=>
Epoch: 15	Batch_num: 900	lr: 0.000	loss: 0.016	acc: 99.260%	train_loss: 0.0166	train_acc: 99.24%
INFO: 2017-09-16 13:35:17: main.py:139 **  Train=>
Epoch: 15	Batch_num: 960	lr: 0.000	loss: 0.016	acc: 99.260%	train_loss: 0.0159	train_acc: 99.29%
INFO: 2017-09-16 13:36:06: main.py:139 **  Train=>
Epoch: 15	Batch_num: 1016	lr: 0.000	loss: 0.016	acc: 99.260%	train_loss: 0.0151	train_acc: 99.32%
INFO: 2017-09-16 13:37:10: main.py:78 **  Validate=>
Epoch: 15	Valid_loss: 0.018	Valid_acc: 99.180%
INFO: 2017-09-16 13:37:13: main.py:139 **  Train=>
Epoch: 16	Batch_num: 0	lr: 0.000	loss: 0.015	acc: 99.340%	train_loss: 0.0153	train_acc: 99.34%
INFO: 2017-09-16 13:38:05: main.py:139 **  Train=>
Epoch: 16	Batch_num: 60	lr: 0.000	loss: 0.016	acc: 99.260%	train_loss: 0.0149	train_acc: 99.33%
INFO: 2017-09-16 13:38:57: main.py:139 **  Train=>
Epoch: 16	Batch_num: 120	lr: 0.000	loss: 0.016	acc: 99.280%	train_loss: 0.0152	train_acc: 99.33%
INFO: 2017-09-16 13:39:49: main.py:139 **  Train=>
Epoch: 16	Batch_num: 180	lr: 0.000	loss: 0.016	acc: 99.270%	train_loss: 0.0155	train_acc: 99.27%
INFO: 2017-09-16 13:40:42: main.py:139 **  Train=>
Epoch: 16	Batch_num: 240	lr: 0.000	loss: 0.017	acc: 99.240%	train_loss: 0.0182	train_acc: 99.22%
INFO: 2017-09-16 13:41:34: main.py:139 **  Train=>
Epoch: 16	Batch_num: 300	lr: 0.000	loss: 0.016	acc: 99.260%	train_loss: 0.0158	train_acc: 99.23%
INFO: 2017-09-16 13:42:26: main.py:139 **  Train=>
Epoch: 16	Batch_num: 360	lr: 0.000	loss: 0.016	acc: 99.270%	train_loss: 0.0165	train_acc: 99.27%
INFO: 2017-09-16 13:43:19: main.py:139 **  Train=>
Epoch: 16	Batch_num: 420	lr: 0.000	loss: 0.016	acc: 99.260%	train_loss: 0.0169	train_acc: 99.19%
INFO: 2017-09-16 13:44:11: main.py:139 **  Train=>
Epoch: 16	Batch_num: 480	lr: 0.000	loss: 0.016	acc: 99.260%	train_loss: 0.0152	train_acc: 99.23%
INFO: 2017-09-16 13:45:03: main.py:139 **  Train=>
Epoch: 16	Batch_num: 540	lr: 0.000	loss: 0.017	acc: 99.250%	train_loss: 0.0179	train_acc: 99.28%
INFO: 2017-09-16 13:45:55: main.py:139 **  Train=>
Epoch: 16	Batch_num: 600	lr: 0.000	loss: 0.016	acc: 99.270%	train_loss: 0.017	train_acc: 99.15%
INFO: 2017-09-16 13:46:48: main.py:139 **  Train=>
Epoch: 16	Batch_num: 660	lr: 0.000	loss: 0.017	acc: 99.250%	train_loss: 0.017	train_acc: 99.3%
INFO: 2017-09-16 13:47:40: main.py:139 **  Train=>
Epoch: 16	Batch_num: 720	lr: 0.000	loss: 0.017	acc: 99.230%	train_loss: 0.0188	train_acc: 99.14%
INFO: 2017-09-16 13:48:32: main.py:139 **  Train=>
Epoch: 16	Batch_num: 780	lr: 0.000	loss: 0.017	acc: 99.270%	train_loss: 0.0158	train_acc: 99.25%
INFO: 2017-09-16 13:49:25: main.py:139 **  Train=>
Epoch: 16	Batch_num: 840	lr: 0.000	loss: 0.016	acc: 99.260%	train_loss: 0.0166	train_acc: 99.24%
INFO: 2017-09-16 13:50:17: main.py:139 **  Train=>
Epoch: 16	Batch_num: 900	lr: 0.000	loss: 0.016	acc: 99.270%	train_loss: 0.0166	train_acc: 99.25%
INFO: 2017-09-16 13:51:09: main.py:139 **  Train=>
Epoch: 16	Batch_num: 960	lr: 0.000	loss: 0.016	acc: 99.270%	train_loss: 0.0158	train_acc: 99.28%
INFO: 2017-09-16 13:51:58: main.py:139 **  Train=>
Epoch: 16	Batch_num: 1016	lr: 0.000	loss: 0.016	acc: 99.270%	train_loss: 0.0152	train_acc: 99.31%
INFO: 2017-09-16 13:53:02: main.py:78 **  Validate=>
Epoch: 16	Valid_loss: 0.019	Valid_acc: 99.160%
INFO: 2017-09-16 13:53:05: main.py:139 **  Train=>
Epoch: 17	Batch_num: 0	lr: 0.000	loss: 0.015	acc: 99.340%	train_loss: 0.0152	train_acc: 99.34%
INFO: 2017-09-16 13:53:57: main.py:139 **  Train=>
Epoch: 17	Batch_num: 60	lr: 0.000	loss: 0.016	acc: 99.270%	train_loss: 0.0147	train_acc: 99.33%
INFO: 2017-09-16 13:54:49: main.py:139 **  Train=>
Epoch: 17	Batch_num: 120	lr: 0.000	loss: 0.016	acc: 99.290%	train_loss: 0.0146	train_acc: 99.35%
INFO: 2017-09-16 13:55:42: main.py:139 **  Train=>
Epoch: 17	Batch_num: 180	lr: 0.000	loss: 0.016	acc: 99.280%	train_loss: 0.0154	train_acc: 99.28%
INFO: 2017-09-16 13:56:34: main.py:139 **  Train=>
Epoch: 17	Batch_num: 240	lr: 0.000	loss: 0.017	acc: 99.250%	train_loss: 0.0176	train_acc: 99.24%
INFO: 2017-09-16 13:57:26: main.py:139 **  Train=>
Epoch: 17	Batch_num: 300	lr: 0.000	loss: 0.016	acc: 99.250%	train_loss: 0.0163	train_acc: 99.2%
INFO: 2017-09-16 13:58:18: main.py:139 **  Train=>
Epoch: 17	Batch_num: 360	lr: 0.000	loss: 0.016	acc: 99.270%	train_loss: 0.0162	train_acc: 99.29%
INFO: 2017-09-16 13:59:10: main.py:139 **  Train=>
Epoch: 17	Batch_num: 420	lr: 0.000	loss: 0.016	acc: 99.270%	train_loss: 0.0171	train_acc: 99.17%
INFO: 2017-09-16 14:00:02: main.py:139 **  Train=>
Epoch: 17	Batch_num: 480	lr: 0.000	loss: 0.016	acc: 99.280%	train_loss: 0.0152	train_acc: 99.25%
INFO: 2017-09-16 14:00:54: main.py:139 **  Train=>
Epoch: 17	Batch_num: 540	lr: 0.000	loss: 0.016	acc: 99.260%	train_loss: 0.017	train_acc: 99.31%
INFO: 2017-09-16 14:01:46: main.py:139 **  Train=>
Epoch: 17	Batch_num: 600	lr: 0.000	loss: 0.016	acc: 99.270%	train_loss: 0.0164	train_acc: 99.18%
INFO: 2017-09-16 14:02:38: main.py:139 **  Train=>
Epoch: 17	Batch_num: 660	lr: 0.000	loss: 0.016	acc: 99.280%	train_loss: 0.016	train_acc: 99.34%
INFO: 2017-09-16 14:03:30: main.py:139 **  Train=>
Epoch: 17	Batch_num: 720	lr: 0.000	loss: 0.016	acc: 99.250%	train_loss: 0.0182	train_acc: 99.16%
INFO: 2017-09-16 14:04:22: main.py:139 **  Train=>
Epoch: 17	Batch_num: 780	lr: 0.000	loss: 0.016	acc: 99.280%	train_loss: 0.0157	train_acc: 99.26%
INFO: 2017-09-16 14:05:14: main.py:139 **  Train=>
Epoch: 17	Batch_num: 840	lr: 0.000	loss: 0.016	acc: 99.270%	train_loss: 0.0167	train_acc: 99.23%
INFO: 2017-09-16 14:06:06: main.py:139 **  Train=>
Epoch: 17	Batch_num: 900	lr: 0.000	loss: 0.016	acc: 99.280%	train_loss: 0.0168	train_acc: 99.24%
INFO: 2017-09-16 14:06:58: main.py:139 **  Train=>
Epoch: 17	Batch_num: 960	lr: 0.000	loss: 0.016	acc: 99.290%	train_loss: 0.0153	train_acc: 99.31%
INFO: 2017-09-16 14:07:46: main.py:139 **  Train=>
Epoch: 17	Batch_num: 1016	lr: 0.000	loss: 0.016	acc: 99.290%	train_loss: 0.0153	train_acc: 99.32%
INFO: 2017-09-16 14:08:50: main.py:78 **  Validate=>
Epoch: 17	Valid_loss: 0.019	Valid_acc: 99.140%
INFO: 2017-09-16 14:08:53: main.py:139 **  Train=>
Epoch: 18	Batch_num: 0	lr: 0.000	loss: 0.015	acc: 99.330%	train_loss: 0.0155	train_acc: 99.33%
INFO: 2017-09-16 14:09:45: main.py:139 **  Train=>
Epoch: 18	Batch_num: 60	lr: 0.000	loss: 0.016	acc: 99.260%	train_loss: 0.0156	train_acc: 99.28%
INFO: 2017-09-16 14:10:37: main.py:139 **  Train=>
Epoch: 18	Batch_num: 120	lr: 0.000	loss: 0.016	acc: 99.300%	train_loss: 0.0147	train_acc: 99.34%
INFO: 2017-09-16 14:11:30: main.py:139 **  Train=>
Epoch: 18	Batch_num: 180	lr: 0.000	loss: 0.016	acc: 99.290%	train_loss: 0.0153	train_acc: 99.3%
INFO: 2017-09-16 14:12:22: main.py:139 **  Train=>
Epoch: 18	Batch_num: 240	lr: 0.000	loss: 0.017	acc: 99.240%	train_loss: 0.0181	train_acc: 99.24%
INFO: 2017-09-16 14:13:14: main.py:139 **  Train=>
Epoch: 18	Batch_num: 300	lr: 0.000	loss: 0.038	acc: 98.480%	train_loss: 0.0556	train_acc: 97.65%
INFO: 2017-09-16 14:14:06: main.py:139 **  Train=>
Epoch: 18	Batch_num: 360	lr: 0.000	loss: 0.076	acc: 97.270%	train_loss: 0.0371	train_acc: 98.52%
INFO: 2017-09-16 14:14:59: main.py:139 **  Train=>
Epoch: 18	Batch_num: 420	lr: 0.000	loss: 0.039	acc: 98.380%	train_loss: 0.0575	train_acc: 97.57%
INFO: 2017-09-16 14:15:51: main.py:139 **  Train=>
Epoch: 18	Batch_num: 480	lr: 0.000	loss: 0.032	acc: 98.630%	train_loss: 0.0272	train_acc: 98.7%
INFO: 2017-09-16 14:16:43: main.py:139 **  Train=>
Epoch: 18	Batch_num: 540	lr: 0.000	loss: 0.028	acc: 98.790%	train_loss: 0.0261	train_acc: 98.99%
INFO: 2017-09-16 14:17:36: main.py:139 **  Train=>
Epoch: 18	Batch_num: 600	lr: 0.000	loss: 0.024	acc: 98.930%	train_loss: 0.0293	train_acc: 98.67%
INFO: 2017-09-16 14:18:28: main.py:139 **  Train=>
Epoch: 18	Batch_num: 660	lr: 0.000	loss: 0.026	acc: 98.920%	train_loss: 0.0276	train_acc: 98.99%
INFO: 2017-09-16 14:19:20: main.py:139 **  Train=>
Epoch: 18	Batch_num: 720	lr: 0.000	loss: 0.024	acc: 98.970%	train_loss: 0.0276	train_acc: 98.79%
INFO: 2017-09-16 14:20:12: main.py:139 **  Train=>
Epoch: 18	Batch_num: 780	lr: 0.000	loss: 0.023	acc: 99.000%	train_loss: 0.0232	train_acc: 98.91%
INFO: 2017-09-16 14:21:05: main.py:139 **  Train=>
Epoch: 18	Batch_num: 840	lr: 0.000	loss: 0.022	acc: 99.020%	train_loss: 0.0238	train_acc: 98.97%
INFO: 2017-09-16 14:21:57: main.py:139 **  Train=>
Epoch: 18	Batch_num: 900	lr: 0.000	loss: 0.021	acc: 99.070%	train_loss: 0.0186	train_acc: 99.18%
INFO: 2017-09-16 14:22:49: main.py:139 **  Train=>
Epoch: 18	Batch_num: 960	lr: 0.000	loss: 0.021	acc: 99.050%	train_loss: 0.0235	train_acc: 98.97%
INFO: 2017-09-16 14:23:38: main.py:139 **  Train=>
Epoch: 18	Batch_num: 1016	lr: 0.000	loss: 0.021	acc: 99.050%	train_loss: 0.0264	train_acc: 98.88%
INFO: 2017-09-16 14:24:42: main.py:78 **  Validate=>
Epoch: 18	Valid_loss: 0.056	Valid_acc: 97.890%
INFO: 2017-09-16 14:24:45: main.py:139 **  Train=>
Epoch: 19	Batch_num: 0	lr: 0.000	loss: 0.019	acc: 99.210%	train_loss: 0.0186	train_acc: 99.21%
INFO: 2017-09-16 14:25:37: main.py:139 **  Train=>
Epoch: 19	Batch_num: 60	lr: 0.000	loss: 0.020	acc: 99.110%	train_loss: 0.0182	train_acc: 99.18%
INFO: 2017-09-16 14:26:29: main.py:139 **  Train=>
Epoch: 19	Batch_num: 120	lr: 0.000	loss: 0.021	acc: 99.110%	train_loss: 0.0198	train_acc: 99.16%
INFO: 2017-09-16 14:27:22: main.py:139 **  Train=>
Epoch: 19	Batch_num: 180	lr: 0.000	loss: 0.021	acc: 99.070%	train_loss: 0.0179	train_acc: 99.16%
INFO: 2017-09-16 14:28:14: main.py:139 **  Train=>
Epoch: 19	Batch_num: 240	lr: 0.000	loss: 0.021	acc: 99.100%	train_loss: 0.0203	train_acc: 99.13%
INFO: 2017-09-16 14:29:06: main.py:139 **  Train=>
Epoch: 19	Batch_num: 300	lr: 0.000	loss: 0.020	acc: 99.100%	train_loss: 0.0186	train_acc: 99.12%
INFO: 2017-09-16 14:29:58: main.py:139 **  Train=>
Epoch: 19	Batch_num: 360	lr: 0.000	loss: 0.020	acc: 99.110%	train_loss: 0.0179	train_acc: 99.24%
INFO: 2017-09-16 14:30:51: main.py:139 **  Train=>
Epoch: 19	Batch_num: 420	lr: 0.000	loss: 0.020	acc: 99.100%	train_loss: 0.0234	train_acc: 98.89%
INFO: 2017-09-16 14:31:43: main.py:139 **  Train=>
Epoch: 19	Batch_num: 480	lr: 0.000	loss: 0.020	acc: 99.100%	train_loss: 0.0185	train_acc: 99.09%
INFO: 2017-09-16 14:32:35: main.py:139 **  Train=>
Epoch: 19	Batch_num: 540	lr: 0.000	loss: 0.019	acc: 99.130%	train_loss: 0.0205	train_acc: 99.17%
INFO: 2017-09-16 14:33:27: main.py:139 **  Train=>
Epoch: 19	Batch_num: 600	lr: 0.000	loss: 0.019	acc: 99.150%	train_loss: 0.0226	train_acc: 98.91%
INFO: 2017-09-16 14:34:20: main.py:139 **  Train=>
Epoch: 19	Batch_num: 660	lr: 0.000	loss: 0.019	acc: 99.180%	train_loss: 0.018	train_acc: 99.28%
INFO: 2017-09-16 14:35:12: main.py:139 **  Train=>
Epoch: 19	Batch_num: 720	lr: 0.000	loss: 0.019	acc: 99.150%	train_loss: 0.0211	train_acc: 99.04%
INFO: 2017-09-16 14:36:04: main.py:139 **  Train=>
Epoch: 19	Batch_num: 780	lr: 0.000	loss: 0.019	acc: 99.170%	train_loss: 0.0183	train_acc: 99.14%
INFO: 2017-09-16 14:36:56: main.py:139 **  Train=>
Epoch: 19	Batch_num: 840	lr: 0.000	loss: 0.018	acc: 99.170%	train_loss: 0.0201	train_acc: 99.12%
INFO: 2017-09-16 14:37:48: main.py:139 **  Train=>
Epoch: 19	Batch_num: 900	lr: 0.000	loss: 0.018	acc: 99.190%	train_loss: 0.0178	train_acc: 99.2%
INFO: 2017-09-16 14:38:40: main.py:139 **  Train=>
Epoch: 19	Batch_num: 960	lr: 0.000	loss: 0.018	acc: 99.180%	train_loss: 0.019	train_acc: 99.14%
INFO: 2017-09-16 14:39:28: main.py:139 **  Train=>
Epoch: 19	Batch_num: 1016	lr: 0.000	loss: 0.018	acc: 99.180%	train_loss: 0.0207	train_acc: 99.09%
INFO: 2017-09-16 14:40:32: main.py:78 **  Validate=>
Epoch: 19	Valid_loss: 0.019	Valid_acc: 99.160%
INFO: 2017-09-16 14:40:35: main.py:139 **  Train=>
Epoch: 20	Batch_num: 0	lr: 0.000	loss: 0.016	acc: 99.330%	train_loss: 0.0156	train_acc: 99.33%
INFO: 2017-09-16 14:41:27: main.py:139 **  Train=>
Epoch: 20	Batch_num: 60	lr: 0.000	loss: 0.018	acc: 99.180%	train_loss: 0.0165	train_acc: 99.25%
INFO: 2017-09-16 14:42:19: main.py:139 **  Train=>
Epoch: 20	Batch_num: 120	lr: 0.000	loss: 0.018	acc: 99.190%	train_loss: 0.0172	train_acc: 99.26%
INFO: 2017-09-16 14:43:11: main.py:139 **  Train=>
Epoch: 20	Batch_num: 180	lr: 0.000	loss: 0.018	acc: 99.190%	train_loss: 0.017	train_acc: 99.21%
INFO: 2017-09-16 14:44:04: main.py:139 **  Train=>
Epoch: 20	Batch_num: 240	lr: 0.000	loss: 0.019	acc: 99.170%	train_loss: 0.0192	train_acc: 99.18%
INFO: 2017-09-16 14:44:56: main.py:139 **  Train=>
Epoch: 20	Batch_num: 300	lr: 0.000	loss: 0.018	acc: 99.190%	train_loss: 0.0163	train_acc: 99.22%
INFO: 2017-09-16 14:45:48: main.py:139 **  Train=>
Epoch: 20	Batch_num: 360	lr: 0.000	loss: 0.018	acc: 99.200%	train_loss: 0.0172	train_acc: 99.26%
INFO: 2017-09-16 14:46:40: main.py:139 **  Train=>
Epoch: 20	Batch_num: 420	lr: 0.000	loss: 0.018	acc: 99.170%	train_loss: 0.0212	train_acc: 98.99%
INFO: 2017-09-16 14:47:32: main.py:139 **  Train=>
Epoch: 20	Batch_num: 480	lr: 0.000	loss: 0.018	acc: 99.180%	train_loss: 0.017	train_acc: 99.14%
INFO: 2017-09-16 14:48:24: main.py:139 **  Train=>
Epoch: 20	Batch_num: 540	lr: 0.000	loss: 0.018	acc: 99.190%	train_loss: 0.019	train_acc: 99.23%
INFO: 2017-09-16 14:49:16: main.py:139 **  Train=>
Epoch: 20	Batch_num: 600	lr: 0.000	loss: 0.017	acc: 99.210%	train_loss: 0.0204	train_acc: 98.99%
INFO: 2017-09-16 14:50:08: main.py:139 **  Train=>
Epoch: 20	Batch_num: 660	lr: 0.000	loss: 0.018	acc: 99.220%	train_loss: 0.0171	train_acc: 99.3%
INFO: 2017-09-16 14:51:00: main.py:139 **  Train=>
Epoch: 20	Batch_num: 720	lr: 0.000	loss: 0.018	acc: 99.200%	train_loss: 0.0197	train_acc: 99.11%
INFO: 2017-09-16 14:51:52: main.py:139 **  Train=>
Epoch: 20	Batch_num: 780	lr: 0.000	loss: 0.018	acc: 99.220%	train_loss: 0.0168	train_acc: 99.2%
INFO: 2017-09-16 14:52:44: main.py:139 **  Train=>
Epoch: 20	Batch_num: 840	lr: 0.000	loss: 0.017	acc: 99.210%	train_loss: 0.0194	train_acc: 99.14%
INFO: 2017-09-16 14:53:38: main.py:139 **  Train=>
Epoch: 20	Batch_num: 900	lr: 0.000	loss: 0.017	acc: 99.220%	train_loss: 0.0169	train_acc: 99.23%
INFO: 2017-09-16 14:54:30: main.py:139 **  Train=>
Epoch: 20	Batch_num: 960	lr: 0.000	loss: 0.017	acc: 99.220%	train_loss: 0.0175	train_acc: 99.21%
INFO: 2017-09-16 14:55:19: main.py:139 **  Train=>
Epoch: 20	Batch_num: 1016	lr: 0.000	loss: 0.017	acc: 99.220%	train_loss: 0.0194	train_acc: 99.15%
INFO: 2017-09-16 14:56:23: main.py:78 **  Validate=>
Epoch: 20	Valid_loss: 0.019	Valid_acc: 99.150%
INFO: 2017-09-16 14:56:26: main.py:139 **  Train=>
Epoch: 21	Batch_num: 0	lr: 0.000	loss: 0.015	acc: 99.340%	train_loss: 0.0152	train_acc: 99.34%
INFO: 2017-09-17 00:37:06: utils.py:92 **  All dataset size is 5088
INFO: 2017-09-17 00:37:06: utils.py:93 **  Train dataset size is 4071
INFO: 2017-09-17 00:37:06: utils.py:95 **  Valid dataset size is 1017
INFO: 2017-09-17 00:37:06: main.py:214 **  Loading dataset...
INFO: 2017-09-17 00:37:06: main.py:228 **  All data sample counts 5088
INFO: 2017-09-17 00:37:06: main.py:229 **  Train data batch size 4
INFO: 2017-09-17 00:37:06: main.py:230 **  Train data sample counts 0
INFO: 2017-09-17 00:37:06: main.py:231 **  Valid data sample counts 1018
INFO: 2017-09-17 00:45:58: utils.py:92 **  All dataset size is 5088
INFO: 2017-09-17 00:45:58: utils.py:93 **  Train dataset size is 4071
INFO: 2017-09-17 00:45:58: utils.py:95 **  Valid dataset size is 1017
INFO: 2017-09-17 00:45:58: main.py:214 **  Loading dataset...
INFO: 2017-09-17 00:45:58: main.py:228 **  All data sample counts 5088
INFO: 2017-09-17 00:45:58: main.py:229 **  Train data batch size 2
INFO: 2017-09-17 00:45:58: main.py:230 **  Train data sample counts 4070
INFO: 2017-09-17 00:45:58: main.py:231 **  Valid data sample counts 1018
INFO: 2017-09-17 00:46:01: main.py:140 **  Train=>
Epoch: 0	Batch_num: 0	lr: 0.000	loss: 1.354	acc: 17.760%	train_loss: 1.3544	train_acc: 17.76%
INFO: 2017-09-17 00:46:50: main.py:140 **  Train=>
Epoch: 0	Batch_num: 60	lr: 0.000	loss: 0.814	acc: 80.060%	train_loss: 0.3848	train_acc: 87.54%
INFO: 2017-09-17 00:47:41: main.py:140 **  Train=>
Epoch: 0	Batch_num: 120	lr: 0.000	loss: 0.203	acc: 93.490%	train_loss: 0.0839	train_acc: 97.63%
INFO: 2017-09-17 00:48:35: main.py:140 **  Train=>
Epoch: 0	Batch_num: 180	lr: 0.000	loss: 0.159	acc: 94.450%	train_loss: 0.1177	train_acc: 96.11%
INFO: 2017-09-17 00:49:31: main.py:140 **  Train=>
Epoch: 0	Batch_num: 240	lr: 0.000	loss: 0.151	acc: 94.600%	train_loss: 0.3286	train_acc: 88.99%
INFO: 2017-09-17 00:50:28: main.py:140 **  Train=>
Epoch: 0	Batch_num: 300	lr: 0.000	loss: 0.176	acc: 93.260%	train_loss: 0.0797	train_acc: 97.04%
INFO: 2017-09-17 00:51:27: main.py:140 **  Train=>
Epoch: 0	Batch_num: 360	lr: 0.000	loss: 0.113	acc: 95.800%	train_loss: 0.0853	train_acc: 96.62%
INFO: 2017-09-17 00:52:25: main.py:140 **  Train=>
Epoch: 0	Batch_num: 420	lr: 0.000	loss: 0.086	acc: 96.780%	train_loss: 0.076	train_acc: 97.33%
INFO: 2017-09-17 00:53:24: main.py:140 **  Train=>
Epoch: 0	Batch_num: 480	lr: 0.000	loss: 0.059	acc: 97.780%	train_loss: 0.0679	train_acc: 97.79%
INFO: 2017-09-17 00:54:20: main.py:140 **  Train=>
Epoch: 0	Batch_num: 540	lr: 0.000	loss: 0.060	acc: 97.710%	train_loss: 0.0564	train_acc: 97.94%
INFO: 2017-09-17 00:55:18: main.py:140 **  Train=>
Epoch: 0	Batch_num: 600	lr: 0.000	loss: 0.054	acc: 97.890%	train_loss: 0.0621	train_acc: 97.18%
INFO: 2017-09-17 00:56:14: main.py:140 **  Train=>
Epoch: 0	Batch_num: 660	lr: 0.000	loss: 0.047	acc: 98.140%	train_loss: 0.0501	train_acc: 98.1%
INFO: 2017-09-17 00:57:11: main.py:140 **  Train=>
Epoch: 0	Batch_num: 720	lr: 0.000	loss: 0.044	acc: 98.280%	train_loss: 0.0663	train_acc: 97.33%
INFO: 2017-09-17 00:58:09: main.py:140 **  Train=>
Epoch: 0	Batch_num: 780	lr: 0.000	loss: 0.040	acc: 98.430%	train_loss: 0.0336	train_acc: 98.78%
INFO: 2017-09-17 00:59:07: main.py:140 **  Train=>
Epoch: 0	Batch_num: 840	lr: 0.000	loss: 0.040	acc: 98.410%	train_loss: 0.0558	train_acc: 97.58%
INFO: 2017-09-17 01:00:04: main.py:140 **  Train=>
Epoch: 0	Batch_num: 900	lr: 0.000	loss: 0.049	acc: 98.090%	train_loss: 0.0547	train_acc: 97.71%
INFO: 2017-09-17 01:01:01: main.py:140 **  Train=>
Epoch: 0	Batch_num: 960	lr: 0.000	loss: 0.039	acc: 98.460%	train_loss: 0.0356	train_acc: 98.36%
INFO: 2017-09-17 01:01:57: main.py:140 **  Train=>
Epoch: 0	Batch_num: 1020	lr: 0.000	loss: 0.044	acc: 98.270%	train_loss: 0.0517	train_acc: 98.02%
INFO: 2017-09-17 01:02:54: main.py:140 **  Train=>
Epoch: 0	Batch_num: 1080	lr: 0.000	loss: 0.046	acc: 98.180%	train_loss: 0.0329	train_acc: 98.73%
INFO: 2017-09-17 01:03:51: main.py:140 **  Train=>
Epoch: 0	Batch_num: 1140	lr: 0.000	loss: 0.042	acc: 98.330%	train_loss: 0.0336	train_acc: 98.46%
INFO: 2017-09-17 01:04:48: main.py:140 **  Train=>
Epoch: 0	Batch_num: 1200	lr: 0.000	loss: 0.035	acc: 98.590%	train_loss: 0.0394	train_acc: 98.29%
INFO: 2017-09-17 01:05:45: main.py:140 **  Train=>
Epoch: 0	Batch_num: 1260	lr: 0.000	loss: 0.034	acc: 98.690%	train_loss: 0.0304	train_acc: 98.75%
INFO: 2017-09-17 01:06:42: main.py:140 **  Train=>
Epoch: 0	Batch_num: 1320	lr: 0.000	loss: 0.038	acc: 98.520%	train_loss: 0.0501	train_acc: 98.29%
INFO: 2017-09-17 01:07:38: main.py:140 **  Train=>
Epoch: 0	Batch_num: 1380	lr: 0.000	loss: 0.033	acc: 98.700%	train_loss: 0.0366	train_acc: 98.54%
INFO: 2017-09-17 01:08:34: main.py:140 **  Train=>
Epoch: 0	Batch_num: 1440	lr: 0.000	loss: 0.031	acc: 98.740%	train_loss: 0.0338	train_acc: 98.49%
INFO: 2017-09-17 01:09:31: main.py:140 **  Train=>
Epoch: 0	Batch_num: 1500	lr: 0.000	loss: 0.031	acc: 98.810%	train_loss: 0.0268	train_acc: 98.91%
INFO: 2017-09-17 01:10:27: main.py:140 **  Train=>
Epoch: 0	Batch_num: 1560	lr: 0.000	loss: 0.031	acc: 98.750%	train_loss: 0.0359	train_acc: 98.26%
INFO: 2017-09-17 01:11:24: main.py:140 **  Train=>
Epoch: 0	Batch_num: 1620	lr: 0.000	loss: 0.031	acc: 98.750%	train_loss: 0.0596	train_acc: 97.28%
INFO: 2017-09-17 01:12:20: main.py:140 **  Train=>
Epoch: 0	Batch_num: 1680	lr: 0.000	loss: 0.044	acc: 98.300%	train_loss: 0.0663	train_acc: 96.97%
INFO: 2017-09-17 01:13:16: main.py:140 **  Train=>
Epoch: 0	Batch_num: 1740	lr: 0.000	loss: 0.045	acc: 98.220%	train_loss: 0.034	train_acc: 98.59%
INFO: 2017-09-17 01:14:11: main.py:140 **  Train=>
Epoch: 0	Batch_num: 1800	lr: 0.000	loss: 0.037	acc: 98.490%	train_loss: 0.0369	train_acc: 98.64%
INFO: 2017-09-17 01:15:07: main.py:140 **  Train=>
Epoch: 0	Batch_num: 1860	lr: 0.000	loss: 0.032	acc: 98.740%	train_loss: 0.0345	train_acc: 98.67%
INFO: 2017-09-17 01:18:50: utils.py:92 **  All dataset size is 5088
INFO: 2017-09-17 01:18:50: utils.py:93 **  Train dataset size is 4071
INFO: 2017-09-17 01:18:50: utils.py:95 **  Valid dataset size is 1017
INFO: 2017-09-17 01:18:50: main.py:214 **  Loading dataset...
INFO: 2017-09-17 01:18:50: main.py:228 **  All data sample counts 5088
INFO: 2017-09-17 01:18:50: main.py:229 **  Train data batch size 2
INFO: 2017-09-17 01:18:50: main.py:230 **  Train data sample counts 4070
INFO: 2017-09-17 01:18:50: main.py:231 **  Valid data sample counts 1018
INFO: 2017-09-17 01:18:53: main.py:140 **  Train=>
Epoch: 0	Batch_num: 0	lr: 0.000	loss: 1.323	acc: 10.020%	train_loss: 1.3235	train_acc: 10.02%
INFO: 2017-09-17 01:19:37: main.py:140 **  Train=>
Epoch: 0	Batch_num: 100	lr: 0.000	loss: 0.550	acc: 81.590%	train_loss: 0.1817	train_acc: 93.35%
INFO: 2017-09-17 01:20:24: main.py:140 **  Train=>
Epoch: 0	Batch_num: 200	lr: 0.000	loss: 0.148	acc: 94.510%	train_loss: 0.1132	train_acc: 95.51%
INFO: 2017-09-17 01:22:35: utils.py:92 **  All dataset size is 5088
INFO: 2017-09-17 01:22:35: utils.py:93 **  Train dataset size is 4071
INFO: 2017-09-17 01:22:35: utils.py:95 **  Valid dataset size is 1017
INFO: 2017-09-17 01:22:35: main.py:214 **  Loading dataset...
INFO: 2017-09-17 01:22:35: main.py:228 **  All data sample counts 5088
INFO: 2017-09-17 01:22:35: main.py:229 **  Train data batch size 2
INFO: 2017-09-17 01:22:35: main.py:230 **  Train data sample counts 4070
INFO: 2017-09-17 01:22:35: main.py:231 **  Valid data sample counts 1018
INFO: 2017-09-17 01:22:38: main.py:140 **  Train=>
Epoch: 15	Batch_num: 0	lr: 0.000	loss: 0.020	acc: 99.080%	train_loss: 0.0197	train_acc: 99.08%
INFO: 2017-09-17 01:23:23: main.py:140 **  Train=>
Epoch: 15	Batch_num: 100	lr: 0.000	loss: 0.022	acc: 99.130%	train_loss: 0.0584	train_acc: 97.5%
INFO: 2017-09-17 01:24:11: main.py:140 **  Train=>
Epoch: 15	Batch_num: 200	lr: 0.000	loss: 0.064	acc: 97.560%	train_loss: 0.0462	train_acc: 98.18%
INFO: 2017-09-17 01:25:01: main.py:140 **  Train=>
Epoch: 15	Batch_num: 300	lr: 0.000	loss: 0.062	acc: 97.770%	train_loss: 0.0275	train_acc: 98.83%
INFO: 2017-09-17 01:25:52: main.py:140 **  Train=>
Epoch: 15	Batch_num: 400	lr: 0.000	loss: 0.029	acc: 98.850%	train_loss: 0.0374	train_acc: 98.79%
INFO: 2017-09-17 01:26:23: utils.py:92 **  All dataset size is 5088
INFO: 2017-09-17 01:26:23: utils.py:93 **  Train dataset size is 4071
INFO: 2017-09-17 01:26:23: utils.py:95 **  Valid dataset size is 1017
INFO: 2017-09-17 01:26:23: main.py:214 **  Loading dataset...
INFO: 2017-09-17 01:26:23: main.py:228 **  All data sample counts 5088
INFO: 2017-09-17 01:26:23: main.py:229 **  Train data batch size 2
INFO: 2017-09-17 01:26:23: main.py:230 **  Train data sample counts 4070
INFO: 2017-09-17 01:26:23: main.py:231 **  Valid data sample counts 1018
INFO: 2017-09-17 01:26:26: main.py:140 **  Train=>
Epoch: 15	Batch_num: 0	lr: 0.000	loss: 0.016	acc: 99.320%	train_loss: 0.0161	train_acc: 99.32%
INFO: 2017-09-17 01:27:15: main.py:140 **  Train=>
Epoch: 15	Batch_num: 100	lr: 0.000	loss: 0.024	acc: 98.950%	train_loss: 0.035	train_acc: 98.26%
INFO: 2017-09-17 01:28:05: utils.py:92 **  All dataset size is 5088
INFO: 2017-09-17 01:28:05: utils.py:93 **  Train dataset size is 4071
INFO: 2017-09-17 01:28:05: utils.py:95 **  Valid dataset size is 1017
INFO: 2017-09-17 01:28:05: main.py:214 **  Loading dataset...
INFO: 2017-09-17 01:28:05: main.py:228 **  All data sample counts 5088
INFO: 2017-09-17 01:28:05: main.py:229 **  Train data batch size 2
INFO: 2017-09-17 01:28:05: main.py:230 **  Train data sample counts 4070
INFO: 2017-09-17 01:28:05: main.py:231 **  Valid data sample counts 1018
INFO: 2017-09-17 01:28:08: main.py:140 **  Train=>
Epoch: 15	Batch_num: 0	lr: 0.000	loss: 0.020	acc: 99.080%	train_loss: 0.0197	train_acc: 99.08%
INFO: 2017-09-17 01:28:55: main.py:140 **  Train=>
Epoch: 15	Batch_num: 100	lr: 0.000	loss: 0.016	acc: 99.340%	train_loss: 0.0206	train_acc: 99.1%
INFO: 2017-09-17 01:29:45: main.py:140 **  Train=>
Epoch: 15	Batch_num: 200	lr: 0.000	loss: 0.117	acc: 95.830%	train_loss: 0.1079	train_acc: 95.81%
INFO: 2017-09-17 01:30:36: main.py:140 **  Train=>
Epoch: 15	Batch_num: 300	lr: 0.000	loss: 0.121	acc: 95.460%	train_loss: 0.0623	train_acc: 97.51%
INFO: 2017-09-17 01:31:26: main.py:140 **  Train=>
Epoch: 15	Batch_num: 400	lr: 0.000	loss: 0.091	acc: 96.600%	train_loss: 0.0589	train_acc: 97.97%
INFO: 2017-09-17 01:32:16: main.py:140 **  Train=>
Epoch: 15	Batch_num: 500	lr: 0.000	loss: 0.054	acc: 97.820%	train_loss: 0.0284	train_acc: 98.81%
INFO: 2017-09-17 01:33:06: main.py:140 **  Train=>
Epoch: 15	Batch_num: 600	lr: 0.000	loss: 0.049	acc: 98.070%	train_loss: 0.0969	train_acc: 95.64%
INFO: 2017-09-17 01:33:57: main.py:140 **  Train=>
Epoch: 15	Batch_num: 700	lr: 0.000	loss: 0.048	acc: 98.060%	train_loss: 0.0322	train_acc: 98.64%
INFO: 2017-09-17 01:34:47: main.py:140 **  Train=>
Epoch: 15	Batch_num: 800	lr: 0.000	loss: 0.039	acc: 98.420%	train_loss: 0.0351	train_acc: 98.46%
INFO: 2017-09-17 01:35:37: main.py:140 **  Train=>
Epoch: 15	Batch_num: 900	lr: 0.000	loss: 0.066	acc: 97.300%	train_loss: 0.1031	train_acc: 95.48%
INFO: 2017-09-17 01:36:27: main.py:140 **  Train=>
Epoch: 15	Batch_num: 1000	lr: 0.000	loss: 0.080	acc: 96.700%	train_loss: 0.0452	train_acc: 98.29%
INFO: 2017-09-17 01:37:18: main.py:140 **  Train=>
Epoch: 15	Batch_num: 1100	lr: 0.000	loss: 0.048	acc: 98.030%	train_loss: 0.0311	train_acc: 98.78%
INFO: 2017-09-17 01:38:07: main.py:140 **  Train=>
Epoch: 15	Batch_num: 1200	lr: 0.000	loss: 0.042	acc: 98.280%	train_loss: 0.048	train_acc: 97.86%
INFO: 2017-09-17 01:38:58: main.py:140 **  Train=>
Epoch: 15	Batch_num: 1300	lr: 0.000	loss: 0.040	acc: 98.430%	train_loss: 0.0332	train_acc: 98.63%
INFO: 2017-09-17 01:39:48: main.py:140 **  Train=>
Epoch: 15	Batch_num: 1400	lr: 0.000	loss: 0.037	acc: 98.500%	train_loss: 0.0419	train_acc: 98.45%
INFO: 2017-09-17 01:40:38: main.py:140 **  Train=>
Epoch: 15	Batch_num: 1500	lr: 0.000	loss: 0.036	acc: 98.550%	train_loss: 0.0301	train_acc: 98.75%
INFO: 2017-09-17 01:41:28: main.py:140 **  Train=>
Epoch: 15	Batch_num: 1600	lr: 0.000	loss: 0.032	acc: 98.730%	train_loss: 0.0229	train_acc: 99.02%
INFO: 2017-09-17 01:42:18: main.py:140 **  Train=>
Epoch: 15	Batch_num: 1700	lr: 0.000	loss: 0.030	acc: 98.760%	train_loss: 0.0256	train_acc: 99.04%
INFO: 2017-09-17 01:43:08: main.py:140 **  Train=>
Epoch: 15	Batch_num: 1800	lr: 0.000	loss: 0.027	acc: 98.910%	train_loss: 0.0186	train_acc: 99.29%
INFO: 2017-09-17 01:43:58: main.py:140 **  Train=>
Epoch: 15	Batch_num: 1900	lr: 0.000	loss: 0.027	acc: 98.910%	train_loss: 0.0209	train_acc: 99.17%
INFO: 2017-09-17 01:44:49: main.py:140 **  Train=>
Epoch: 15	Batch_num: 2000	lr: 0.000	loss: 0.028	acc: 98.920%	train_loss: 0.0294	train_acc: 98.78%
INFO: 2017-09-17 01:45:05: main.py:140 **  Train=>
Epoch: 15	Batch_num: 2034	lr: 0.000	loss: 0.028	acc: 98.920%	train_loss: 0.0215	train_acc: 99.1%
INFO: 2017-09-17 01:46:09: utils.py:92 **  All dataset size is 5088
INFO: 2017-09-17 01:46:09: utils.py:93 **  Train dataset size is 4071
INFO: 2017-09-17 01:46:09: utils.py:95 **  Valid dataset size is 1017
INFO: 2017-09-17 01:46:09: main.py:214 **  Loading dataset...
INFO: 2017-09-17 01:46:09: main.py:228 **  All data sample counts 5088
INFO: 2017-09-17 01:46:09: main.py:229 **  Train data batch size 6
INFO: 2017-09-17 01:46:09: main.py:230 **  Train data sample counts 4068
INFO: 2017-09-17 01:46:09: main.py:231 **  Valid data sample counts 1018
INFO: 2017-09-17 01:50:57: utils.py:92 **  All dataset size is 5088
INFO: 2017-09-17 01:50:57: utils.py:93 **  Train dataset size is 4071
INFO: 2017-09-17 01:50:57: utils.py:95 **  Valid dataset size is 1017
INFO: 2017-09-17 01:50:57: main.py:214 **  Loading dataset...
INFO: 2017-09-17 01:50:57: main.py:228 **  All data sample counts 5088
INFO: 2017-09-17 01:50:57: main.py:229 **  Train data batch size 4
INFO: 2017-09-17 01:50:57: main.py:230 **  Train data sample counts 4068
INFO: 2017-09-17 01:50:57: main.py:231 **  Valid data sample counts 1018
INFO: 2017-09-17 01:51:00: main.py:140 **  Train=>
Epoch: 15	Batch_num: 0	lr: 0.000	loss: 0.019	acc: 99.100%	train_loss: 0.019	train_acc: 99.1%
INFO: 2017-09-17 01:52:17: main.py:140 **  Train=>
Epoch: 15	Batch_num: 100	lr: 0.000	loss: 0.010	acc: 99.580%	train_loss: 0.0089	train_acc: 99.61%
INFO: 2017-09-17 01:53:41: main.py:140 **  Train=>
Epoch: 15	Batch_num: 200	lr: 0.000	loss: 0.010	acc: 99.620%	train_loss: 0.0099	train_acc: 99.65%
INFO: 2017-09-17 01:55:07: main.py:140 **  Train=>
Epoch: 15	Batch_num: 300	lr: 0.000	loss: 0.009	acc: 99.620%	train_loss: 0.0083	train_acc: 99.63%
INFO: 2017-09-17 01:56:35: main.py:140 **  Train=>
Epoch: 15	Batch_num: 400	lr: 0.000	loss: 0.009	acc: 99.630%	train_loss: 0.0083	train_acc: 99.66%
INFO: 2017-09-17 01:58:02: main.py:140 **  Train=>
Epoch: 15	Batch_num: 500	lr: 0.000	loss: 0.009	acc: 99.630%	train_loss: 0.0088	train_acc: 99.64%
INFO: 2017-09-17 01:59:30: main.py:140 **  Train=>
Epoch: 15	Batch_num: 600	lr: 0.000	loss: 0.010	acc: 99.600%	train_loss: 0.0109	train_acc: 99.48%
INFO: 2017-09-17 02:00:58: main.py:140 **  Train=>
Epoch: 15	Batch_num: 700	lr: 0.000	loss: 0.009	acc: 99.620%	train_loss: 0.0108	train_acc: 99.57%
INFO: 2017-09-17 02:02:25: main.py:140 **  Train=>
Epoch: 15	Batch_num: 800	lr: 0.000	loss: 0.010	acc: 99.590%	train_loss: 0.0105	train_acc: 99.54%
INFO: 2017-09-17 02:03:53: main.py:140 **  Train=>
Epoch: 15	Batch_num: 900	lr: 0.000	loss: 0.009	acc: 99.620%	train_loss: 0.0086	train_acc: 99.64%
INFO: 2017-09-17 02:05:21: main.py:140 **  Train=>
Epoch: 15	Batch_num: 1000	lr: 0.000	loss: 0.009	acc: 99.620%	train_loss: 0.011	train_acc: 99.51%
INFO: 2017-09-17 02:05:35: main.py:140 **  Train=>
Epoch: 15	Batch_num: 1016	lr: 0.000	loss: 0.009	acc: 99.620%	train_loss: 0.0084	train_acc: 99.64%
INFO: 2017-09-17 02:06:39: main.py:79 **  Validate=>
Epoch: 15	Valid_loss: 0.011	Valid_acc: 99.570%
INFO: 2017-09-17 02:06:43: main.py:140 **  Train=>
Epoch: 16	Batch_num: 0	lr: 0.000	loss: 0.008	acc: 99.680%	train_loss: 0.0081	train_acc: 99.68%
INFO: 2017-09-17 02:08:10: main.py:140 **  Train=>
Epoch: 16	Batch_num: 100	lr: 0.000	loss: 0.009	acc: 99.640%	train_loss: 0.0092	train_acc: 99.6%
INFO: 2017-09-17 02:09:36: main.py:140 **  Train=>
Epoch: 16	Batch_num: 200	lr: 0.000	loss: 0.009	acc: 99.650%	train_loss: 0.0098	train_acc: 99.66%
INFO: 2017-09-17 02:11:02: main.py:140 **  Train=>
Epoch: 16	Batch_num: 300	lr: 0.000	loss: 0.009	acc: 99.630%	train_loss: 0.0085	train_acc: 99.63%
INFO: 2017-09-17 02:12:28: main.py:140 **  Train=>
Epoch: 16	Batch_num: 400	lr: 0.000	loss: 0.009	acc: 99.640%	train_loss: 0.0084	train_acc: 99.66%
INFO: 2017-09-17 02:13:54: main.py:140 **  Train=>
Epoch: 16	Batch_num: 500	lr: 0.000	loss: 0.009	acc: 99.610%	train_loss: 0.0101	train_acc: 99.59%
INFO: 2017-09-17 02:15:20: main.py:140 **  Train=>
Epoch: 16	Batch_num: 600	lr: 0.000	loss: 0.009	acc: 99.620%	train_loss: 0.0098	train_acc: 99.56%
INFO: 2017-09-17 02:16:46: main.py:140 **  Train=>
Epoch: 16	Batch_num: 700	lr: 0.000	loss: 0.009	acc: 99.640%	train_loss: 0.0092	train_acc: 99.63%
INFO: 2017-09-17 02:18:12: main.py:140 **  Train=>
Epoch: 16	Batch_num: 800	lr: 0.000	loss: 0.009	acc: 99.650%	train_loss: 0.0093	train_acc: 99.58%
INFO: 2017-09-17 02:19:38: main.py:140 **  Train=>
Epoch: 16	Batch_num: 900	lr: 0.000	loss: 0.009	acc: 99.630%	train_loss: 0.0084	train_acc: 99.65%
INFO: 2017-09-17 02:21:05: main.py:140 **  Train=>
Epoch: 16	Batch_num: 1000	lr: 0.000	loss: 0.009	acc: 99.640%	train_loss: 0.0084	train_acc: 99.63%
INFO: 2017-09-17 02:21:18: main.py:140 **  Train=>
Epoch: 16	Batch_num: 1016	lr: 0.000	loss: 0.009	acc: 99.640%	train_loss: 0.0075	train_acc: 99.68%
INFO: 2017-09-17 02:22:22: main.py:79 **  Validate=>
Epoch: 16	Valid_loss: 0.011	Valid_acc: 99.580%
INFO: 2017-09-17 02:22:26: main.py:140 **  Train=>
Epoch: 17	Batch_num: 0	lr: 0.000	loss: 0.008	acc: 99.700%	train_loss: 0.0076	train_acc: 99.7%
INFO: 2017-09-17 02:23:52: main.py:140 **  Train=>
Epoch: 17	Batch_num: 100	lr: 0.000	loss: 0.008	acc: 99.650%	train_loss: 0.0105	train_acc: 99.55%
INFO: 2017-09-17 02:25:18: main.py:140 **  Train=>
Epoch: 17	Batch_num: 200	lr: 0.000	loss: 0.009	acc: 99.640%	train_loss: 0.0121	train_acc: 99.58%
INFO: 2017-09-17 02:26:44: main.py:140 **  Train=>
Epoch: 17	Batch_num: 300	lr: 0.000	loss: 0.009	acc: 99.630%	train_loss: 0.0081	train_acc: 99.64%
INFO: 2017-09-17 02:28:10: main.py:140 **  Train=>
Epoch: 17	Batch_num: 400	lr: 0.000	loss: 0.008	acc: 99.660%	train_loss: 0.008	train_acc: 99.67%
INFO: 2017-09-17 02:29:36: main.py:140 **  Train=>
Epoch: 17	Batch_num: 500	lr: 0.000	loss: 0.009	acc: 99.640%	train_loss: 0.0085	train_acc: 99.65%
INFO: 2017-09-17 02:31:02: main.py:140 **  Train=>
Epoch: 17	Batch_num: 600	lr: 0.000	loss: 0.009	acc: 99.640%	train_loss: 0.0095	train_acc: 99.57%
INFO: 2017-09-17 02:32:28: main.py:140 **  Train=>
Epoch: 17	Batch_num: 700	lr: 0.000	loss: 0.008	acc: 99.660%	train_loss: 0.0087	train_acc: 99.65%
INFO: 2017-09-17 02:33:54: main.py:140 **  Train=>
Epoch: 17	Batch_num: 800	lr: 0.000	loss: 0.008	acc: 99.660%	train_loss: 0.0088	train_acc: 99.61%
INFO: 2017-09-17 02:35:21: main.py:140 **  Train=>
Epoch: 17	Batch_num: 900	lr: 0.000	loss: 0.008	acc: 99.660%	train_loss: 0.0079	train_acc: 99.67%
INFO: 2017-09-17 02:36:47: main.py:140 **  Train=>
Epoch: 17	Batch_num: 1000	lr: 0.000	loss: 0.009	acc: 99.650%	train_loss: 0.009	train_acc: 99.6%
INFO: 2017-09-17 02:37:00: main.py:140 **  Train=>
Epoch: 17	Batch_num: 1016	lr: 0.000	loss: 0.009	acc: 99.650%	train_loss: 0.0072	train_acc: 99.7%
INFO: 2017-09-17 02:38:04: main.py:79 **  Validate=>
Epoch: 17	Valid_loss: 0.010	Valid_acc: 99.590%
INFO: 2017-09-17 02:38:08: main.py:140 **  Train=>
Epoch: 18	Batch_num: 0	lr: 0.000	loss: 0.007	acc: 99.710%	train_loss: 0.0073	train_acc: 99.71%
INFO: 2017-09-17 02:39:34: main.py:140 **  Train=>
Epoch: 18	Batch_num: 100	lr: 0.000	loss: 0.008	acc: 99.660%	train_loss: 0.0084	train_acc: 99.62%
INFO: 2017-09-17 02:41:00: main.py:140 **  Train=>
Epoch: 18	Batch_num: 200	lr: 0.000	loss: 0.008	acc: 99.660%	train_loss: 0.0098	train_acc: 99.64%
INFO: 2017-09-17 02:42:26: main.py:140 **  Train=>
Epoch: 18	Batch_num: 300	lr: 0.000	loss: 0.008	acc: 99.650%	train_loss: 0.008	train_acc: 99.64%
INFO: 2017-09-17 02:43:52: main.py:140 **  Train=>
Epoch: 18	Batch_num: 400	lr: 0.000	loss: 0.008	acc: 99.670%	train_loss: 0.0074	train_acc: 99.69%
INFO: 2017-09-17 02:45:19: main.py:140 **  Train=>
Epoch: 18	Batch_num: 500	lr: 0.000	loss: 0.008	acc: 99.650%	train_loss: 0.008	train_acc: 99.67%
INFO: 2017-09-17 02:46:45: main.py:140 **  Train=>
Epoch: 18	Batch_num: 600	lr: 0.000	loss: 0.008	acc: 99.660%	train_loss: 0.0082	train_acc: 99.62%
INFO: 2017-09-17 02:48:11: main.py:140 **  Train=>
Epoch: 18	Batch_num: 700	lr: 0.000	loss: 0.008	acc: 99.670%	train_loss: 0.0085	train_acc: 99.66%
INFO: 2017-09-17 02:49:37: main.py:140 **  Train=>
Epoch: 18	Batch_num: 800	lr: 0.000	loss: 0.008	acc: 99.670%	train_loss: 0.0089	train_acc: 99.6%
INFO: 2017-09-17 02:51:03: main.py:140 **  Train=>
Epoch: 18	Batch_num: 900	lr: 0.000	loss: 0.008	acc: 99.670%	train_loss: 0.008	train_acc: 99.66%
INFO: 2017-09-17 02:52:29: main.py:140 **  Train=>
Epoch: 18	Batch_num: 1000	lr: 0.000	loss: 0.008	acc: 99.660%	train_loss: 0.0083	train_acc: 99.63%
INFO: 2017-09-17 02:52:43: main.py:140 **  Train=>
Epoch: 18	Batch_num: 1016	lr: 0.000	loss: 0.008	acc: 99.660%	train_loss: 0.0073	train_acc: 99.69%
INFO: 2017-09-17 02:53:46: main.py:79 **  Validate=>
Epoch: 18	Valid_loss: 0.010	Valid_acc: 99.580%
INFO: 2017-09-17 02:53:49: main.py:140 **  Train=>
Epoch: 19	Batch_num: 0	lr: 0.000	loss: 0.007	acc: 99.700%	train_loss: 0.0075	train_acc: 99.7%
INFO: 2017-09-17 02:55:15: main.py:140 **  Train=>
Epoch: 19	Batch_num: 100	lr: 0.000	loss: 0.008	acc: 99.670%	train_loss: 0.0081	train_acc: 99.64%
INFO: 2017-09-17 02:56:41: main.py:140 **  Train=>
Epoch: 19	Batch_num: 200	lr: 0.000	loss: 0.008	acc: 99.670%	train_loss: 0.0099	train_acc: 99.68%
INFO: 2017-09-17 02:58:07: main.py:140 **  Train=>
Epoch: 19	Batch_num: 300	lr: 0.000	loss: 0.008	acc: 99.660%	train_loss: 0.0072	train_acc: 99.7%
INFO: 2017-09-17 02:59:34: main.py:140 **  Train=>
Epoch: 19	Batch_num: 400	lr: 0.000	loss: 0.008	acc: 99.680%	train_loss: 0.0077	train_acc: 99.67%
INFO: 2017-09-17 03:01:00: main.py:140 **  Train=>
Epoch: 19	Batch_num: 500	lr: 0.000	loss: 0.008	acc: 99.660%	train_loss: 0.0091	train_acc: 99.61%
INFO: 2017-09-17 03:02:26: main.py:140 **  Train=>
Epoch: 19	Batch_num: 600	lr: 0.000	loss: 0.008	acc: 99.670%	train_loss: 0.0085	train_acc: 99.61%
INFO: 2017-09-17 03:03:52: main.py:140 **  Train=>
Epoch: 19	Batch_num: 700	lr: 0.000	loss: 0.009	acc: 99.650%	train_loss: 0.0096	train_acc: 99.62%
INFO: 2017-09-17 03:05:18: main.py:140 **  Train=>
Epoch: 19	Batch_num: 800	lr: 0.000	loss: 0.008	acc: 99.660%	train_loss: 0.0084	train_acc: 99.62%
INFO: 2017-09-17 03:06:44: main.py:140 **  Train=>
Epoch: 19	Batch_num: 900	lr: 0.000	loss: 0.008	acc: 99.670%	train_loss: 0.0076	train_acc: 99.68%
INFO: 2017-09-17 03:08:10: main.py:140 **  Train=>
Epoch: 19	Batch_num: 1000	lr: 0.000	loss: 0.008	acc: 99.660%	train_loss: 0.008	train_acc: 99.64%
INFO: 2017-09-17 03:08:24: main.py:140 **  Train=>
Epoch: 19	Batch_num: 1016	lr: 0.000	loss: 0.008	acc: 99.660%	train_loss: 0.0071	train_acc: 99.7%
INFO: 2017-09-17 03:09:27: main.py:79 **  Validate=>
Epoch: 19	Valid_loss: 0.011	Valid_acc: 99.560%
INFO: 2017-09-17 03:09:30: main.py:140 **  Train=>
Epoch: 20	Batch_num: 0	lr: 0.000	loss: 0.007	acc: 99.720%	train_loss: 0.007	train_acc: 99.72%
INFO: 2017-09-17 03:10:56: main.py:140 **  Train=>
Epoch: 20	Batch_num: 100	lr: 0.000	loss: 0.008	acc: 99.680%	train_loss: 0.0088	train_acc: 99.61%
INFO: 2017-09-17 03:12:22: main.py:140 **  Train=>
Epoch: 20	Batch_num: 200	lr: 0.000	loss: 0.008	acc: 99.680%	train_loss: 0.0092	train_acc: 99.69%
INFO: 2017-09-17 03:13:48: main.py:140 **  Train=>
Epoch: 20	Batch_num: 300	lr: 0.000	loss: 0.008	acc: 99.670%	train_loss: 0.0077	train_acc: 99.65%
INFO: 2017-09-17 03:15:14: main.py:140 **  Train=>
Epoch: 20	Batch_num: 400	lr: 0.000	loss: 0.008	acc: 99.670%	train_loss: 0.0072	train_acc: 99.69%
INFO: 2017-09-17 03:16:40: main.py:140 **  Train=>
Epoch: 20	Batch_num: 500	lr: 0.000	loss: 0.008	acc: 99.670%	train_loss: 0.0083	train_acc: 99.65%
INFO: 2017-09-17 03:18:06: main.py:140 **  Train=>
Epoch: 20	Batch_num: 600	lr: 0.000	loss: 0.008	acc: 99.680%	train_loss: 0.0079	train_acc: 99.65%
INFO: 2017-09-17 03:19:32: main.py:140 **  Train=>
Epoch: 20	Batch_num: 700	lr: 0.000	loss: 0.008	acc: 99.680%	train_loss: 0.0088	train_acc: 99.64%
INFO: 2017-09-17 03:20:58: main.py:140 **  Train=>
Epoch: 20	Batch_num: 800	lr: 0.000	loss: 0.008	acc: 99.680%	train_loss: 0.0084	train_acc: 99.62%
INFO: 2017-09-17 03:22:24: main.py:140 **  Train=>
Epoch: 20	Batch_num: 900	lr: 0.000	loss: 0.007	acc: 99.680%	train_loss: 0.0074	train_acc: 99.69%
INFO: 2017-09-17 03:23:50: main.py:140 **  Train=>
Epoch: 20	Batch_num: 1000	lr: 0.000	loss: 0.008	acc: 99.680%	train_loss: 0.0098	train_acc: 99.56%
INFO: 2017-09-17 03:24:04: main.py:140 **  Train=>
Epoch: 20	Batch_num: 1016	lr: 0.000	loss: 0.008	acc: 99.680%	train_loss: 0.012	train_acc: 99.56%
INFO: 2017-09-17 03:25:07: main.py:79 **  Validate=>
Epoch: 20	Valid_loss: 0.011	Valid_acc: 99.560%
INFO: 2017-09-17 03:25:10: main.py:140 **  Train=>
Epoch: 21	Batch_num: 0	lr: 0.000	loss: 0.007	acc: 99.700%	train_loss: 0.0074	train_acc: 99.7%
INFO: 2017-09-17 03:26:36: main.py:140 **  Train=>
Epoch: 21	Batch_num: 100	lr: 0.000	loss: 0.008	acc: 99.680%	train_loss: 0.0087	train_acc: 99.62%
INFO: 2017-09-17 03:28:02: main.py:140 **  Train=>
Epoch: 21	Batch_num: 200	lr: 0.000	loss: 0.008	acc: 99.690%	train_loss: 0.0077	train_acc: 99.73%
INFO: 2017-09-17 03:29:28: main.py:140 **  Train=>
Epoch: 21	Batch_num: 300	lr: 0.000	loss: 0.007	acc: 99.690%	train_loss: 0.0076	train_acc: 99.67%
INFO: 2017-09-17 03:30:54: main.py:140 **  Train=>
Epoch: 21	Batch_num: 400	lr: 0.000	loss: 0.007	acc: 99.690%	train_loss: 0.007	train_acc: 99.71%
INFO: 2017-09-17 03:32:20: main.py:140 **  Train=>
Epoch: 21	Batch_num: 500	lr: 0.000	loss: 0.009	acc: 99.620%	train_loss: 0.0091	train_acc: 99.61%
INFO: 2017-09-17 03:33:46: main.py:140 **  Train=>
Epoch: 21	Batch_num: 600	lr: 0.000	loss: 0.008	acc: 99.670%	train_loss: 0.008	train_acc: 99.64%
INFO: 2017-09-17 03:35:12: main.py:140 **  Train=>
Epoch: 21	Batch_num: 700	lr: 0.000	loss: 0.008	acc: 99.690%	train_loss: 0.0082	train_acc: 99.67%
INFO: 2017-09-17 03:36:39: main.py:140 **  Train=>
Epoch: 21	Batch_num: 800	lr: 0.000	loss: 0.008	acc: 99.680%	train_loss: 0.0083	train_acc: 99.62%
INFO: 2017-09-17 03:38:05: main.py:140 **  Train=>
Epoch: 21	Batch_num: 900	lr: 0.000	loss: 0.007	acc: 99.690%	train_loss: 0.0075	train_acc: 99.69%
INFO: 2017-09-17 03:39:31: main.py:140 **  Train=>
Epoch: 21	Batch_num: 1000	lr: 0.000	loss: 0.008	acc: 99.680%	train_loss: 0.0086	train_acc: 99.62%
INFO: 2017-09-17 03:39:45: main.py:140 **  Train=>
Epoch: 21	Batch_num: 1016	lr: 0.000	loss: 0.008	acc: 99.680%	train_loss: 0.007	train_acc: 99.7%
INFO: 2017-09-17 03:40:48: main.py:79 **  Validate=>
Epoch: 21	Valid_loss: 0.013	Valid_acc: 99.510%
INFO: 2017-09-17 03:40:51: main.py:140 **  Train=>
Epoch: 22	Batch_num: 0	lr: 0.000	loss: 0.008	acc: 99.670%	train_loss: 0.0081	train_acc: 99.67%
INFO: 2017-09-17 03:42:17: main.py:140 **  Train=>
Epoch: 22	Batch_num: 100	lr: 0.000	loss: 0.007	acc: 99.690%	train_loss: 0.0076	train_acc: 99.66%
INFO: 2017-09-17 03:43:43: main.py:140 **  Train=>
Epoch: 22	Batch_num: 200	lr: 0.000	loss: 0.007	acc: 99.700%	train_loss: 0.0075	train_acc: 99.73%
INFO: 2017-09-17 03:45:09: main.py:140 **  Train=>
Epoch: 22	Batch_num: 300	lr: 0.000	loss: 0.007	acc: 99.690%	train_loss: 0.0069	train_acc: 99.7%
INFO: 2017-09-17 03:46:35: main.py:140 **  Train=>
Epoch: 22	Batch_num: 400	lr: 0.000	loss: 0.007	acc: 99.700%	train_loss: 0.0071	train_acc: 99.7%
INFO: 2017-09-17 03:48:01: main.py:140 **  Train=>
Epoch: 22	Batch_num: 500	lr: 0.000	loss: 0.008	acc: 99.680%	train_loss: 0.008	train_acc: 99.67%
INFO: 2017-09-17 03:49:27: main.py:140 **  Train=>
Epoch: 22	Batch_num: 600	lr: 0.000	loss: 0.007	acc: 99.690%	train_loss: 0.0079	train_acc: 99.63%
INFO: 2017-09-17 03:50:53: main.py:140 **  Train=>
Epoch: 22	Batch_num: 700	lr: 0.000	loss: 0.007	acc: 99.700%	train_loss: 0.0076	train_acc: 99.69%
INFO: 2017-09-17 03:52:19: main.py:140 **  Train=>
Epoch: 22	Batch_num: 800	lr: 0.000	loss: 0.007	acc: 99.690%	train_loss: 0.0079	train_acc: 99.65%
INFO: 2017-09-17 03:53:45: main.py:140 **  Train=>
Epoch: 22	Batch_num: 900	lr: 0.000	loss: 0.007	acc: 99.690%	train_loss: 0.007	train_acc: 99.71%
INFO: 2017-09-17 03:55:11: main.py:140 **  Train=>
Epoch: 22	Batch_num: 1000	lr: 0.000	loss: 0.007	acc: 99.700%	train_loss: 0.0077	train_acc: 99.66%
INFO: 2017-09-17 03:55:25: main.py:140 **  Train=>
Epoch: 22	Batch_num: 1016	lr: 0.000	loss: 0.007	acc: 99.700%	train_loss: 0.0062	train_acc: 99.74%
INFO: 2017-09-17 03:56:28: main.py:79 **  Validate=>
Epoch: 22	Valid_loss: 0.011	Valid_acc: 99.550%
INFO: 2017-09-17 03:56:31: main.py:140 **  Train=>
Epoch: 23	Batch_num: 0	lr: 0.000	loss: 0.007	acc: 99.700%	train_loss: 0.0074	train_acc: 99.7%
INFO: 2017-09-17 03:57:57: main.py:140 **  Train=>
Epoch: 23	Batch_num: 100	lr: 0.000	loss: 0.007	acc: 99.700%	train_loss: 0.0083	train_acc: 99.63%
INFO: 2017-09-17 03:59:23: main.py:140 **  Train=>
Epoch: 23	Batch_num: 200	lr: 0.000	loss: 0.007	acc: 99.700%	train_loss: 0.0071	train_acc: 99.75%
INFO: 2017-09-17 04:00:49: main.py:140 **  Train=>
Epoch: 23	Batch_num: 300	lr: 0.000	loss: 0.007	acc: 99.690%	train_loss: 0.0067	train_acc: 99.71%
INFO: 2017-09-17 04:02:15: main.py:140 **  Train=>
Epoch: 23	Batch_num: 400	lr: 0.000	loss: 0.007	acc: 99.700%	train_loss: 0.0068	train_acc: 99.71%
INFO: 2017-09-17 04:03:41: main.py:140 **  Train=>
Epoch: 23	Batch_num: 500	lr: 0.000	loss: 0.007	acc: 99.690%	train_loss: 0.0077	train_acc: 99.68%
INFO: 2017-09-17 04:05:07: main.py:140 **  Train=>
Epoch: 23	Batch_num: 600	lr: 0.000	loss: 0.007	acc: 99.700%	train_loss: 0.0074	train_acc: 99.67%
INFO: 2017-09-17 04:06:33: main.py:140 **  Train=>
Epoch: 23	Batch_num: 700	lr: 0.000	loss: 0.007	acc: 99.700%	train_loss: 0.0073	train_acc: 99.7%
INFO: 2017-09-17 04:07:59: main.py:140 **  Train=>
Epoch: 23	Batch_num: 800	lr: 0.000	loss: 0.007	acc: 99.690%	train_loss: 0.0076	train_acc: 99.66%
INFO: 2017-09-17 04:09:25: main.py:140 **  Train=>
Epoch: 23	Batch_num: 900	lr: 0.000	loss: 0.007	acc: 99.700%	train_loss: 0.0077	train_acc: 99.68%
INFO: 2017-09-17 04:10:52: main.py:140 **  Train=>
Epoch: 23	Batch_num: 1000	lr: 0.000	loss: 0.007	acc: 99.700%	train_loss: 0.0073	train_acc: 99.68%
INFO: 2017-09-17 04:11:05: main.py:140 **  Train=>
Epoch: 23	Batch_num: 1016	lr: 0.000	loss: 0.007	acc: 99.700%	train_loss: 0.0063	train_acc: 99.74%
INFO: 2017-09-17 04:12:08: main.py:79 **  Validate=>
Epoch: 23	Valid_loss: 0.011	Valid_acc: 99.550%
INFO: 2017-09-17 04:12:11: main.py:140 **  Train=>
Epoch: 24	Batch_num: 0	lr: 0.000	loss: 0.006	acc: 99.740%	train_loss: 0.0065	train_acc: 99.74%
INFO: 2017-09-17 04:13:37: main.py:140 **  Train=>
Epoch: 24	Batch_num: 100	lr: 0.000	loss: 0.007	acc: 99.710%	train_loss: 0.008	train_acc: 99.65%
INFO: 2017-09-17 04:15:03: main.py:140 **  Train=>
Epoch: 24	Batch_num: 200	lr: 0.000	loss: 0.007	acc: 99.710%	train_loss: 0.0072	train_acc: 99.75%
INFO: 2017-09-17 04:16:29: main.py:140 **  Train=>
Epoch: 24	Batch_num: 300	lr: 0.000	loss: 0.007	acc: 99.690%	train_loss: 0.0067	train_acc: 99.71%
INFO: 2017-09-17 04:17:55: main.py:140 **  Train=>
Epoch: 24	Batch_num: 400	lr: 0.000	loss: 0.007	acc: 99.710%	train_loss: 0.0068	train_acc: 99.71%
INFO: 2017-09-17 04:19:21: main.py:140 **  Train=>
Epoch: 24	Batch_num: 500	lr: 0.000	loss: 0.007	acc: 99.690%	train_loss: 0.0071	train_acc: 99.7%
INFO: 2017-09-17 04:20:48: main.py:140 **  Train=>
Epoch: 24	Batch_num: 600	lr: 0.000	loss: 0.007	acc: 99.700%	train_loss: 0.0071	train_acc: 99.68%
INFO: 2017-09-17 04:22:14: main.py:140 **  Train=>
Epoch: 24	Batch_num: 700	lr: 0.000	loss: 0.007	acc: 99.700%	train_loss: 0.0072	train_acc: 99.7%
INFO: 2017-09-17 04:23:40: main.py:140 **  Train=>
Epoch: 24	Batch_num: 800	lr: 0.000	loss: 0.007	acc: 99.690%	train_loss: 0.008	train_acc: 99.64%
INFO: 2017-09-17 04:25:07: main.py:140 **  Train=>
Epoch: 24	Batch_num: 900	lr: 0.000	loss: 0.008	acc: 99.680%	train_loss: 0.0079	train_acc: 99.68%
INFO: 2017-09-17 04:26:33: main.py:140 **  Train=>
Epoch: 24	Batch_num: 1000	lr: 0.000	loss: 0.009	acc: 99.640%	train_loss: 0.0075	train_acc: 99.66%
INFO: 2017-09-17 04:26:46: main.py:140 **  Train=>
Epoch: 24	Batch_num: 1016	lr: 0.000	loss: 0.009	acc: 99.640%	train_loss: 0.0066	train_acc: 99.72%
INFO: 2017-09-17 04:27:49: main.py:79 **  Validate=>
Epoch: 24	Valid_loss: 0.010	Valid_acc: 99.590%
INFO: 2017-09-17 04:27:51: main.py:140 **  Train=>
Epoch: 25	Batch_num: 0	lr: 0.000	loss: 0.007	acc: 99.720%	train_loss: 0.007	train_acc: 99.72%
INFO: 2017-09-17 04:29:17: main.py:140 **  Train=>
Epoch: 25	Batch_num: 100	lr: 0.000	loss: 0.007	acc: 99.690%	train_loss: 0.0075	train_acc: 99.67%
INFO: 2017-09-17 04:30:44: main.py:140 **  Train=>
Epoch: 25	Batch_num: 200	lr: 0.000	loss: 0.008	acc: 99.670%	train_loss: 0.0117	train_acc: 99.62%
INFO: 2017-09-17 04:32:10: main.py:140 **  Train=>
Epoch: 25	Batch_num: 300	lr: 0.000	loss: 0.009	acc: 99.640%	train_loss: 0.0075	train_acc: 99.67%
INFO: 2017-09-17 04:33:36: main.py:140 **  Train=>
Epoch: 25	Batch_num: 400	lr: 0.000	loss: 0.008	acc: 99.670%	train_loss: 0.0072	train_acc: 99.69%
INFO: 2017-09-17 04:35:02: main.py:140 **  Train=>
Epoch: 25	Batch_num: 500	lr: 0.000	loss: 0.009	acc: 99.610%	train_loss: 0.0112	train_acc: 99.53%
INFO: 2017-09-17 04:36:28: main.py:140 **  Train=>
Epoch: 25	Batch_num: 600	lr: 0.000	loss: 0.026	acc: 98.980%	train_loss: 0.0198	train_acc: 99.13%
INFO: 2017-09-17 04:37:54: main.py:140 **  Train=>
Epoch: 25	Batch_num: 700	lr: 0.000	loss: 0.014	acc: 99.450%	train_loss: 0.0102	train_acc: 99.61%
INFO: 2017-09-17 04:39:21: main.py:140 **  Train=>
Epoch: 25	Batch_num: 800	lr: 0.000	loss: 0.012	acc: 99.530%	train_loss: 0.0109	train_acc: 99.51%
INFO: 2017-09-17 04:40:47: main.py:140 **  Train=>
Epoch: 25	Batch_num: 900	lr: 0.000	loss: 0.011	acc: 99.530%	train_loss: 0.0091	train_acc: 99.63%
INFO: 2017-09-17 04:42:13: main.py:140 **  Train=>
Epoch: 25	Batch_num: 1000	lr: 0.000	loss: 0.011	acc: 99.560%	train_loss: 0.0127	train_acc: 99.43%
INFO: 2017-09-17 04:42:27: main.py:140 **  Train=>
Epoch: 25	Batch_num: 1016	lr: 0.000	loss: 0.011	acc: 99.560%	train_loss: 0.0137	train_acc: 99.43%
INFO: 2017-09-17 04:43:30: main.py:79 **  Validate=>
Epoch: 25	Valid_loss: 0.012	Valid_acc: 99.520%
INFO: 2017-09-17 04:43:32: main.py:140 **  Train=>
Epoch: 26	Batch_num: 0	lr: 0.000	loss: 0.009	acc: 99.650%	train_loss: 0.009	train_acc: 99.65%
INFO: 2017-09-17 04:44:58: main.py:140 **  Train=>
Epoch: 26	Batch_num: 100	lr: 0.000	loss: 0.010	acc: 99.590%	train_loss: 0.0119	train_acc: 99.48%
INFO: 2017-09-17 04:46:24: main.py:140 **  Train=>
Epoch: 26	Batch_num: 200	lr: 0.000	loss: 0.010	acc: 99.610%	train_loss: 0.0131	train_acc: 99.59%
INFO: 2017-09-17 04:47:51: main.py:140 **  Train=>
Epoch: 26	Batch_num: 300	lr: 0.000	loss: 0.009	acc: 99.610%	train_loss: 0.0079	train_acc: 99.65%
INFO: 2017-09-17 04:49:17: main.py:140 **  Train=>
Epoch: 26	Batch_num: 400	lr: 0.000	loss: 0.010	acc: 99.590%	train_loss: 0.0092	train_acc: 99.6%
INFO: 2017-09-17 04:50:43: main.py:140 **  Train=>
Epoch: 26	Batch_num: 500	lr: 0.000	loss: 0.009	acc: 99.600%	train_loss: 0.0106	train_acc: 99.57%
INFO: 2017-09-17 04:52:09: main.py:140 **  Train=>
Epoch: 26	Batch_num: 600	lr: 0.000	loss: 0.011	acc: 99.550%	train_loss: 0.0239	train_acc: 98.98%
INFO: 2017-09-17 04:53:35: main.py:140 **  Train=>
Epoch: 26	Batch_num: 700	lr: 0.000	loss: 0.044	acc: 98.380%	train_loss: 0.0271	train_acc: 98.91%
INFO: 2017-09-17 04:55:01: main.py:140 **  Train=>
Epoch: 26	Batch_num: 800	lr: 0.000	loss: 0.031	acc: 98.860%	train_loss: 0.0158	train_acc: 99.32%
INFO: 2017-09-17 04:56:27: main.py:140 **  Train=>
Epoch: 26	Batch_num: 900	lr: 0.000	loss: 0.016	acc: 99.350%	train_loss: 0.0118	train_acc: 99.52%
INFO: 2017-09-17 04:57:53: main.py:140 **  Train=>
Epoch: 26	Batch_num: 1000	lr: 0.000	loss: 0.015	acc: 99.420%	train_loss: 0.0132	train_acc: 99.4%
INFO: 2017-09-17 04:58:07: main.py:140 **  Train=>
Epoch: 26	Batch_num: 1016	lr: 0.000	loss: 0.015	acc: 99.420%	train_loss: 0.0201	train_acc: 99.2%
INFO: 2017-09-17 04:59:10: main.py:79 **  Validate=>
Epoch: 26	Valid_loss: 0.015	Valid_acc: 99.400%
INFO: 2017-09-17 04:59:12: main.py:140 **  Train=>
Epoch: 27	Batch_num: 0	lr: 0.000	loss: 0.010	acc: 99.630%	train_loss: 0.0098	train_acc: 99.63%
INFO: 2017-09-17 05:00:39: main.py:140 **  Train=>
Epoch: 27	Batch_num: 100	lr: 0.000	loss: 0.012	acc: 99.500%	train_loss: 0.0131	train_acc: 99.43%
INFO: 2017-09-17 05:02:05: main.py:140 **  Train=>
Epoch: 27	Batch_num: 200	lr: 0.000	loss: 0.013	acc: 99.480%	train_loss: 0.0332	train_acc: 99.13%
INFO: 2017-09-17 05:03:31: main.py:140 **  Train=>
Epoch: 27	Batch_num: 300	lr: 0.000	loss: 0.012	acc: 99.530%	train_loss: 0.0083	train_acc: 99.65%
INFO: 2017-09-17 05:04:57: main.py:140 **  Train=>
Epoch: 27	Batch_num: 400	lr: 0.000	loss: 0.011	acc: 99.540%	train_loss: 0.0108	train_acc: 99.54%
INFO: 2017-09-17 05:06:23: main.py:140 **  Train=>
Epoch: 27	Batch_num: 500	lr: 0.000	loss: 0.011	acc: 99.540%	train_loss: 0.0115	train_acc: 99.52%
INFO: 2017-09-17 05:07:49: main.py:140 **  Train=>
Epoch: 27	Batch_num: 600	lr: 0.000	loss: 0.011	acc: 99.550%	train_loss: 0.0156	train_acc: 99.3%
INFO: 2017-09-17 05:09:15: main.py:140 **  Train=>
Epoch: 27	Batch_num: 700	lr: 0.000	loss: 0.011	acc: 99.570%	train_loss: 0.0096	train_acc: 99.61%
INFO: 2017-09-17 05:10:41: main.py:140 **  Train=>
Epoch: 27	Batch_num: 800	lr: 0.000	loss: 0.010	acc: 99.600%	train_loss: 0.0102	train_acc: 99.54%
INFO: 2017-09-17 05:12:07: main.py:140 **  Train=>
Epoch: 27	Batch_num: 900	lr: 0.000	loss: 0.010	acc: 99.590%	train_loss: 0.0094	train_acc: 99.6%
INFO: 2017-09-17 05:13:33: main.py:140 **  Train=>
Epoch: 27	Batch_num: 1000	lr: 0.000	loss: 0.010	acc: 99.590%	train_loss: 0.0104	train_acc: 99.52%
INFO: 2017-09-17 05:13:47: main.py:140 **  Train=>
Epoch: 27	Batch_num: 1016	lr: 0.000	loss: 0.010	acc: 99.590%	train_loss: 0.0138	train_acc: 99.47%
INFO: 2017-09-17 05:14:51: main.py:79 **  Validate=>
Epoch: 27	Valid_loss: 0.011	Valid_acc: 99.560%
INFO: 2017-09-17 05:14:53: main.py:140 **  Train=>
Epoch: 28	Batch_num: 0	lr: 0.000	loss: 0.007	acc: 99.710%	train_loss: 0.0075	train_acc: 99.71%
INFO: 2017-09-17 05:16:19: main.py:140 **  Train=>
Epoch: 28	Batch_num: 100	lr: 0.000	loss: 0.009	acc: 99.610%	train_loss: 0.0112	train_acc: 99.51%
INFO: 2017-09-17 05:17:45: main.py:140 **  Train=>
Epoch: 28	Batch_num: 200	lr: 0.000	loss: 0.009	acc: 99.620%	train_loss: 0.0115	train_acc: 99.59%
INFO: 2017-09-17 05:19:11: main.py:140 **  Train=>
Epoch: 28	Batch_num: 300	lr: 0.000	loss: 0.009	acc: 99.610%	train_loss: 0.0078	train_acc: 99.67%
INFO: 2017-09-17 05:20:37: main.py:140 **  Train=>
Epoch: 28	Batch_num: 400	lr: 0.000	loss: 0.009	acc: 99.630%	train_loss: 0.008	train_acc: 99.66%
INFO: 2017-09-17 05:22:03: main.py:140 **  Train=>
Epoch: 28	Batch_num: 500	lr: 0.000	loss: 0.009	acc: 99.600%	train_loss: 0.0097	train_acc: 99.59%
INFO: 2017-09-17 05:23:29: main.py:140 **  Train=>
Epoch: 28	Batch_num: 600	lr: 0.000	loss: 0.009	acc: 99.620%	train_loss: 0.0113	train_acc: 99.49%
INFO: 2017-09-17 05:24:55: main.py:140 **  Train=>
Epoch: 28	Batch_num: 700	lr: 0.000	loss: 0.009	acc: 99.630%	train_loss: 0.0089	train_acc: 99.64%
INFO: 2017-09-17 05:26:21: main.py:140 **  Train=>
Epoch: 28	Batch_num: 800	lr: 0.000	loss: 0.009	acc: 99.640%	train_loss: 0.0101	train_acc: 99.55%
INFO: 2017-09-17 05:27:47: main.py:140 **  Train=>
Epoch: 28	Batch_num: 900	lr: 0.000	loss: 0.009	acc: 99.640%	train_loss: 0.0085	train_acc: 99.65%
INFO: 2017-09-17 05:29:13: main.py:140 **  Train=>
Epoch: 28	Batch_num: 1000	lr: 0.000	loss: 0.009	acc: 99.640%	train_loss: 0.009	train_acc: 99.6%
INFO: 2017-09-17 05:29:27: main.py:140 **  Train=>
Epoch: 28	Batch_num: 1016	lr: 0.000	loss: 0.009	acc: 99.640%	train_loss: 0.0084	train_acc: 99.64%
INFO: 2017-09-17 05:30:30: main.py:79 **  Validate=>
Epoch: 28	Valid_loss: 0.011	Valid_acc: 99.580%
INFO: 2017-09-17 05:30:33: main.py:140 **  Train=>
Epoch: 29	Batch_num: 0	lr: 0.000	loss: 0.007	acc: 99.720%	train_loss: 0.0072	train_acc: 99.72%
INFO: 2017-09-17 05:31:59: main.py:140 **  Train=>
Epoch: 29	Batch_num: 100	lr: 0.000	loss: 0.009	acc: 99.650%	train_loss: 0.0091	train_acc: 99.59%
INFO: 2017-09-17 05:33:25: main.py:140 **  Train=>
Epoch: 29	Batch_num: 200	lr: 0.000	loss: 0.009	acc: 99.650%	train_loss: 0.009	train_acc: 99.68%
INFO: 2017-09-17 05:34:51: main.py:140 **  Train=>
Epoch: 29	Batch_num: 300	lr: 0.000	loss: 0.009	acc: 99.640%	train_loss: 0.0075	train_acc: 99.67%
INFO: 2017-09-17 05:36:17: main.py:140 **  Train=>
Epoch: 29	Batch_num: 400	lr: 0.000	loss: 0.009	acc: 99.640%	train_loss: 0.008	train_acc: 99.66%
INFO: 2017-09-17 05:37:43: main.py:140 **  Train=>
Epoch: 29	Batch_num: 500	lr: 0.000	loss: 0.009	acc: 99.620%	train_loss: 0.0094	train_acc: 99.61%
INFO: 2017-09-17 05:39:09: main.py:140 **  Train=>
Epoch: 29	Batch_num: 600	lr: 0.000	loss: 0.008	acc: 99.650%	train_loss: 0.0095	train_acc: 99.57%
INFO: 2017-09-17 05:40:36: main.py:140 **  Train=>
Epoch: 29	Batch_num: 700	lr: 0.000	loss: 0.009	acc: 99.650%	train_loss: 0.0088	train_acc: 99.64%
INFO: 2017-09-17 05:42:02: main.py:140 **  Train=>
Epoch: 29	Batch_num: 800	lr: 0.000	loss: 0.008	acc: 99.660%	train_loss: 0.0096	train_acc: 99.56%
INFO: 2017-09-17 05:43:28: main.py:140 **  Train=>
Epoch: 29	Batch_num: 900	lr: 0.000	loss: 0.008	acc: 99.660%	train_loss: 0.0081	train_acc: 99.67%
INFO: 2017-09-17 05:44:54: main.py:140 **  Train=>
Epoch: 29	Batch_num: 1000	lr: 0.000	loss: 0.008	acc: 99.660%	train_loss: 0.0087	train_acc: 99.62%
INFO: 2017-09-17 05:45:08: main.py:140 **  Train=>
Epoch: 29	Batch_num: 1016	lr: 0.000	loss: 0.008	acc: 99.660%	train_loss: 0.0074	train_acc: 99.68%
INFO: 2017-09-17 05:46:11: main.py:79 **  Validate=>
Epoch: 29	Valid_loss: 0.010	Valid_acc: 99.590%
INFO: 2017-09-17 15:27:56: utils.py:92 **  All dataset size is 5088
INFO: 2017-09-17 15:27:56: utils.py:93 **  Train dataset size is 4071
INFO: 2017-09-17 15:27:56: utils.py:95 **  Valid dataset size is 1017
INFO: 2017-09-17 15:27:56: main.py:216 **  Loading dataset...
INFO: 2017-09-17 15:27:56: main.py:230 **  All data sample counts 5088
INFO: 2017-09-17 15:27:56: main.py:231 **  Train data batch size 4
INFO: 2017-09-17 15:27:56: main.py:232 **  Train data sample counts 4068
INFO: 2017-09-17 15:27:56: main.py:233 **  Valid data sample counts 1018
INFO: 2017-09-17 15:28:00: main.py:140 **  Train=>
Epoch: 30	Batch_num: 0	lr: 0.000	loss: 0.007	acc: 99.710%	train_loss: 0.0073	train_acc: 99.71%
INFO: 2017-09-17 15:29:18: main.py:140 **  Train=>
Epoch: 30	Batch_num: 100	lr: 0.000	loss: 0.008	acc: 99.670%	train_loss: 0.0085	train_acc: 99.62%
INFO: 2017-09-17 15:30:42: main.py:140 **  Train=>
Epoch: 30	Batch_num: 200	lr: 0.000	loss: 0.008	acc: 99.670%	train_loss: 0.0089	train_acc: 99.68%
INFO: 2017-09-17 15:32:11: main.py:140 **  Train=>
Epoch: 30	Batch_num: 300	lr: 0.000	loss: 0.008	acc: 99.660%	train_loss: 0.0075	train_acc: 99.67%
INFO: 2017-09-17 15:33:40: main.py:140 **  Train=>
Epoch: 30	Batch_num: 400	lr: 0.000	loss: 0.008	acc: 99.670%	train_loss: 0.0077	train_acc: 99.66%
INFO: 2017-09-17 15:35:08: main.py:140 **  Train=>
Epoch: 30	Batch_num: 500	lr: 0.000	loss: 0.008	acc: 99.660%	train_loss: 0.0084	train_acc: 99.65%
INFO: 2017-09-17 15:36:39: main.py:140 **  Train=>
Epoch: 30	Batch_num: 600	lr: 0.000	loss: 0.008	acc: 99.660%	train_loss: 0.0089	train_acc: 99.59%
INFO: 2017-09-17 15:38:05: main.py:140 **  Train=>
Epoch: 30	Batch_num: 700	lr: 0.000	loss: 0.008	acc: 99.670%	train_loss: 0.0085	train_acc: 99.65%
INFO: 2017-09-17 15:39:31: main.py:140 **  Train=>
Epoch: 30	Batch_num: 800	lr: 0.000	loss: 0.008	acc: 99.670%	train_loss: 0.0092	train_acc: 99.59%
INFO: 2017-09-17 15:40:58: main.py:140 **  Train=>
Epoch: 30	Batch_num: 900	lr: 0.000	loss: 0.008	acc: 99.670%	train_loss: 0.0078	train_acc: 99.68%
INFO: 2017-09-17 15:42:26: main.py:140 **  Train=>
Epoch: 30	Batch_num: 1000	lr: 0.000	loss: 0.008	acc: 99.670%	train_loss: 0.0085	train_acc: 99.62%
INFO: 2017-09-17 15:42:40: main.py:140 **  Train=>
Epoch: 30	Batch_num: 1016	lr: 0.000	loss: 0.008	acc: 99.670%	train_loss: 0.0074	train_acc: 99.68%
INFO: 2017-09-17 15:43:46: main.py:79 **  Validate=>
Epoch: 30	Valid_loss: 0.009	Valid_acc: 99.610%
INFO: 2017-09-17 15:43:50: main.py:140 **  Train=>
Epoch: 31	Batch_num: 0	lr: 0.000	loss: 0.007	acc: 99.720%	train_loss: 0.007	train_acc: 99.72%
INFO: 2017-09-17 15:45:18: main.py:140 **  Train=>
Epoch: 31	Batch_num: 100	lr: 0.000	loss: 0.008	acc: 99.680%	train_loss: 0.0079	train_acc: 99.65%
INFO: 2017-09-17 15:46:46: main.py:140 **  Train=>
Epoch: 31	Batch_num: 200	lr: 0.000	loss: 0.008	acc: 99.690%	train_loss: 0.0086	train_acc: 99.7%
INFO: 2017-09-17 15:48:14: main.py:140 **  Train=>
Epoch: 31	Batch_num: 300	lr: 0.000	loss: 0.008	acc: 99.670%	train_loss: 0.0071	train_acc: 99.69%
INFO: 2017-09-17 15:49:43: main.py:140 **  Train=>
Epoch: 31	Batch_num: 400	lr: 0.000	loss: 0.007	acc: 99.680%	train_loss: 0.007	train_acc: 99.7%
INFO: 2017-09-17 15:51:11: main.py:140 **  Train=>
Epoch: 31	Batch_num: 500	lr: 0.000	loss: 0.013	acc: 99.480%	train_loss: 0.0138	train_acc: 99.44%
INFO: 2017-09-17 15:52:40: main.py:140 **  Train=>
Epoch: 31	Batch_num: 600	lr: 0.000	loss: 0.012	acc: 99.510%	train_loss: 0.0132	train_acc: 99.4%
INFO: 2017-09-17 15:54:07: main.py:140 **  Train=>
Epoch: 31	Batch_num: 700	lr: 0.000	loss: 0.011	acc: 99.550%	train_loss: 0.0097	train_acc: 99.6%
INFO: 2017-09-17 15:55:35: main.py:140 **  Train=>
Epoch: 31	Batch_num: 800	lr: 0.000	loss: 0.010	acc: 99.580%	train_loss: 0.0097	train_acc: 99.54%
INFO: 2017-09-17 15:57:02: main.py:140 **  Train=>
Epoch: 31	Batch_num: 900	lr: 0.000	loss: 0.009	acc: 99.620%	train_loss: 0.0088	train_acc: 99.63%
INFO: 2017-09-17 15:58:31: main.py:140 **  Train=>
Epoch: 31	Batch_num: 1000	lr: 0.000	loss: 0.009	acc: 99.610%	train_loss: 0.009	train_acc: 99.59%
INFO: 2017-09-17 15:58:44: main.py:140 **  Train=>
Epoch: 31	Batch_num: 1016	lr: 0.000	loss: 0.009	acc: 99.610%	train_loss: 0.0083	train_acc: 99.65%
INFO: 2017-09-17 15:59:48: main.py:79 **  Validate=>
Epoch: 31	Valid_loss: 0.011	Valid_acc: 99.570%
INFO: 2017-09-17 15:59:51: main.py:140 **  Train=>
Epoch: 32	Batch_num: 0	lr: 0.000	loss: 0.007	acc: 99.720%	train_loss: 0.0072	train_acc: 99.72%
INFO: 2017-09-17 16:01:18: main.py:140 **  Train=>
Epoch: 32	Batch_num: 100	lr: 0.000	loss: 0.009	acc: 99.640%	train_loss: 0.0099	train_acc: 99.56%
INFO: 2017-09-17 16:02:47: main.py:140 **  Train=>
Epoch: 32	Batch_num: 200	lr: 0.000	loss: 0.009	acc: 99.650%	train_loss: 0.0103	train_acc: 99.64%
INFO: 2017-09-17 16:04:15: main.py:140 **  Train=>
Epoch: 32	Batch_num: 300	lr: 0.000	loss: 0.009	acc: 99.640%	train_loss: 0.0091	train_acc: 99.62%
INFO: 2017-09-17 16:05:42: main.py:140 **  Train=>
Epoch: 32	Batch_num: 400	lr: 0.000	loss: 0.009	acc: 99.630%	train_loss: 0.0074	train_acc: 99.69%
INFO: 2017-09-17 16:07:12: main.py:140 **  Train=>
Epoch: 32	Batch_num: 500	lr: 0.000	loss: 0.009	acc: 99.630%	train_loss: 0.0085	train_acc: 99.64%
INFO: 2017-09-17 16:08:38: main.py:140 **  Train=>
Epoch: 32	Batch_num: 600	lr: 0.000	loss: 0.008	acc: 99.650%	train_loss: 0.0107	train_acc: 99.51%
INFO: 2017-09-17 16:10:04: main.py:140 **  Train=>
Epoch: 32	Batch_num: 700	lr: 0.000	loss: 0.008	acc: 99.650%	train_loss: 0.0086	train_acc: 99.65%
INFO: 2017-09-17 16:11:31: main.py:140 **  Train=>
Epoch: 32	Batch_num: 800	lr: 0.000	loss: 0.008	acc: 99.660%	train_loss: 0.0096	train_acc: 99.55%
INFO: 2017-09-17 16:12:57: main.py:140 **  Train=>
Epoch: 32	Batch_num: 900	lr: 0.000	loss: 0.008	acc: 99.670%	train_loss: 0.008	train_acc: 99.68%
INFO: 2017-09-17 16:14:23: main.py:140 **  Train=>
Epoch: 32	Batch_num: 1000	lr: 0.000	loss: 0.008	acc: 99.670%	train_loss: 0.0078	train_acc: 99.65%
INFO: 2017-09-17 16:14:37: main.py:140 **  Train=>
Epoch: 32	Batch_num: 1016	lr: 0.000	loss: 0.008	acc: 99.670%	train_loss: 0.0075	train_acc: 99.68%
INFO: 2017-09-17 16:15:41: main.py:79 **  Validate=>
Epoch: 32	Valid_loss: 0.009	Valid_acc: 99.620%
INFO: 2017-09-17 16:15:45: main.py:140 **  Train=>
Epoch: 33	Batch_num: 0	lr: 0.000	loss: 0.007	acc: 99.740%	train_loss: 0.0067	train_acc: 99.74%
INFO: 2017-09-17 16:17:11: main.py:140 **  Train=>
Epoch: 33	Batch_num: 100	lr: 0.000	loss: 0.008	acc: 99.670%	train_loss: 0.008	train_acc: 99.64%
INFO: 2017-09-17 16:18:37: main.py:140 **  Train=>
Epoch: 33	Batch_num: 200	lr: 0.000	loss: 0.008	acc: 99.680%	train_loss: 0.0082	train_acc: 99.71%
INFO: 2017-09-17 16:20:04: main.py:140 **  Train=>
Epoch: 33	Batch_num: 300	lr: 0.000	loss: 0.008	acc: 99.670%	train_loss: 0.0079	train_acc: 99.64%
INFO: 2017-09-17 16:21:30: main.py:140 **  Train=>
Epoch: 33	Batch_num: 400	lr: 0.000	loss: 0.008	acc: 99.660%	train_loss: 0.0075	train_acc: 99.67%
INFO: 2017-09-17 16:22:58: main.py:140 **  Train=>
Epoch: 33	Batch_num: 500	lr: 0.000	loss: 0.008	acc: 99.660%	train_loss: 0.0082	train_acc: 99.65%
INFO: 2017-09-17 16:24:28: main.py:140 **  Train=>
Epoch: 33	Batch_num: 600	lr: 0.000	loss: 0.008	acc: 99.680%	train_loss: 0.0084	train_acc: 99.61%
INFO: 2017-09-17 16:25:56: main.py:140 **  Train=>
Epoch: 33	Batch_num: 700	lr: 0.000	loss: 0.008	acc: 99.680%	train_loss: 0.0083	train_acc: 99.66%
INFO: 2017-09-17 16:27:25: main.py:140 **  Train=>
Epoch: 33	Batch_num: 800	lr: 0.000	loss: 0.008	acc: 99.680%	train_loss: 0.0085	train_acc: 99.61%
INFO: 2017-09-17 16:28:54: main.py:140 **  Train=>
Epoch: 33	Batch_num: 900	lr: 0.000	loss: 0.007	acc: 99.690%	train_loss: 0.0077	train_acc: 99.68%
INFO: 2017-09-17 16:30:22: main.py:140 **  Train=>
Epoch: 33	Batch_num: 1000	lr: 0.000	loss: 0.008	acc: 99.680%	train_loss: 0.0077	train_acc: 99.66%
INFO: 2017-09-17 16:30:36: main.py:140 **  Train=>
Epoch: 33	Batch_num: 1016	lr: 0.000	loss: 0.008	acc: 99.680%	train_loss: 0.0069	train_acc: 99.7%
INFO: 2017-09-17 16:31:41: main.py:79 **  Validate=>
Epoch: 33	Valid_loss: 0.009	Valid_acc: 99.620%
INFO: 2017-09-17 16:31:46: main.py:140 **  Train=>
Epoch: 34	Batch_num: 0	lr: 0.000	loss: 0.007	acc: 99.740%	train_loss: 0.0067	train_acc: 99.74%
INFO: 2017-09-17 16:33:14: main.py:140 **  Train=>
Epoch: 34	Batch_num: 100	lr: 0.000	loss: 0.007	acc: 99.690%	train_loss: 0.0075	train_acc: 99.67%
INFO: 2017-09-17 16:34:44: main.py:140 **  Train=>
Epoch: 34	Batch_num: 200	lr: 0.000	loss: 0.007	acc: 99.690%	train_loss: 0.0073	train_acc: 99.74%
INFO: 2017-09-17 16:36:12: main.py:140 **  Train=>
Epoch: 34	Batch_num: 300	lr: 0.000	loss: 0.008	acc: 99.680%	train_loss: 0.0069	train_acc: 99.7%
INFO: 2017-09-17 16:37:42: main.py:140 **  Train=>
Epoch: 34	Batch_num: 400	lr: 0.000	loss: 0.008	acc: 99.680%	train_loss: 0.0069	train_acc: 99.69%
INFO: 2017-09-17 16:39:11: main.py:140 **  Train=>
Epoch: 34	Batch_num: 500	lr: 0.000	loss: 0.007	acc: 99.680%	train_loss: 0.0079	train_acc: 99.66%
INFO: 2017-09-17 16:40:40: main.py:140 **  Train=>
Epoch: 34	Batch_num: 600	lr: 0.000	loss: 0.007	acc: 99.690%	train_loss: 0.0081	train_acc: 99.63%
INFO: 2017-09-17 16:42:08: main.py:140 **  Train=>
Epoch: 34	Batch_num: 700	lr: 0.000	loss: 0.007	acc: 99.690%	train_loss: 0.008	train_acc: 99.67%
INFO: 2017-09-17 16:43:37: main.py:140 **  Train=>
Epoch: 34	Batch_num: 800	lr: 0.000	loss: 0.007	acc: 99.690%	train_loss: 0.0081	train_acc: 99.63%
INFO: 2017-09-17 16:45:05: main.py:140 **  Train=>
Epoch: 34	Batch_num: 900	lr: 0.000	loss: 0.007	acc: 99.700%	train_loss: 0.0074	train_acc: 99.69%
INFO: 2017-09-17 16:46:32: main.py:140 **  Train=>
Epoch: 34	Batch_num: 1000	lr: 0.000	loss: 0.007	acc: 99.690%	train_loss: 0.0079	train_acc: 99.65%
INFO: 2017-09-17 16:46:46: main.py:140 **  Train=>
Epoch: 34	Batch_num: 1016	lr: 0.000	loss: 0.007	acc: 99.690%	train_loss: 0.0067	train_acc: 99.71%
INFO: 2017-09-17 16:47:52: main.py:79 **  Validate=>
Epoch: 34	Valid_loss: 0.009	Valid_acc: 99.620%
INFO: 2017-09-17 16:47:54: main.py:140 **  Train=>
Epoch: 35	Batch_num: 0	lr: 0.000	loss: 0.007	acc: 99.720%	train_loss: 0.0068	train_acc: 99.72%
INFO: 2017-09-17 16:49:22: main.py:140 **  Train=>
Epoch: 35	Batch_num: 100	lr: 0.000	loss: 0.007	acc: 99.700%	train_loss: 0.0075	train_acc: 99.67%
INFO: 2017-09-17 16:50:49: main.py:140 **  Train=>
Epoch: 35	Batch_num: 200	lr: 0.000	loss: 0.007	acc: 99.700%	train_loss: 0.0076	train_acc: 99.73%
INFO: 2017-09-17 16:52:18: main.py:140 **  Train=>
Epoch: 35	Batch_num: 300	lr: 0.000	loss: 0.007	acc: 99.690%	train_loss: 0.0074	train_acc: 99.67%
INFO: 2017-09-17 16:53:48: main.py:140 **  Train=>
Epoch: 35	Batch_num: 400	lr: 0.000	loss: 0.007	acc: 99.690%	train_loss: 0.0067	train_acc: 99.71%
INFO: 2017-09-17 16:55:17: main.py:140 **  Train=>
Epoch: 35	Batch_num: 500	lr: 0.000	loss: 0.008	acc: 99.660%	train_loss: 0.0095	train_acc: 99.6%
INFO: 2017-09-17 16:56:46: main.py:140 **  Train=>
Epoch: 35	Batch_num: 600	lr: 0.000	loss: 0.007	acc: 99.690%	train_loss: 0.0086	train_acc: 99.6%
INFO: 2017-09-17 16:58:15: main.py:140 **  Train=>
Epoch: 35	Batch_num: 700	lr: 0.000	loss: 0.007	acc: 99.700%	train_loss: 0.0074	train_acc: 99.7%
INFO: 2017-09-17 16:59:44: main.py:140 **  Train=>
Epoch: 35	Batch_num: 800	lr: 0.000	loss: 0.007	acc: 99.700%	train_loss: 0.0081	train_acc: 99.62%
INFO: 2017-09-17 17:01:13: main.py:140 **  Train=>
Epoch: 35	Batch_num: 900	lr: 0.000	loss: 0.008	acc: 99.660%	train_loss: 0.0078	train_acc: 99.67%
INFO: 2017-09-17 17:02:41: main.py:140 **  Train=>
Epoch: 35	Batch_num: 1000	lr: 0.000	loss: 0.007	acc: 99.690%	train_loss: 0.0082	train_acc: 99.63%
INFO: 2017-09-17 17:02:56: main.py:140 **  Train=>
Epoch: 35	Batch_num: 1016	lr: 0.000	loss: 0.007	acc: 99.690%	train_loss: 0.0066	train_acc: 99.72%
INFO: 2017-09-17 17:04:01: main.py:79 **  Validate=>
Epoch: 35	Valid_loss: 0.009	Valid_acc: 99.620%
INFO: 2017-09-17 17:04:05: main.py:140 **  Train=>
Epoch: 36	Batch_num: 0	lr: 0.000	loss: 0.007	acc: 99.730%	train_loss: 0.0067	train_acc: 99.73%
INFO: 2017-09-17 17:05:34: main.py:140 **  Train=>
Epoch: 36	Batch_num: 100	lr: 0.000	loss: 0.007	acc: 99.700%	train_loss: 0.0075	train_acc: 99.66%
INFO: 2017-09-17 17:07:02: main.py:140 **  Train=>
Epoch: 36	Batch_num: 200	lr: 0.000	loss: 0.007	acc: 99.710%	train_loss: 0.0069	train_acc: 99.76%
INFO: 2017-09-17 17:08:31: main.py:140 **  Train=>
Epoch: 36	Batch_num: 300	lr: 0.000	loss: 0.007	acc: 99.700%	train_loss: 0.0066	train_acc: 99.71%
INFO: 2017-09-17 17:09:59: main.py:140 **  Train=>
Epoch: 36	Batch_num: 400	lr: 0.000	loss: 0.007	acc: 99.700%	train_loss: 0.0064	train_acc: 99.71%
INFO: 2017-09-17 17:11:28: main.py:140 **  Train=>
Epoch: 36	Batch_num: 500	lr: 0.000	loss: 0.007	acc: 99.690%	train_loss: 0.0079	train_acc: 99.66%
INFO: 2017-09-17 17:12:56: main.py:140 **  Train=>
Epoch: 36	Batch_num: 600	lr: 0.000	loss: 0.007	acc: 99.700%	train_loss: 0.008	train_acc: 99.64%
INFO: 2017-09-17 17:14:24: main.py:140 **  Train=>
Epoch: 36	Batch_num: 700	lr: 0.000	loss: 0.007	acc: 99.700%	train_loss: 0.0072	train_acc: 99.7%
INFO: 2017-09-17 17:15:53: main.py:140 **  Train=>
Epoch: 36	Batch_num: 800	lr: 0.000	loss: 0.007	acc: 99.700%	train_loss: 0.0079	train_acc: 99.64%
INFO: 2017-09-17 17:17:23: main.py:140 **  Train=>
Epoch: 36	Batch_num: 900	lr: 0.000	loss: 0.008	acc: 99.670%	train_loss: 0.008	train_acc: 99.66%
INFO: 2017-09-17 17:18:52: main.py:140 **  Train=>
Epoch: 36	Batch_num: 1000	lr: 0.000	loss: 0.008	acc: 99.680%	train_loss: 0.0079	train_acc: 99.65%
INFO: 2017-09-17 17:19:06: main.py:140 **  Train=>
Epoch: 36	Batch_num: 1016	lr: 0.000	loss: 0.008	acc: 99.680%	train_loss: 0.0068	train_acc: 99.71%
INFO: 2017-09-17 17:20:11: main.py:79 **  Validate=>
Epoch: 36	Valid_loss: 0.009	Valid_acc: 99.620%
INFO: 2017-09-17 17:20:14: main.py:140 **  Train=>
Epoch: 37	Batch_num: 0	lr: 0.000	loss: 0.006	acc: 99.740%	train_loss: 0.0065	train_acc: 99.74%
INFO: 2017-09-17 17:21:43: main.py:140 **  Train=>
Epoch: 37	Batch_num: 100	lr: 0.000	loss: 0.007	acc: 99.690%	train_loss: 0.0076	train_acc: 99.66%
INFO: 2017-09-17 17:23:12: main.py:140 **  Train=>
Epoch: 37	Batch_num: 200	lr: 0.000	loss: 0.007	acc: 99.710%	train_loss: 0.0079	train_acc: 99.72%
INFO: 2017-09-17 17:24:44: main.py:140 **  Train=>
Epoch: 37	Batch_num: 300	lr: 0.000	loss: 0.007	acc: 99.700%	train_loss: 0.0066	train_acc: 99.71%
INFO: 2017-09-17 17:26:13: main.py:140 **  Train=>
Epoch: 37	Batch_num: 400	lr: 0.000	loss: 0.007	acc: 99.710%	train_loss: 0.0064	train_acc: 99.73%
INFO: 2017-09-17 17:27:41: main.py:140 **  Train=>
Epoch: 37	Batch_num: 500	lr: 0.000	loss: 0.007	acc: 99.700%	train_loss: 0.0075	train_acc: 99.68%
INFO: 2017-09-17 17:29:09: main.py:140 **  Train=>
Epoch: 37	Batch_num: 600	lr: 0.000	loss: 0.007	acc: 99.710%	train_loss: 0.0074	train_acc: 99.66%
INFO: 2017-09-17 17:30:38: main.py:140 **  Train=>
Epoch: 37	Batch_num: 700	lr: 0.000	loss: 0.007	acc: 99.710%	train_loss: 0.0069	train_acc: 99.72%
INFO: 2017-09-17 17:32:08: main.py:140 **  Train=>
Epoch: 37	Batch_num: 800	lr: 0.000	loss: 0.007	acc: 99.720%	train_loss: 0.0077	train_acc: 99.65%
INFO: 2017-09-17 17:33:37: main.py:140 **  Train=>
Epoch: 37	Batch_num: 900	lr: 0.000	loss: 0.007	acc: 99.710%	train_loss: 0.0074	train_acc: 99.69%
INFO: 2017-09-17 17:35:06: main.py:140 **  Train=>
Epoch: 37	Batch_num: 1000	lr: 0.000	loss: 0.007	acc: 99.710%	train_loss: 0.008	train_acc: 99.64%
INFO: 2017-09-17 17:35:20: main.py:140 **  Train=>
Epoch: 37	Batch_num: 1016	lr: 0.000	loss: 0.007	acc: 99.710%	train_loss: 0.0062	train_acc: 99.74%
INFO: 2017-09-17 17:36:25: main.py:79 **  Validate=>
Epoch: 37	Valid_loss: 0.009	Valid_acc: 99.630%
INFO: 2017-09-17 17:36:29: main.py:140 **  Train=>
Epoch: 38	Batch_num: 0	lr: 0.000	loss: 0.006	acc: 99.740%	train_loss: 0.0064	train_acc: 99.74%
INFO: 2017-09-17 17:37:57: main.py:140 **  Train=>
Epoch: 38	Batch_num: 100	lr: 0.000	loss: 0.007	acc: 99.710%	train_loss: 0.007	train_acc: 99.69%
INFO: 2017-09-17 17:39:25: main.py:140 **  Train=>
Epoch: 38	Batch_num: 200	lr: 0.000	loss: 0.007	acc: 99.730%	train_loss: 0.0068	train_acc: 99.76%
INFO: 2017-09-17 17:40:54: main.py:140 **  Train=>
Epoch: 38	Batch_num: 300	lr: 0.000	loss: 0.007	acc: 99.710%	train_loss: 0.0065	train_acc: 99.71%
INFO: 2017-09-17 17:42:23: main.py:140 **  Train=>
Epoch: 38	Batch_num: 400	lr: 0.000	loss: 0.007	acc: 99.710%	train_loss: 0.0059	train_acc: 99.74%
INFO: 2017-09-17 17:43:50: main.py:140 **  Train=>
Epoch: 38	Batch_num: 500	lr: 0.000	loss: 0.007	acc: 99.710%	train_loss: 0.0079	train_acc: 99.67%
INFO: 2017-09-17 17:45:17: main.py:140 **  Train=>
Epoch: 38	Batch_num: 600	lr: 0.000	loss: 0.007	acc: 99.710%	train_loss: 0.007	train_acc: 99.68%
INFO: 2017-09-17 17:46:46: main.py:140 **  Train=>
Epoch: 38	Batch_num: 700	lr: 0.000	loss: 0.007	acc: 99.720%	train_loss: 0.0068	train_acc: 99.72%
INFO: 2017-09-17 17:48:14: main.py:140 **  Train=>
Epoch: 38	Batch_num: 800	lr: 0.000	loss: 0.007	acc: 99.710%	train_loss: 0.0072	train_acc: 99.68%
INFO: 2017-09-17 17:49:42: main.py:140 **  Train=>
Epoch: 38	Batch_num: 900	lr: 0.000	loss: 0.007	acc: 99.720%	train_loss: 0.0073	train_acc: 99.7%
INFO: 2017-09-17 17:51:10: main.py:140 **  Train=>
Epoch: 38	Batch_num: 1000	lr: 0.000	loss: 0.007	acc: 99.710%	train_loss: 0.0072	train_acc: 99.67%
INFO: 2017-09-17 17:51:25: main.py:140 **  Train=>
Epoch: 38	Batch_num: 1016	lr: 0.000	loss: 0.007	acc: 99.710%	train_loss: 0.0066	train_acc: 99.72%
INFO: 2017-09-17 17:52:29: main.py:79 **  Validate=>
Epoch: 38	Valid_loss: 0.009	Valid_acc: 99.630%
INFO: 2017-09-17 17:52:32: main.py:140 **  Train=>
Epoch: 39	Batch_num: 0	lr: 0.000	loss: 0.007	acc: 99.730%	train_loss: 0.0066	train_acc: 99.73%
INFO: 2017-09-17 17:54:00: main.py:140 **  Train=>
Epoch: 39	Batch_num: 100	lr: 0.000	loss: 0.007	acc: 99.720%	train_loss: 0.007	train_acc: 99.69%
INFO: 2017-09-17 17:55:27: main.py:140 **  Train=>
Epoch: 39	Batch_num: 200	lr: 0.000	loss: 0.006	acc: 99.730%	train_loss: 0.0065	train_acc: 99.77%
INFO: 2017-09-17 17:56:55: main.py:140 **  Train=>
Epoch: 39	Batch_num: 300	lr: 0.000	loss: 0.007	acc: 99.710%	train_loss: 0.0063	train_acc: 99.73%
INFO: 2017-09-17 17:58:22: main.py:140 **  Train=>
Epoch: 39	Batch_num: 400	lr: 0.000	loss: 0.007	acc: 99.720%	train_loss: 0.0058	train_acc: 99.75%
INFO: 2017-09-17 17:59:50: main.py:140 **  Train=>
Epoch: 39	Batch_num: 500	lr: 0.000	loss: 0.007	acc: 99.720%	train_loss: 0.0088	train_acc: 99.63%
INFO: 2017-09-17 18:01:18: main.py:140 **  Train=>
Epoch: 39	Batch_num: 600	lr: 0.000	loss: 0.007	acc: 99.720%	train_loss: 0.0071	train_acc: 99.67%
INFO: 2017-09-17 18:02:46: main.py:140 **  Train=>
Epoch: 39	Batch_num: 700	lr: 0.000	loss: 0.007	acc: 99.720%	train_loss: 0.0071	train_acc: 99.7%
INFO: 2017-09-17 18:04:14: main.py:140 **  Train=>
Epoch: 39	Batch_num: 800	lr: 0.000	loss: 0.007	acc: 99.720%	train_loss: 0.0072	train_acc: 99.68%
INFO: 2017-09-17 18:05:44: main.py:140 **  Train=>
Epoch: 39	Batch_num: 900	lr: 0.000	loss: 0.007	acc: 99.720%	train_loss: 0.008	train_acc: 99.67%
INFO: 2017-09-17 18:07:12: main.py:140 **  Train=>
Epoch: 39	Batch_num: 1000	lr: 0.000	loss: 0.007	acc: 99.700%	train_loss: 0.0081	train_acc: 99.64%
INFO: 2017-09-17 18:07:26: main.py:140 **  Train=>
Epoch: 39	Batch_num: 1016	lr: 0.000	loss: 0.007	acc: 99.700%	train_loss: 0.0066	train_acc: 99.72%
INFO: 2017-09-17 18:08:32: main.py:79 **  Validate=>
Epoch: 39	Valid_loss: 0.009	Valid_acc: 99.630%
INFO: 2017-09-17 18:08:35: main.py:140 **  Train=>
Epoch: 40	Batch_num: 0	lr: 0.000	loss: 0.006	acc: 99.750%	train_loss: 0.0061	train_acc: 99.75%
INFO: 2017-09-17 18:10:03: main.py:140 **  Train=>
Epoch: 40	Batch_num: 100	lr: 0.000	loss: 0.020	acc: 99.220%	train_loss: 0.0172	train_acc: 99.32%
INFO: 2017-09-17 19:00:20: utils.py:94 **  All dataset size is 5088
INFO: 2017-09-17 19:00:20: utils.py:95 **  Train dataset size is 4071
INFO: 2017-09-17 19:00:20: utils.py:97 **  Valid dataset size is 1017
INFO: 2017-09-17 19:00:20: main.py:222 **  Loading dataset...
INFO: 2017-09-17 19:00:20: main.py:236 **  All data sample counts 5088
INFO: 2017-09-17 19:00:20: main.py:237 **  Train data batch size 4
INFO: 2017-09-17 19:00:20: main.py:238 **  Train data sample counts 4068
INFO: 2017-09-17 19:00:20: main.py:239 **  Valid data sample counts 1018
INFO: 2017-09-17 19:00:57: utils.py:94 **  All dataset size is 5088
INFO: 2017-09-17 19:00:57: utils.py:95 **  Train dataset size is 4071
INFO: 2017-09-17 19:00:57: utils.py:97 **  Valid dataset size is 1017
INFO: 2017-09-17 19:00:57: main.py:222 **  Loading dataset...
INFO: 2017-09-17 19:00:57: main.py:236 **  All data sample counts 5088
INFO: 2017-09-17 19:00:57: main.py:237 **  Train data batch size 4
INFO: 2017-09-17 19:00:57: main.py:238 **  Train data sample counts 4068
INFO: 2017-09-17 19:00:57: main.py:239 **  Valid data sample counts 1018
INFO: 2017-09-17 19:01:34: utils.py:94 **  All dataset size is 5088
INFO: 2017-09-17 19:01:34: utils.py:95 **  Train dataset size is 4071
INFO: 2017-09-17 19:01:34: utils.py:97 **  Valid dataset size is 1017
INFO: 2017-09-17 19:01:34: main.py:222 **  Loading dataset...
INFO: 2017-09-17 19:01:34: main.py:236 **  All data sample counts 5088
INFO: 2017-09-17 19:01:34: main.py:237 **  Train data batch size 4
INFO: 2017-09-17 19:01:34: main.py:238 **  Train data sample counts 4068
INFO: 2017-09-17 19:01:34: main.py:239 **  Valid data sample counts 1018
INFO: 2017-09-17 19:02:25: utils.py:94 **  All dataset size is 5088
INFO: 2017-09-17 19:02:25: utils.py:95 **  Train dataset size is 4071
INFO: 2017-09-17 19:02:25: utils.py:97 **  Valid dataset size is 1017
INFO: 2017-09-17 19:02:25: main.py:222 **  Loading dataset...
INFO: 2017-09-17 19:02:25: main.py:236 **  All data sample counts 5088
INFO: 2017-09-17 19:02:25: main.py:237 **  Train data batch size 4
INFO: 2017-09-17 19:02:25: main.py:238 **  Train data sample counts 4068
INFO: 2017-09-17 19:02:25: main.py:239 **  Valid data sample counts 1018
INFO: 2017-09-17 19:02:29: main.py:141 **  Train=>
Epoch: 38	Batch_num: 0	lr: 0.000	loss: 0.310	acc: 92.690%	train_loss: 0.3098	train_acc: 92.69%
INFO: 2017-09-17 19:02:54: utils.py:94 **  All dataset size is 5088
INFO: 2017-09-17 19:02:54: utils.py:95 **  Train dataset size is 4071
INFO: 2017-09-17 19:02:54: utils.py:97 **  Valid dataset size is 1017
INFO: 2017-09-17 19:02:54: main.py:222 **  Loading dataset...
INFO: 2017-09-17 19:02:54: main.py:236 **  All data sample counts 5088
INFO: 2017-09-17 19:02:54: main.py:237 **  Train data batch size 4
INFO: 2017-09-17 19:02:54: main.py:238 **  Train data sample counts 4068
INFO: 2017-09-17 19:02:54: main.py:239 **  Valid data sample counts 1018
INFO: 2017-09-17 19:02:59: main.py:141 **  Train=>
Epoch: 38	Batch_num: 0	lr: 0.000	loss: 0.006	acc: 99.740%	train_loss: 0.0064	train_acc: 99.74%
INFO: 2017-09-17 19:04:17: main.py:141 **  Train=>
Epoch: 38	Batch_num: 100	lr: 0.000	loss: 0.200	acc: 92.330%	train_loss: 0.1975	train_acc: 92.2%
INFO: 2017-09-17 19:05:40: main.py:141 **  Train=>
Epoch: 38	Batch_num: 200	lr: 0.000	loss: 0.102	acc: 96.080%	train_loss: 0.1258	train_acc: 95.76%
INFO: 2017-09-17 19:07:06: main.py:141 **  Train=>
Epoch: 38	Batch_num: 300	lr: 0.000	loss: 0.056	acc: 97.720%	train_loss: 0.0457	train_acc: 97.97%
INFO: 2017-09-17 19:08:35: main.py:141 **  Train=>
Epoch: 38	Batch_num: 400	lr: 0.000	loss: 0.048	acc: 98.100%	train_loss: 0.032	train_acc: 98.63%
INFO: 2017-09-17 19:10:03: main.py:141 **  Train=>
Epoch: 38	Batch_num: 500	lr: 0.000	loss: 0.045	acc: 98.140%	train_loss: 0.0513	train_acc: 97.95%
INFO: 2017-09-17 19:11:31: main.py:141 **  Train=>
Epoch: 38	Batch_num: 600	lr: 0.000	loss: 0.039	acc: 98.400%	train_loss: 0.039	train_acc: 98.18%
INFO: 2017-09-17 19:13:02: main.py:141 **  Train=>
Epoch: 38	Batch_num: 700	lr: 0.000	loss: 0.034	acc: 98.630%	train_loss: 0.0332	train_acc: 98.71%
